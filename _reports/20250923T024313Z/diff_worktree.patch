diff --git a/01-introduction-applications/01_applications_calibration_conceptuel.tex b/01-introduction-applications/01_applications_calibration_conceptuel.tex
index d50f2a7..64b42b3 100755
--- a/01-introduction-applications/01_applications_calibration_conceptuel.tex
+++ b/01-introduction-applications/01_applications_calibration_conceptuel.tex
@@ -1,27 +1,27 @@
 \subsection{Aperçu de l’invariant \(I_{1}(T)=P(T)/T\)}

-Pour visualiser la dynamique de l’invariant \(I_{1}(T)\), calculé à partir de l’intégration de \(\dot P(T)\) (grille~v3), on trace \(I_{1}(T)\) en échelle \emph{log–log}, comparée à la valeur unité du modèle standard \(\Lambda\mathrm{CDM}\).
+Pour visualiser la dynamique de l’invariant \(I_{1}(T)\), calculé à partir de l’intégration de \(\dot P(T)\) (grille~v3), on trace \(I_{1}(T)\) en échelle \emph{log–log}, comparée à la valeur unité du modèle standard \(\Lambda\mathrm{CDM}\).

-Le script \texttt{zz-scripts/chapter01/trace\_fig02.py} importe la table dense
-\texttt{zz-data/chapter01/01\_P\_en\_fonction\_de\_T.dat} et calcule
-l’invariant \(I_{1}(T)=P(T)/T\) avant de générer la figure suivante :
+Le script \texttt{zz-scripts/chapter01/trace\_fig02.py} importe la table dense
+\texttt{zz-data/chapter01/01\_P\_en\_fonction\_de\_T.dat} et calcule
+l’invariant \(I_{1}(T)=P(T)/T\) avant de générer la figure suivante :

 \begin{figure}[htbp]
   \centering
   \includegraphics{zz-figures/chapter01/fig_02_calibration_logistique.png}
-  \caption{Fig.~02 – Évolution de l’invariant \(I_{1}(T)=P(T)/T\) (échelle log–log, grille~v3) :
+  \caption{Fig.~02 – Évolution de l’invariant \(I_{1}(T)=P(T)/T\) (échelle log–log, grille~v3) :
            MCGT : \(I_{1}=P/T\) et référence \(\,I_{1}=1\) (ligne pointillée).}
   \label{fig:ratio_PT_log}
 \end{figure}

 Ce tracé met en évidence :
 \begin{itemize}
-  \item un pic initial modéré dû à la transition logistique et à l’intégration de \(\dot P(T)\),
+  \item un pic initial modéré dû à la transition logistique et à l’intégration de \(\dot P(T)\),
   \item une stabilisation vers la valeur unité (\(I_{1}=1\)) pour \(T\gg T_{c}\),
 \end{itemize}
 validant ainsi la cohérence physique et la convergence de l’invariant sous l’effet du paramétrage logistique optimisé.

-Ce pic initial modéré de \(I_{1}(T)\) traduit la même dynamique de croissance rapide
+Ce pic initial modéré de \(I_{1}(T)\) traduit la même dynamique de croissance rapide
 qui conduit à la formation précoce des galaxies massives (cf. section suivante).

 \subsection{Formation précoce des galaxies (JWST, \(T=0.30\) Gyr)}
@@ -45,11 +45,11 @@ On calcule alors \(\alpha_{\mathrm{log}}(T)\) et \(\alpha(T)\) comme en section~
 \quad
   T_{0}=10^{-6}\,\mathrm{Gyr}.
 \]
-L’invariant s’écrit
+L’invariant s’écrit
 \[
   I_{1}(T)=\frac{P(T)}{T},
 \]
-et son évaluation à \(T=0.30\) Gyr, obtenue à partir des données
+et son évaluation à \(T=0.30\) Gyr, obtenue à partir des données
 \texttt{zz-data/chapter01/01\_chronologie\_resultats.csv}, donne
 \[
   I_{1}(0.30)\approx1.32.
@@ -60,7 +60,7 @@ Pour une comparaison entre ce jeu historique et la calibration optimisée v3, vo

 \subsection{Correction de la tension sur \(H_{0}\) à la recombinaison}

-Au moment de la recombinaison (\(T_{\mathrm{rec}}\approx3.8\times10^{-4}\) Gyr),
+Au moment de la recombinaison (\(T_{\mathrm{rec}}\approx3.8\times10^{-4}\) Gyr),
 puisque \(H\propto P/a\), l’intégration de \(\dot P(T)\) dans la configuration optimisée induit une légère correction de l’expansion précoce :
 \[
   \frac{\delta H_{0}}{H_{0}}
@@ -70,39 +70,39 @@ puisque \(H\propto P/a\), l’intégration de \(\dot P(T)\) dans la configuratio
   \quad
   a=(1+z)^{-1}\;(\text{où }z_{\rm rec}\approx1100).
 \]
-Les valeurs mesurées sont
+Les valeurs mesurées sont
 \[
   H_{0}^{\rm loc}=73.0\pm1.0\;\mathrm{km\,s^{-1}\,Mpc^{-1}},
   \quad
   H_{0}^{\rm CMB}=67.4\pm0.5\;\mathrm{km\,s^{-1}\,Mpc^{-1}},
 \]
-soit un écart initial d’environ \(6\%\), ramené à
+soit un écart initial d’environ \(6\%\), ramené à
 \[
   \frac{\delta H_{0}}{H_{0}}\simeq9.2\times10^{-4}
   \quad(\text{soit }0.092\%).
 \]

-Les données numériques exactes figurent dans
-\texttt{zz-data/chapter01/01\_H0\_tension\_MCGT.csv}.
+Les données numériques exactes figurent dans
+\texttt{zz-data/chapter01/01\_H0\_tension\_MCGT.csv}.

-Les écarts relatifs \(\varepsilon_i\) aux neuf jalons sont détaillés dans
-\texttt{zz-data/chapter01/01\_ecart\_relatif\_chronologie.csv}.
+Les écarts relatifs \(\varepsilon_i\) aux neuf jalons sont détaillés dans
+\texttt{zz-data/chapter01/01\_ecart\_relatif\_chronologie.csv}.

-Le script \texttt{zz-scripts/chapter01/trace\_fig03.py} importe ces données
+Le script \texttt{zz-scripts/chapter01/trace\_fig03.py} importe ces données
 et génère la figure suivante :

 \begin{figure}[htbp]
   \centering
   \includegraphics[width=0.75\linewidth]{zz-figures/chapter01/fig_03_ecart_rel_chronologie.png}
-  \caption{Fig.~03 – Écart relatif
+  \caption{Fig.~03 – Écart relatif
     \(\varepsilon(T)
       =\frac{|P_{\rm calc}(T)-P_{\rm ref}(T)|}{P_{\rm ref}(T)}\times100\%\)
-    aux neuf âges clés (grille~v3, configuration optimisée) : tous les écarts
+    aux neuf âges clés (grille~v3, configuration optimisée) : tous les écarts
     restent \(<1\%\).}
   \label{fig:ecart_relatif_chronologie}
 \end{figure}

-Au-delà de cette correction relative de la recombinaison, la dynamique des invariants confirme la robustesse du paramétrage logistique optimisé.
+Au-delà de cette correction relative de la recombinaison, la dynamique des invariants confirme la robustesse du paramétrage logistique optimisé.

 \subsection{Invariants adimensionnels}

@@ -112,7 +112,7 @@ Le MCGT définit trois invariants :
   I_{2}(T)=\frac{\dot P(T)}{P(T)},\quad
   I_{3}(T)=\frac{T\,\dot P(T)}{P(T)},
 \]
-qui rendent compte de l’évolution relative de la fonction propre du temps.
+qui rendent compte de l’évolution relative de la fonction propre du temps.
 Dans la configuration optimisée (grille~v3) on observe :
 \[
   I_{1}(T)\;\text{pic modéré à }\mathcal{O}(10)\text{ autour de }T_{c},
@@ -121,31 +121,31 @@ Dans la configuration optimisée (grille~v3) on observe :
   \quad
   \text{pour }T\in[10^{-6},14]\;\mathrm{Gyr}.
 \]
-Pour une visualisation détaillée en échelle log–log et des histogrammes des distributions, voir la Fig.~03 au Chapitre 4 (Invariants adimensionnels) et les données dans
-\texttt{zz-data/chapter01/01\_invariants_adimensionnels.csv}.
+Pour une visualisation détaillée en échelle log–log et des histogrammes des distributions, voir la Fig.~03 au Chapitre 4 (Invariants adimensionnels) et les données dans
+\texttt{zz-data/chapter01/01\_invariants_adimensionnels.csv}.

 \subsection{Calibration logistique détaillée}

 \subsubsection{Objectif et données d’entrée}

-L’objectif est de reproduire exactement les neuf jalons
-\((T_i,P_{\mathrm{ref}}(T_i))\) (cf. Chapitre 2) en ajustant la fonction propre
+L’objectif est de reproduire exactement les neuf jalons
+\((T_i,P_{\mathrm{ref}}(T_i))\) (cf. Chapitre 2) en ajustant la fonction propre
 du temps \(P(T)\) pour que
 \[
-  P_{\mathrm{calc}}(T_{i}) = P_{\mathrm{ref}}(T_{i}),
+  P_{\mathrm{calc}}(T_{i}) = P_{\mathrm{ref}}(T_{i}),
   \quad i=1,\dots,9.
 \]

 Les données d’entrée sont :
 \begin{itemize}
-  \item \texttt{zz-data/chapter01/01\_chronologie\_resultats.csv} :
+  \item \texttt{zz-data/chapter01/01\_chronologie\_resultats.csv} :
         neuf paires \((T_i,P_{\mathrm{ref}})\).
-  \item \texttt{zz-data/chapter01/01\_P\_en\_fonction\_de\_T.dat} :
+  \item \texttt{zz-data/chapter01/01\_P\_en\_fonction\_de\_T.dat} :
         table dense \((T,P_{\mathrm{calc}}(T))\) sur \(T\in[10^{-6},14]\) Gyr.
 \end{itemize}

 \subsubsection{Paramètres considérés}
-On conserve la forme paramétrique v3 (section~\ref{sec:cadre_theorique}),
+On conserve la forme paramétrique v3 (section~\ref{sec:cadre_theorique}),
 avec paramètres libres :
 \[
   \alpha_{0},\quad T_{c},\quad \Delta,\quad \frac{T_{p}}{T_{c}}\quad\bigl(T_{p}/T_{c}=0.14\bigr).
@@ -177,23 +177,23 @@ c’est-à-dire un écart relatif \(<1\%\).
 Le script \texttt{zz-scripts/chapter01/integration\_chronologie.py} suit ces étapes :
 \begin{enumerate}
   \item Lecture des neuf paires \texttt{zz-data/chapter01/01\_chronologie\_resultats.csv}.
-  \item Interpolation \emph{log–log} de la table brute
-      \texttt{zz-data/chapter01/01\_P\_en\_fonction\_de\_T.dat}
+  \item Interpolation \emph{log–log} de la table brute
+      \texttt{zz-data/chapter01/01\_P\_en\_fonction\_de\_T.dat}
       (le tracé de la Fig.~\ref{fig:p_vs_t_calibration} est en échelle log–lin).
-  \item Pour chaque quadruplet
+  \item Pour chaque quadruplet
         \((\alpha_{0},T_{c},\Delta,T_{p}/T_{c})\) de la grille testée :
     \begin{itemize}
-      \item Calcul de \(\alpha(T)\) et \(P_{\mathrm{calc}}(T)\) par intégration de \(\dot P\).
-      \item Évaluation de
+      \item Calcul de \(\alpha(T)\) et \(P_{\mathrm{calc}}(T)\) par intégration de \(\dot P\).
+      \item Évaluation de
       \[
         \chi^{2}
-        =
+        =
         \sum_{i=1}^{9}
         \bigl[P_{\mathrm{calc}}(T_{i}) - P_{\mathrm{ref}}(T_{i})\bigr]^{2}
         \quad(\text{sans pondération}).
       \]
     \end{itemize}
-  \item Sélection du jeu de paramètres minimisant \(\chi^{2}\).
+  \item Sélection du jeu de paramètres minimisant \(\chi^{2}\).
   \item Sauvegarde du résultat optimal et génération des figures de référence.
 \end{enumerate}

@@ -201,97 +201,97 @@ Le script \texttt{zz-scripts/chapter01/integration\_chronologie.py} suit ces ét

 Les sources pour ces tracés sont :
 \begin{itemize}
-  \item Données :
-    \texttt{zz-data/chapter01/01\_chronologie\_resultats.csv},
-    \texttt{zz-data/chapter01/01\_P\_en\_fonction\_de\_T.dat},
-    \texttt{zz-data/chapter01/01\_P\_initial.csv},
+  \item Données :
+    \texttt{zz-data/chapter01/01\_chronologie\_resultats.csv},
+    \texttt{zz-data/chapter01/01\_P\_en\_fonction\_de\_T.dat},
+    \texttt{zz-data/chapter01/01\_P\_initial.csv},
     \texttt{zz-data/chapter01/01\_P\_optimise.csv},
-    \texttt{zz-data/chapter01/01\_P\_initial\_calc\_en\_fonction\_de\_T.dat},
-    \texttt{zz-data/chapter01/01\_P\_optimise\_calc\_en\_fonction\_de\_T.dat},
+    \texttt{zz-data/chapter01/01\_P\_initial\_calc\_en\_fonction\_de\_T.dat},
+    \texttt{zz-data/chapter01/01\_P\_optimise\_calc\_en\_fonction\_de\_T.dat},
     \texttt{zz-data/chapter01/01\_deriveeP\_initiale.csv}.
-  \item Scripts :
-    \texttt{zz-scripts/chapter01/integration\_chronologie.py},
-    \texttt{zz-scripts/chapter01/trace\_fig04.py},
+  \item Scripts :
+    \texttt{zz-scripts/chapter01/integration\_chronologie.py},
+    \texttt{zz-scripts/chapter01/trace\_fig04.py},
     \texttt{zz-scripts/chapter01/trace\_fig05.py}.
 \end{itemize}

 \begin{figure}[htbp]
   \centering
   \includegraphics[width=0.75\linewidth]{zz-figures/chapter01/fig_04_evolution_P_en_fonction_de_T.png}
-  \caption{Fig.~04 – Évolution de \(P_{\mathrm{calc}}(T)\) (trait continu, log–lin) :
+  \caption{Fig.~04 – Évolution de \(P_{\mathrm{calc}}(T)\) (trait continu, log–lin) :
            configuration optimisée (grille~v3) avec interpolation log–log sur la table brute.}
   \label{fig:p_vs_t_calibration}
 \end{figure}

-Cette représentation de \(P(T)\) montre clairement la progression continue de la courbe optimisée.
+Cette représentation de \(P(T)\) montre clairement la progression continue de la courbe optimisée.
 Le diagramme suivant met en perspective cette évolution en la confrontant directement aux neuf jalons.

 \begin{figure}[htbp]
   \centering
   \includegraphics[width=0.75\linewidth]{zz-figures/chapter01/fig_05_diagramme_calibration.png}
-  \caption{Fig.~05 – Diagramme de calibration (grille~v3, échelle log–log) :
+  \caption{Fig.~05 – Diagramme de calibration (grille~v3, échelle log–log) :
            configuration optimisée et points de référence aux neuf jalons.}
   \label{fig:diagramme_calibration}
 \end{figure}

 \subsubsection{Comparaison des dérivées temporelles}

-Pour évaluer l’impact de l’optimisation sur la dérivée propre du temps \(\dot P(T)\),
+Pour évaluer l’impact de l’optimisation sur la dérivée propre du temps \(\dot P(T)\),
 nous exploitons trois jeux de données :

 \begin{itemize}
-  \item \texttt{zz-data/chapter01/01\_deriveeP\_initiale.csv} :
+  \item \texttt{zz-data/chapter01/01\_deriveeP\_initiale.csv} :
         dérivées \(\dot P\) historiques aux neuf jalons.
-  \item \texttt{zz-data/chapter01/01\_deriveeP\_optimisee.csv} :
+  \item \texttt{zz-data/chapter01/01\_deriveeP\_optimisee.csv} :
         dérivées \(\dot P\) optimisées aux neuf jalons (grille~v3).
-  \item \texttt{zz-data/chapter01/01\_deriveeP\_en\_fonction\_de\_T.dat} :
+  \item \texttt{zz-data/chapter01/01\_deriveeP\_en\_fonction\_de\_T.dat} :
         table dense \(\dot P(T)\) sur \(T\in[10^{-6},14]\) Gyr pour la configuration optimisée.
 \end{itemize}

-Le script \texttt{zz-scripts/chapter01/trace\_fig06.py} importe ces trois fichiers
+Le script \texttt{zz-scripts/chapter01/trace\_fig06.py} importe ces trois fichiers
 et produit la figure suivante :

 \begin{figure}[htbp]
   \centering
   \includegraphics[width=0.75\linewidth]{zz-figures/chapter01/fig_06_comparaison_deriveeP_initiale_vs_deriveeP_optimisee.png}
-  \caption{Fig.~06 – Comparaison de \(\dot P(T)\) :
-           dérivées aux neuf jalons pour la configuration historique (gris clair)
+  \caption{Fig.~06 – Comparaison de \(\dot P(T)\) :
+           dérivées aux neuf jalons pour la configuration historique (gris clair)
            et pour la configuration optimisée (orange, grille~v3), ainsi que la courbe dense optimisée.}
   \label{fig:comparaison_deriveeP}
 \end{figure}

-Cette comparaison met en évidence la réduction des écarts sur \(\dot P\)
-au niveau de la transition logistique, confirmant l’atténuation du pic
+Cette comparaison met en évidence la réduction des écarts sur \(\dot P\)
+au niveau de la transition logistique, confirmant l’atténuation du pic
 et la cohérence physique de l’optimisation v3.

 \subsection{Synthèse des données finales}

-La calibration de \(P(T)\) sur les neuf jalons confirme un écart relatif
-inférieur à \(1\%\), validant la cohérence de l’ajustement paramétrique optimisé.
+La calibration de \(P(T)\) sur les neuf jalons confirme un écart relatif
+inférieur à \(1\%\), validant la cohérence de l’ajustement paramétrique optimisé.

 \subsection{Glossaire}

 \begin{description}
-  \item[$T$] Âge de l’Univers, en milliards d’années (\(\mathrm{Gyr}\)).
-  \item[$T_{0}$] Âge initial de normalisation (\(10^{-6}\)\,\(\mathrm{Gyr}\)).
-  \item[$\alpha_{\mathrm{log}}(T)$] Exposant logistique pur,
-    \(\displaystyle \alpha_{0}
+  \item[$T$] Âge de l’Univers, en milliards d’années (\(\mathrm{Gyr}\)).
+  \item[$T_{0}$] Âge initial de normalisation (\(10^{-6}\)\,\(\mathrm{Gyr}\)).
+  \item[$\alpha_{\mathrm{log}}(T)$] Exposant logistique pur,
+    \(\displaystyle \alpha_{0}
       + \frac{1-\alpha_{0}}{1 + e^{-(T - T_{c})/\Delta}}\).
-  \item[$P(T)$] Fonction propre du temps,
+  \item[$P(T)$] Fonction propre du temps,
   normalisée à \(P(T_{0})=1\) et obtenue par intégration de \(\dot P(T)\).
-  \item[Calibration logistique] Ajustement paramétrique de \(P(T)\) sur neuf jalons (grille~v3).
-  \item[$\alpha(T)$] Exposant complet intégrant le plateau précoce,
-    \(\displaystyle \alpha_{\mathrm{log}}(T)\bigl[1 - e^{-(T/T_{p})^{2}}\bigr]\),
-    avec \(T_{p}/T_{c}=0.14\) (grille~v3).
-  \item[$T_{c},\,\Delta$] Paramètres central et largeur de la transition logistique (Gyr).
-  \item[$T_{p}$] Paramètre du plateau précoce (seul paramètre associé au plateau).
-  \item[$\dot P(T)$] Dérivée de \(P\) :
+  \item[Calibration logistique] Ajustement paramétrique de \(P(T)\) sur neuf jalons (grille~v3).
+  \item[$\alpha(T)$] Exposant complet intégrant le plateau précoce,
+    \(\displaystyle \alpha_{\mathrm{log}}(T)\bigl[1 - e^{-(T/T_{p})^{2}}\bigr]\),
+    avec \(T_{p}/T_{c}=0.14\) (grille~v3).
+  \item[$T_{c},\,\Delta$] Paramètres central et largeur de la transition logistique (Gyr).
+  \item[$T_{p}$] Paramètre du plateau précoce (seul paramètre associé au plateau).
+  \item[$\dot P(T)$] Dérivée de \(P\) :
     \[
       \dot P(T)
       = \alpha(T)\,T^{\alpha(T)-1}
       + T^{\alpha(T)}\,\ln T\,\frac{d\alpha}{dT}.
     \]
   \item[$I_{1}(T)=P(T)/T$] Invariant adimensionnel décrivant l’évolution relative de \(P(T)\) (cf. Chapitre 4 pour \(I_{2}\) et \(I_{3}\)).
-  \item[$\delta H_{0}/H_{0}$] Correction relative de la constante de Hubble à la recombinaison,
-    \(\displaystyle\frac{\delta H_{0}}{H_{0}}\approx9.2\times10^{-4}.\)
-\end{description}
\ No newline at end of file
+  \item[$\delta H_{0}/H_{0}$] Correction relative de la constante de Hubble à la recombinaison,
+    \(\displaystyle\frac{\delta H_{0}}{H_{0}}\approx9.2\times10^{-4}.\)
+\end{description}
diff --git a/01-introduction-applications/01_introduction_conceptuel.tex b/01-introduction-applications/01_introduction_conceptuel.tex
index d9ca0e8..707e858 100755
--- a/01-introduction-applications/01_introduction_conceptuel.tex
+++ b/01-introduction-applications/01_introduction_conceptuel.tex
@@ -10,8 +10,8 @@ Dans le Modèle de Courbe Gravitationnelle du Temps (MCGT), on module l’évolu
 \]
 où :
 \begin{itemize}
-  \item \(\alpha_{0}\) est l’exposant initial (\(T\ll T_{c}\)),
-  \item \(T_{c}\) est l’âge central de la transition,
+  \item \(\alpha_{0}\) est l’exposant initial (\(T\ll T_{c}\)),
+  \item \(T_{c}\) est l’âge central de la transition,
   \item \(\Delta\) est la largeur de la zone de transition.
 \end{itemize}

@@ -27,7 +27,7 @@ où :
 \end{center}
 Les valeurs proviennent de l’optimisation \texttt{least\_squares} (cf. Tab.~\ref{tab:parametres_opt}).

-Pour tenir compte d’un plateau précoce, on introduit le facteur
+Pour tenir compte d’un plateau précoce, on introduit le facteur
 \[
   1 - \exp\!\bigl(-(T/T_{p})^{2}\bigr),
 \]
@@ -37,7 +37,7 @@ de sorte que l’exposant complet s’écrit :
   = \alpha_{\mathrm{log}}(T)\,\bigl(1 - \exp\!\bigl(-(T/T_{p})^{2}\bigr)\bigr).
 \]

-On normalise la fonction propre du temps par
+On normalise la fonction propre du temps par
 \[
   P(T_{0}) = 1
   \quad\bigl(\text{plateau initial de normalisation, }T_{0}=10^{-6}\,\mathrm{Gyr}\bigr).
@@ -63,29 +63,29 @@ On a par ailleurs :
   + \alpha_{\mathrm{log}}(T)\,\frac{2T}{T_{p}^{2}}\,e^{-(T/T_{p})^{2}}.
 \]

-Les neuf jalons \((T_i,P_{\mathrm{ref}})\) employés pour l’optimisation sont récapitulés dans la Table~\ref{tab:jalons_ecarts}.
-Les valeurs exactes sont dans le fichier
-\texttt{zz-data/chapter01/01\_chronologie\_resultats.csv}.
+Les neuf jalons \((T_i,P_{\mathrm{ref}})\) employés pour l’optimisation sont récapitulés dans la Table~\ref{tab:jalons_ecarts}.
+Les valeurs exactes sont dans le fichier
+\texttt{zz-data/chapter01/01\_chronologie\_resultats.csv}.

-Le script \texttt{zz-scripts/chapter01/trace\_fig01.py} importe ces jalons
+Le script \texttt{zz-scripts/chapter01/trace\_fig01.py} importe ces jalons
 et génère la figure suivante :

 \begin{figure}[htbp]
   \centering
   \includegraphics[width=\linewidth]{zz-figures/chapter01/fig_01_plateau_precoce.png}
-  \caption{Fig.~01 – Plateau précoce de \(P(T)\) (grille~v3) :
-           intégration complète de \(\dot P(T)\) avec les paramètres de la Table~\ref{tab:parametres_opt};
+  \caption{Fig.~01 – Plateau précoce de \(P(T)\) (grille~v3) :
+           intégration complète de \(\dot P(T)\) avec les paramètres de la Table~\ref{tab:parametres_opt};
            repère vertical à \(T_{p}\simeq0.087\,\mathrm{Gyr}\).}
   \label{fig:plateau_precoce}
 \end{figure}

-Pour \(T\ll T_{p}\), \(\dot P\approx0\) et donc \(P(T)\approx1\),
-alors que pour \(T\gtrsim T_{p}\), l’intégration de \(\dot P\) relance la croissance selon la loi logistique.
-Dans la limite \(T\to+\infty\), \(\alpha\to1\) et \(P(T)\sim T\), assurant \(I_{1}\to1\) conformément à la relativité générale.
+Pour \(T\ll T_{p}\), \(\dot P\approx0\) et donc \(P(T)\approx1\),
+alors que pour \(T\gtrsim T_{p}\), l’intégration de \(\dot P\) relance la croissance selon la loi logistique.
+Dans la limite \(T\to+\infty\), \(\alpha\to1\) et \(P(T)\sim T\), assurant \(I_{1}\to1\) conformément à la relativité générale.

 \subsection{Motivations fondamentales et portée multidisciplinaire}

-\paragraph{Limites du modèle \(\Lambda\)CDM}
+\paragraph{Limites du modèle \(\Lambda\)CDM}
 Le modèle standard \(\Lambda\)CDM, malgré son succès, présente plusieurs tensions :
 \begin{itemize}
   \item Formation précoce de galaxies massives (\(T\sim0.3\) Gyr) détectées par le JWST (JADES ; CEERS) \cite{JWST:JADES,JWST:CEERS}.
@@ -107,32 +107,32 @@ Grâce à l’intégration de \(\dot P(T)\) et au couplage sombre modéré\cite{
 où \(a=(1+z)^{-1}\).

 \begin{itemize}
-  \item En \(\Lambda\mathrm{CDM}\) :
-    \(H_{0}^{\rm CMB}\approx67\)\,\(\mathrm{km\,s^{-1}\,Mpc^{-1}}\),
-    \(H_{0}^{\rm loc}\approx73\)\,\(\mathrm{km\,s^{-1}\,Mpc^{-1}}\),
+  \item En \(\Lambda\mathrm{CDM}\) :
+    \(H_{0}^{\rm CMB}\approx67\)\,\(\mathrm{km\,s^{-1}\,Mpc^{-1}}\),
+    \(H_{0}^{\rm loc}\approx73\)\,\(\mathrm{km\,s^{-1}\,Mpc^{-1}}\),
     soit un décalage d’environ 6 \%.
-  \item Sous MCGT, l’écart résiduel passe de 6 \% à 2.1 \% (grille v3),
+  \item Sous MCGT, l’écart résiduel passe de 6 \% à 2.1 \% (grille v3),
     atténuant significativement la tension.
-  \item L’introduction de couplages additionnels ou de degrés de liberté dynamiques
-    dans \(\dot P(T)\) pourrait encore accroître \(\delta H_{0}\) et tendre vers
+  \item L’introduction de couplages additionnels ou de degrés de liberté dynamiques
+    dans \(\dot P(T)\) pourrait encore accroître \(\delta H_{0}\) et tendre vers
     une résolution complète de la tension.
 \end{itemize}

-Les valeurs chiffrées proviennent du fichier
+Les valeurs chiffrées proviennent du fichier
 \texttt{12-donnees/chapitre1/01\_H0\_tension\_MCGT.csv}.

-\paragraph{Applications et domaines d’intérêt}
+\paragraph{Applications et domaines d’intérêt}
 \begin{itemize}
-  \item Cosmologie précoce : modulation de la croissance accélérée des structures
+  \item Cosmologie précoce : modulation de la croissance accélérée des structures
         explicative des objets détectés par le JWST.
-  \item Ondes gravitationnelles : correction du décalage temporel et de phase
+  \item Ondes gravitationnelles : correction du décalage temporel et de phase
         dans l’analyse des signaux LIGO/Virgo et futurs signaux LISA.
-  \item Couplage sombre : réconciliation partielle des mesures SNIa, BAO
+  \item Couplage sombre : réconciliation partielle des mesures SNIa, BAO
         et \(H_{0}\) (voir Chapitre 8).
 \end{itemize}

-Les chapitres 2 à 4 détaillent la validation observationnelle,
-tandis que le Chapitre 8 explore l’extension du couplage sombre.
+Les chapitres 2 à 4 détaillent la validation observationnelle,
+tandis que le Chapitre 8 explore l’extension du couplage sombre.

 \subsection{Principes structurants du MCGT}

@@ -142,18 +142,18 @@ tandis que le Chapitre 8 explore l’extension du couplage sombre.

 Le MCGT s’inscrit dans la classe des modifications \(f(R)\) de forme minimale
 \[
-  f(R) = R + \beta\,R^{n},
+  f(R) = R + \beta\,R^{n},
   \quad n>1,
 \]
-avec \(\beta\) et \(n\) ajustés par best-fit (Tab.~\ref{tab:FR_fit}) \cite{MCGT_FR_analysis}.
+avec \(\beta\) et \(n\) ajustés par best-fit (Tab.~\ref{tab:FR_fit}) \cite{MCGT_FR_analysis}.
 Les dérivées sont notées :
 \[
   f_{R}\equiv\frac{\mathrm d f}{\mathrm d R},
   \quad
   f_{RR}\equiv\frac{\mathrm d^{2} f}{\mathrm d R^{2}}.
 \]
-La validité de ce modèle couvre la gamme de courbure
-\(\lvert R\rvert\in[10^{-5},10^{4}]\,H_{0}^{2}\).
+La validité de ce modèle couvre la gamme de courbure
+\(\lvert R\rvert\in[10^{-5},10^{4}]\,H_{0}^{2}\).
 On définit l’invariant scalaire
 \[
   m_{s}^{2}(R)
@@ -163,8 +163,8 @@ Les conditions de stabilité linéaire sont alors :
 \[
   f_{R}(R) > 0,\quad f_{RR}(R) > 0,\quad m_{s}^{2}(R) > 0,
 \]
-cette dernière assurant l’absence de tachyon et la stabilité du mode scalaire associé.
-Les détails chiffrés et les tracés se trouvent au Chapitre 3 \cite{MCGT_FR_analysis}.
+cette dernière assurant l’absence de tachyon et la stabilité du mode scalaire associé.
+Les détails chiffrés et les tracés se trouvent au Chapitre 3 \cite{MCGT_FR_analysis}.

 \subsubsection{Couplage sombre modéré}

@@ -175,18 +175,18 @@ Pour réconcilier les mesures de \(H_{0}\) et tenir compte des observations SNIa
   \dot{\rho}_{\phi} + 3H\,(1+w_{\phi})\,\rho_{\phi} = +Q_{0}\,H_{0}\,\rho_{m},
 \quad w_{\phi}=-1,
 \]
-où
+où
 \[
-  Q_{0}\in[0.0,0.2]\quad\text{(adimensionné)},
-  \quad
+  Q_{0}\in[0.0,0.2]\quad\text{(adimensionné)},
+  \quad
   H_{0}=67.4\,\mathrm{km\,s^{-1}\,Mpc^{-1}}\;(\text{Planck 2018}).
 \]
-Le meilleur ajustement donne
+Le meilleur ajustement donne
 \[
   Q_{0}^{\mathrm{best}} = 0.04 \pm 0.02
   \quad(\text{voir Tab.~\ref{tab:Q0_fit}})\cite{Smith2024_CouplageSombre}.
 \]
-Les calculs détaillés, la structure des données et les résultats se trouvent au Chapitre 8.
+Les calculs détaillés, la structure des données et les résultats se trouvent au Chapitre 8.

 \subsection{Renvois et perspectives}\label{sec:renvois}

@@ -201,4 +201,4 @@ guide de chaque chapitre.} :
 \end{itemize}

 \smallskip
-\noindent\emph{Le volet conceptuel du Chapitre 1 s’achève à la section précédente. La partie qui suit présente uniquement les applications chiffrées et la calibration logistique.}
\ No newline at end of file
+\noindent\emph{Le volet conceptuel du Chapitre 1 s’achève à la section précédente. La partie qui suit présente uniquement les applications chiffrées et la calibration logistique.}
diff --git a/01-introduction-applications/CHAPTER01_GUIDE.txt b/01-introduction-applications/CHAPTER01_GUIDE.txt
index 2c1f897..d006da5 100755
--- a/01-introduction-applications/CHAPTER01_GUIDE.txt
+++ b/01-introduction-applications/CHAPTER01_GUIDE.txt
@@ -137,4 +137,3 @@ Introduction & applications (MCGT)
 11. Compilation LaTeX
    pdflatex -jobname=chap1_conceptuel 01-introduction-applications/01_introduction_conceptuel.tex
    pdflatex -jobname=chap1_applications 01-introduction-applications/01_applications_calibration_conceptuel.tex
-
diff --git a/02-validation-chronologique/02_validation_chronologique_conceptuel.tex b/02-validation-chronologique/02_validation_chronologique_conceptuel.tex
index d067f99..0581c7a 100755
--- a/02-validation-chronologique/02_validation_chronologique_conceptuel.tex
+++ b/02-validation-chronologique/02_validation_chronologique_conceptuel.tex
@@ -13,7 +13,7 @@ issue de l’intégration complète de
   = \alpha(T)\,T^{\alpha(T)-1}
   + T^{\alpha(T)}\,\ln T\,\frac{\mathrm d\alpha}{\mathrm dT},
 \]
-Pour la forme analytique de $\alpha(T)$ et de $\mathrm d\alpha/\mathrm dT$, voir la section « Cadre théorique et paramétrisation logistique » (Chapitre 1).
+Pour la forme analytique de $\alpha(T)$ et de $\mathrm d\alpha/\mathrm dT$, voir la section « Cadre théorique et paramétrisation logistique » (Chapitre 1).
 coïncide avec la valeur de référence \(P_{\rm ref}(T_i)\) obtenue par calibration fine.

 Pour la forme analytique de $\alpha(T)$ et de $\mathrm d\alpha/\mathrm dT$, voir la section « Cadre théorique et paramétrisation logistique » (Chapitre 1).
@@ -24,11 +24,11 @@ On définit la fonction de coût
   = \sum_{i=1}^{9}
     \bigl[P_{\rm calc}(T_{i}) - P_{\rm ref}(T_{i})\bigr]^{2},
 \]
-et on ajuste les paramètres
+et on ajuste les paramètres
 \(\alpha_{0},\,T_{c},\,\Delta,\,T_{p}\)
-pour minimiser \(\chi^{2}\).
+pour minimiser \(\chi^{2}\).

-Les âges clés et leurs valeurs de référence sont listés dans
+Les âges clés et leurs valeurs de référence sont listés dans
 \texttt{zz-data/chapter02/02\_chronologie\_resultats.csv}.

 \begin{table}[htbp]
@@ -55,7 +55,7 @@ Pour visualiser la concordance entre la modélisation et les points de référen

 \subsection{Critères d’acceptation}

-Les paramètres sont validés si l’écart relatif maximal
+Les paramètres sont validés si l’écart relatif maximal
 \[
   \max_{i}\,\varepsilon(T_{i})
   = \max_{i}\,\frac{\lvert P_{\rm calc}(T_{i}) - P_{\rm ref}(T_{i})\rvert}{P_{\rm ref}(T_{i})}
@@ -65,7 +65,7 @@ Ce critère sera démontré numériquement dans la section « Résultats numéri

 \subsection{Conclusion conceptuelle}

-La fonction \(P(T)\), combinée à la comparaison systématique sur neuf âges clés, constitue la validation chronologique de base.
+La fonction \(P(T)\), combinée à la comparaison systématique sur neuf âges clés, constitue la validation chronologique de base.
 Pour passer des définitions à la mise en œuvre concrète — c’est-à-dire pour consulter la structure exacte des fichiers de données, les extraits de CSV, les légendes des figures et les résultats numériques détaillés — se reporter à :

 \begin{center}
diff --git a/02-validation-chronologique/02_validation_chronologique_details.tex b/02-validation-chronologique/02_validation_chronologique_details.tex
index f8837d1..648985a 100755
--- a/02-validation-chronologique/02_validation_chronologique_details.tex
+++ b/02-validation-chronologique/02_validation_chronologique_details.tex
@@ -4,7 +4,7 @@ La liste détaillée des fichiers de données (formats CSV et DAT) et leur descr

 \subsection{Script et mise en œuvre}

-Les dépendances Python nécessaires sont listées dans \texttt{zz-scripts/chapter02/requirements.txt}.
+Les dépendances Python nécessaires sont listées dans \texttt{zz-scripts/chapter02/requirements.txt}.
 Pour les installer, exécuter :
 \begin{verbatim}
 pip install -r zz-scripts/chapter02/requirements.txt
@@ -26,12 +26,12 @@ python zz-scripts/chapter02/trace_fig04_schema_calibration.py
 # Durée : quelques secondes par script
 \end{verbatim}

-\noindent\textbf{Note :} le script \texttt{integration\_validation\_chronologie.py} normalise la fonction propre en fixant
+\noindent\textbf{Note :} le script \texttt{integration\_validation\_chronologie.py} normalise la fonction propre en fixant
 \(\displaystyle P\bigl(T_{0}=10^{-6}\,\mathrm{Gyr}\bigr)=1\).

 \begin{mdframed}
   \paragraph{Encadré — Exécution et résultats}
-  Après exécution des quatre scripts, le maximum d’écart relatif atteint est
+  Après exécution des quatre scripts, le maximum d’écart relatif atteint est
   \[
     \max_{i}\,\varepsilon(T_i)\approx0{,}083\%\;<1\%.
   \]
@@ -50,10 +50,10 @@ Avant de présenter les résultats numériques, illustrons le fonctionnement gé
 \begin{figure}[htbp]
   \centering
   \includegraphics{zz-figures/chapter02/fig_04_schema_chaine_calibration.png}
-  \caption{Chaîne de calibration chronologique. Ce diagramme, généré par \texttt{zz-scripts/chapter02/trace\_fig04\_schema\_calibration.py}, décrit :
-    (i) lecture des neuf jalons \((T_i,P_{\rm ref})\),
-    (ii) génération du tableau dense \(P(T)\) (\texttt{02\_P\_lin\_vs\_T.dat}),
-    (iii) optimisation de \(\chi^2\), et
+  \caption{Chaîne de calibration chronologique. Ce diagramme, généré par \texttt{zz-scripts/chapter02/trace\_fig04\_schema\_calibration.py}, décrit :
+    (i) lecture des neuf jalons \((T_i,P_{\rm ref})\),
+    (ii) génération du tableau dense \(P(T)\) (\texttt{02\_P\_lin\_vs\_T.dat}),
+    (iii) optimisation de \(\chi^2\), et
     (iv) sélection du meilleur jeu de paramètres.}
   \label{fig:schema_chaine_calibration}
 \end{figure}
@@ -63,29 +63,29 @@ Avant de présenter les résultats numériques, illustrons le fonctionnement gé
 Les résultats de la validation chronologique sont illustrés par trois figures, générées dans \texttt{zz-figures/chapter02/} :

 \begin{itemize}
-  \item Figure \ref{fig:p_vs_t_calibration_ch2} :
-    trace dense de \(P_{\rm calc}(T)\) vs \(T\) (échelle log–log) avec superposition des neuf points de référence.
+  \item Figure \ref{fig:p_vs_t_calibration_ch2} :
+    trace dense de \(P_{\rm calc}(T)\) vs \(T\) (échelle log–log) avec superposition des neuf points de référence.
     Fichier : \texttt{fig\_01\_P\_vs\_T.png}.
-
-  \item Figure \ref{fig:calibration_9points} :
-    diagramme de calibration (échelle log–log) : scatter des neuf jalons avec barres d’erreur \(\pm1\%\) et courbe modèle.
+
+  \item Figure \ref{fig:calibration_9points} :
+    diagramme de calibration (échelle log–log) : scatter des neuf jalons avec barres d’erreur \(\pm1\%\) et courbe modèle.
     Fichier : \texttt{fig\_02\_calibration\_9points.png}.
-
-  \item Figure \ref{fig:ecart_relatif_chronologie_ch2} :
-    écart relatif \(\varepsilon(T_i)\) vs \(T_i\) en échelle log–log, confirmant tous les écarts \(<1\%\).
+
+  \item Figure \ref{fig:ecart_relatif_chronologie_ch2} :
+    écart relatif \(\varepsilon(T_i)\) vs \(T_i\) en échelle log–log, confirmant tous les écarts \(<1\%\).
     Fichier : \texttt{fig\_03\_ecart\_relatif.png}.
 \end{itemize}

 \subsection{Résultats numériques et cohérence}

-Les écarts relatifs
+Les écarts relatifs
 \[
   \varepsilon(T_{i})
   = \frac{\bigl|P_{\rm calc}(T_{i}) - P_{\rm ref}(T_{i})\bigr|}
          {P_{\rm ref}(T_{i})}\times100\%
 \]
-restent inférieurs à 1 % aux neuf âges clés, confirmant la réussite de la calibration.
-Les valeurs numériques complètes sont disponibles dans
+restent inférieurs à 1 % aux neuf âges clés, confirmant la réussite de la calibration.
+Les valeurs numériques complètes sont disponibles dans
 \texttt{zz-data/chapter02/02\_ecart\_relatif\_chronologie.csv}.

 \subsubsection*{Discussion rapide des erreurs systématiques}
@@ -93,49 +93,49 @@ Les valeurs numériques complètes sont disponibles dans
 Les principales sources d’incertitude dans cette validation chronologique sont :

 \begin{itemize}
-  \item \textbf{Intégration trapézoïdale sur grille log :}
-    le calcul de \(P(T)\) par intégration trapézoïdale est effectué sur une grille logarithmique de 800 points entre \(10^{-3}\) et \(14\) Gyr.
+  \item \textbf{Intégration trapézoïdale sur grille log :}
+    le calcul de \(P(T)\) par intégration trapézoïdale est effectué sur une grille logarithmique de 800 points entre \(10^{-3}\) et \(14\) Gyr.
     L’erreur numérique est de l’ordre de \(\mathcal{O}\bigl((\Delta \ln T)^2\bigr)\approx10^{-5}\), contrôlée par la densité de la grille.

-  \item \textbf{Précision des âges clés :}
+  \item \textbf{Précision des âges clés :}
     un décalage de \(\pm1\%\) sur un jalon \(T_i\) se traduit par une variation de \(\varepsilon(T_i)\) d’environ \(\pm0{,}1\%\).

-  \item \textbf{Paramètres de plateau précoce et transition :}
-    le choix des paramètres \(T_{p}\) et \(\Delta\) module la forme de \(\dot P(T)\) autour de la transition logistique.
+  \item \textbf{Paramètres de plateau précoce et transition :}
+    le choix des paramètres \(T_{p}\) et \(\Delta\) module la forme de \(\dot P(T)\) autour de la transition logistique.
     Leur estimation peut biaiser localement \(P(T)\), avec un effet global généralement inférieur à \(0{,}2\%\) sur \(\varepsilon_{\max}\).

-  \item \textbf{Erreur d’interpolation :}
+  \item \textbf{Erreur d’interpolation :}
     lors de l’évaluation de \(P_{\rm calc}(T_i)\) sur les jalons, l’interpolation linéaire en \(\log T\) introduit une imprécision de l’ordre de \(10^{-4}\).

-  \item \textbf{Artefact numérique et seuil minimal :}
+  \item \textbf{Artefact numérique et seuil minimal :}
     pour \(T<10^{-3}\) Gyr, on fixe \(T_{\min}=10^{-3}\) Gyr comme borne inférieure pour éviter les instabilités en double précision.
 \end{itemize}

 \subsection{Conclusion détaillée}

-\noindent\textbf{Rappel :} le critère d’acceptation impose
+\noindent\textbf{Rappel :} le critère d’acceptation impose
 \(\max_{i}\,\varepsilon(T_i)<1\%\).

-La comparaison entre \(P_{\rm calc}(T)\) et \(P_{\rm ref}(T)\) sur l’ensemble des neuf âges clés valide la fonction propre avec une précision meilleure que \(1\%\), depuis l’ère planckienne (\(T_{0}=10^{-6}\,\mathrm{Gyr}\)) jusqu’à \(T=13{,}8\) Gyr.
+La comparaison entre \(P_{\rm calc}(T)\) et \(P_{\rm ref}(T)\) sur l’ensemble des neuf âges clés valide la fonction propre avec une précision meilleure que \(1\%\), depuis l’ère planckienne (\(T_{0}=10^{-6}\,\mathrm{Gyr}\)) jusqu’à \(T=13{,}8\) Gyr.
 Cette validation chronologique, à la fois robuste et précise, constitue la base solide pour les analyses suivantes : BBN (Chapitre 5), CMB (Chapitre 6) et perturbations scalaires (Chapitre 7).

 \subsubsection*{Transition vers le Chapitre 3}

-Les paramètres logistiques optimisés et validés ici seront désormais réutilisés pour étudier l’extension \(f(R)\) du modèle et analyser la stabilité linéaire des solutions, telles que développées dans le Chapitre 3.
+Les paramètres logistiques optimisés et validés ici seront désormais réutilisés pour étudier l’extension \(f(R)\) du modèle et analyser la stabilité linéaire des solutions, telles que développées dans le Chapitre 3.

 \subsection{Glossaire}
 \begin{itemize}
   \item $T_i$ : neuf âges clés de l’Univers (Gyr), définis dans \texttt{zz-data/chapter02/02\_chronologie\_resultats.csv}.
-  \item $P_{\rm calc}(T)$ : fonction propre du temps calculée par intégration de $\dot P(T)$,
+  \item $P_{\rm calc}(T)$ : fonction propre du temps calculée par intégration de $\dot P(T)$,
     \[
       P_{\rm calc}(T)=T^{\alpha(T)}.
     \]
   \item $P_{\rm ref}(T_i)$ : valeur de référence de la fonction propre aux âges clés, tirée de \texttt{zz-data/chapter02/02\_chronologie\_resultats.csv}.
-  \item $\chi^2$ : fonction de coût pour la calibration,
+  \item $\chi^2$ : fonction de coût pour la calibration,
     \[
       \chi^2 = \sum_{i=1}^{9}\bigl[P_{\rm calc}(T_i)-P_{\rm ref}(T_i)\bigr]^{2}.
     \]
-  \item $\varepsilon(T_i)$ : écart relatif en pourcentage,
+  \item $\varepsilon(T_i)$ : écart relatif en pourcentage,
     \[
       \varepsilon(T_i)
       = \frac{\lvert P_{\rm calc}(T_i)-P_{\rm ref}(T_i)\rvert}
@@ -148,4 +148,4 @@ Les paramètres logistiques optimisés et validés ici seront désormais réutil
 \end{itemize}

 \bigskip
-\noindent\emph{Fin de la partie détaillée, Chapitre 2.}
\ No newline at end of file
+\noindent\emph{Fin de la partie détaillée, Chapitre 2.}
diff --git a/02-validation-chronologique/CHAPTER02_GUIDE.txt b/02-validation-chronologique/CHAPTER02_GUIDE.txt
index 831cc30..2876760 100755
--- a/02-validation-chronologique/CHAPTER02_GUIDE.txt
+++ b/02-validation-chronologique/CHAPTER02_GUIDE.txt
@@ -58,9 +58,9 @@ Ce guide décrit la production des données, des figures et la procédure d’ex
 | `classe`| Catégorie d’exigence : `primary` (≤ 1 %) / `order2` (≤ 10 %) | —     |

 ### 3.2 Spécification du spectre primordial — `02_primordial_spectrum_spec.json`
-Champs :
-- `label_eq`, `formula`, `description`
-- `constants` : `A_s0`, `ns0`
+Champs :
+- `label_eq`, `formula`, `description`
+- `constants` : `A_s0`, `ns0`
 - `coefficients` : `c1`, `c1_2`, `c2`, `c2_2`

 > Règle d’homogénéisation : les noms de champs et l’usage de la notation \(P_R(k;\alpha)\) sont **alignés** sur les chapitres 1 et 6 (paramètres cosmologiques globaux dans `mcgt-global-config.ini`).
@@ -97,10 +97,10 @@ Champs :
 | `epsilon_i` | Écart relatif aux jalons                  |

 ### 4.5 Paramètres optimaux — `02_optimal_parameters.json`
-Champs :
-- `T_split_Gyr`
-- `segments.low|high` : `alpha0`, `alpha_inf`, `Tc`, `Delta`, `Tp`
-- `thresholds` : `primary`, `order2`
+Champs :
+- `T_split_Gyr`
+- `segments.low|high` : `alpha0`, `alpha_inf`, `Tc`, `Delta`, `Tp`
+- `thresholds` : `primary`, `order2`
 - `max_epsilon_primary`, `max_epsilon_order2`

 ### 4.6 Échantillonnage du spectre — `02_P_R_sampling.csv`
@@ -127,12 +127,12 @@ Champs :
 ---

 ## 5. Figures produites
-- `fig_00_spectrum.png` : spectre primordial MCGT (log–log).
-- `fig_01_P_vs_T_evolution.png` : \(P_{\rm calc}(T)\) vs jalons \(P_{\rm ref}(T_i)\).
-- `fig_02_calibration.png` : nuage \(P_{\rm ref}\) vs \(P_{\rm calc}\) + ligne d’identité.
-- `fig_03_relative_errors.png` : \(\epsilon_i\) vs \(T\) (échelle adaptée).
-- `fig_04_pipeline_diagram.png` : schéma du pipeline de calcul.
-- `fig_05_FG_series.png` : séries \(F(\alpha)-1\) et \(G(\alpha)\).
+- `fig_00_spectrum.png` : spectre primordial MCGT (log–log).
+- `fig_01_P_vs_T_evolution.png` : \(P_{\rm calc}(T)\) vs jalons \(P_{\rm ref}(T_i)\).
+- `fig_02_calibration.png` : nuage \(P_{\rm ref}\) vs \(P_{\rm calc}\) + ligne d’identité.
+- `fig_03_relative_errors.png` : \(\epsilon_i\) vs \(T\) (échelle adaptée).
+- `fig_04_pipeline_diagram.png` : schéma du pipeline de calcul.
+- `fig_05_FG_series.png` : séries \(F(\alpha)-1\) et \(G(\alpha)\).
 - `fig_06_fit_alpha.png` : ajustements \(A_s(\alpha)\) et \(n_s(\alpha)\).

 ---
@@ -181,4 +181,3 @@ python zz-scripts/chapter02/plot_fig06_alpha_fit.py

 * JSON : `zz-schemas/02_optimal_parameters.schema.json`, `zz-schemas/02_spec_spectrum.schema.json`
 * CSV : gabarit tabulaire `zz-schemas/mc_results_table_schema.json` et schémas spécifiques au chapitre
-
diff --git a/03-stabilite-fR/03_stabilite_fR_conceptuel.tex b/03-stabilite-fR/03_stabilite_fR_conceptuel.tex
index 27ca487..064f7bc 100755
--- a/03-stabilite-fR/03_stabilite_fR_conceptuel.tex
+++ b/03-stabilite-fR/03_stabilite_fR_conceptuel.tex
@@ -15,12 +15,12 @@ Les données chiffrées, les tableaux et les tracés correspondants sont compil
 Nous commençons par rappeler deux définitions-clés :

 \begin{itemize}
-  \item \textbf{Fonction de Hubble}
+  \item \textbf{Fonction de Hubble}
     \[
       H(z) \;=\; H_0\,\sqrt{\Omega_m(1+z)^3 + \Omega_r(1+z)^4 + \Omega_\Lambda}\,.
     \]
     où \(\Omega_m\), \(\Omega_r\) et \(\Omega_\Lambda\) sont respectivement les densités réduites de matière, radiation et constante cosmologique, satisfaisant \(\Omega_m+\Omega_r+\Omega_\Lambda=1\).
-  \item \textbf{Scalaire de Ricci} en fonction du redshift
+  \item \textbf{Scalaire de Ricci} en fonction du redshift
     \[
       R(z)
       =6\Bigl[2H^2(z)+(1+z)\,H(z)\,\frac{dH}{dz}\Bigr].
@@ -36,8 +36,8 @@ Nous définissons ensuite la quantité de référence

 À partir de ces définitions, l’extension considérée s’écrit sous la forme polynomiale :
 \[
-  f(R) \;=\; R
-    \;+\; \frac{\beta}{2\,R_{0}}\,(R - R_{0})^{2}
+  f(R) \;=\; R
+    \;+\; \frac{\beta}{2\,R_{0}}\,(R - R_{0})^{2}
     \;+\; \frac{\gamma}{3\,R_{0}^{2}}\,(R - R_{0})^{3}.
 \]

@@ -64,14 +64,14 @@ qui satisfont simultanément les contraintes CMB, solaires et assurent une robus
 \subsection{Dérivées et masse scalaire}
 On note
 \[
-  f_{R}(R) \;=\; \frac{d f}{dR},
+  f_{R}(R) \;=\; \frac{d f}{dR},
   \qquad
   f_{RR}(R) \;=\; \frac{d^{2} f}{dR^{2}},
 \]
 et l’on définit la masse scalaire associée par :
 \[
-  m_{s}^{2}(R)
-  \;=\;
+  m_{s}^{2}(R)
+  \;=\;
   \frac{\,f_{R}(R) \;-\; R\,f_{RR}(R)\,}{3\,f_{RR}(R)}.
 \]
 Ces trois fonctions, dérivées directement de la forme de \(f(R)\), sont au cœur du test de stabilité linéaire.
@@ -87,10 +87,10 @@ Sur toute la plage cosmologique d’intérêt \(0 \le z \le 1000\), on vérifie
   \frac{m_{s}^{2}\bigl(R(z)\bigr)}{R_{0}} \;>\; 0.
 \]
 \begin{itemize}
-  \item \(f_{R}(R)>0\) garantit l’absence de ghost.
-  \item \(f_{RR}(R)>0\) assure la convexité de \(f\).
-  \item \(\dfrac{m_{s}^{2}(R)}{R_{0}}>0\) exclut les modes tachyoniques et correspond à la quantité tracée dans les figures (fig.~\ref{fig:ms2_vs_z}).
-\end{itemize}
+  \item \(f_{R}(R)>0\) garantit l’absence de ghost.
+  \item \(f_{RR}(R)>0\) assure la convexité de \(f\).
+  \item \(\dfrac{m_{s}^{2}(R)}{R_{0}}>0\) exclut les modes tachyoniques et correspond à la quantité tracée dans les figures (fig.~\ref{fig:ms2_vs_z}).
+\end{itemize}
 (voir Glossaire pour les définitions et unités de chaque terme).

 \subsection{Sources et domaine de stabilité}
@@ -109,19 +109,19 @@ Les données et scripts sources pour reproduire les tests de stabilité sont :
 \begin{figure}[htbp]
   \centering
   \includegraphics[width=0.75\linewidth]{03-stabilite-fR/fig_01_stabilite_fR_domaine.png}
-  \caption{Domaine de stabilité linéaire de l’extension \(f(R)\) dans l’espace des paramètres \((\beta,\gamma)\), tracé en échelles logarithmiques.
-    En vert, la région stable où \(f_R>0\), \(f_{RR}>0\) et \(m_s^2>0\) pour tous les redshifts clés; en rouge, la zone instable.
-    La frontière critique (ligne pointillée) est extraite de \texttt{03\_stabilite\_fR\_frontiere.csv}.
-    Le point bleu (\(\beta=10^{-6},\,\gamma=5\times10^{-9}\)) marque les paramètres de référence.
+  \caption{Domaine de stabilité linéaire de l’extension \(f(R)\) dans l’espace des paramètres \((\beta,\gamma)\), tracé en échelles logarithmiques.
+    En vert, la région stable où \(f_R>0\), \(f_{RR}>0\) et \(m_s^2>0\) pour tous les redshifts clés; en rouge, la zone instable.
+    La frontière critique (ligne pointillée) est extraite de \texttt{03\_stabilite\_fR\_frontiere.csv}.
+    Le point bleu (\(\beta=10^{-6},\,\gamma=5\times10^{-9}\)) marque les paramètres de référence.
     Données issues de \texttt{03\_stabilite\_fR\_domaine.csv}.}
   \label{fig:stabilite_fR_domaine}
 \end{figure}

 \subsection{Conclusion conceptuelle}
-Les définitions de \(f(R)\), de ses dérivées \(f_{R}\), \(f_{RR}\) et de la masse scalaire \(m_{s}^{2}\), ainsi que les trois conditions de positivité, suffisent à poser le test de stabilité linéaire de l’extension $f(R)$.
+Les définitions de \(f(R)\), de ses dérivées \(f_{R}\), \(f_{RR}\) et de la masse scalaire \(m_{s}^{2}\), ainsi que les trois conditions de positivité, suffisent à poser le test de stabilité linéaire de l’extension $f(R)$.
 Pour visualiser les résultats numériques détaillés, les tableaux complets et les tracés, se reporter à :
 \[
   \texttt{03\_stabilite\_fR\_details.tex}.
 \]

-\noindent\emph{Fin du volet conceptuel du Chapitre 3. La partie opérationnelle détaillée commence ci-dessous.}
\ No newline at end of file
+\noindent\emph{Fin du volet conceptuel du Chapitre 3. La partie opérationnelle détaillée commence ci-dessous.}
diff --git a/03-stabilite-fR/03_stabilite_fR_details.tex b/03-stabilite-fR/03_stabilite_fR_details.tex
index aca72c3..5d8dd0d 100755
--- a/03-stabilite-fR/03_stabilite_fR_details.tex
+++ b/03-stabilite-fR/03_stabilite_fR_details.tex
@@ -62,11 +62,11 @@ Les valeurs numériques ont été obtenues de la manière suivante :
     S[table-format=1]
   }
     \toprule
-    { \(z\) }
-      & { \(\tfrac{R(z)}{R_{0}}\) }
-      & { \(f_{R}\) }
-      & { \(f_{RR}\) }
-      & { \(\tfrac{m_{s}^{2}}{R_{0}}\) }
+    { \(z\) }
+      & { \(\tfrac{R(z)}{R_{0}}\) }
+      & { \(f_{R}\) }
+      & { \(f_{RR}\) }
+      & { \(\tfrac{m_{s}^{2}}{R_{0}}\) }
       & { OK } \\
     \midrule
     0    &    1.236430     & 1.00000000 & 0.00000100 &     6.114730 & 1 \\
@@ -82,8 +82,8 @@ Les valeurs numériques ont été obtenues de la manière suivante :

 \subsubsection*{Tableau de contrôle rapide}

-Le statut \texttt{OK}, qui signifie que toutes les conditions de stabilité linéaire
-(\(f_{R}>0\), \(f_{RR}>0\), \(m_{s}^{2}/R_{0}>0\)) sont satisfaites, reste à 1 pour
+Le statut \texttt{OK}, qui signifie que toutes les conditions de stabilité linéaire
+(\(f_{R}>0\), \(f_{RR}>0\), \(m_{s}^{2}/R_{0}>0\)) sont satisfaites, reste à 1 pour
 \(z \le 200\) puis devient 0 pour \(z \ge 200\), signalant l’apparition d’instabilités linéaires au-delà de ce redshift.

 \begin{table}[htbp]
@@ -110,28 +110,28 @@ Le statut \texttt{OK}, qui signifie que toutes les conditions de stabilité lin
 Les données numériques utilisées pour vérifier les conditions de stabilité linéaire proviennent des fichiers suivants :

 \begin{itemize}
-  \item \texttt{03\_ricci\_vs\_z.csv}
-    (colonnes \texttt{z}, \texttt{R\_over\_R0} – grille fine $0\le z\le1000$ par pas $\Delta z=1$,
+  \item \texttt{03\_ricci\_vs\_z.csv}
+    (colonnes \texttt{z}, \texttt{R\_over\_R0} – grille fine $0\le z\le1000$ par pas $\Delta z=1$,
     où
     \[
     R(z)=6\bigl[2H^2(z)+(1+z)\,H(z)\,\tfrac{dH}{dz}\bigr]
     \]
     est calculé puis normalisé en \texttt{R\_over\_R0} = \(R(z)/R_0\).)
-  \item \texttt{03\_ricci\_vs\_t.csv}
-    (colonnes \texttt{T\_Gyr}, \texttt{R\_over\_R0} – grille log–uniforme de l’âge cosmique $T$ en Gyr, où $R(T)$ est calculé et normalisé par $R_0$, utilisée pour le calcul de l’invariant adimensionnel
+  \item \texttt{03\_ricci\_vs\_t.csv}
+    (colonnes \texttt{T\_Gyr}, \texttt{R\_over\_R0} – grille log–uniforme de l’âge cosmique $T$ en Gyr, où $R(T)$ est calculé et normalisé par $R_0$, utilisée pour le calcul de l’invariant adimensionnel
     \[
      I_{3}(T) \;=\; f_{R}\bigl(R(T)\bigr)\;-\;1
     \]
-    via interpolation sur \texttt{03\_ricci\_fR\_exact.csv}.)
-  \item \texttt{03\_ricci\_fR\_exact.csv}
-    (colonnes \texttt{R\_over\_R0}, \texttt{f\_R}, \texttt{f\_RR} – grille $R/R_0\in[0,10]$ par pas de $0{,}1$,
-    interpolation linéaire (via \texttt{numpy.interp}) des valeurs de \(f_R\) et \(f_{RR}\)
+    via interpolation sur \texttt{03\_ricci\_fR\_exact.csv}.)
+  \item \texttt{03\_ricci\_fR\_exact.csv}
+    (colonnes \texttt{R\_over\_R0}, \texttt{f\_R}, \texttt{f\_RR} – grille $R/R_0\in[0,10]$ par pas de $0{,}1$,
+    interpolation linéaire (via \texttt{numpy.interp}) des valeurs de \(f_R\) et \(f_{RR}\)
      pour les paramètres de référence $\beta=10^{-6}$, $\gamma=5\times10^{-9}$.)
-  \item \texttt{03\_r\_sur\_r0.csv}
+  \item \texttt{03\_r\_sur\_r0.csv}
     (colonnes \texttt{z}, \texttt{R\_over\_R0} – extrait interpolé aux redshifts clés $\{0,10,50,100,200,500,1000\}$, arrondi à 6 décimales)
-  \item \texttt{03\_stabilite\_fR\_frontiere.csv}
+  \item \texttt{03\_stabilite\_fR\_frontiere.csv}
     (colonnes \texttt{beta}, \texttt{gamma}, \texttt{OK} – liste des couples $(\beta,\gamma)$ à la frontière où OK passe de 1 à 0)
-  \item \texttt{03\_stabilite\_fR\_domaine.csv}
+  \item \texttt{03\_stabilite\_fR\_domaine.csv}
     (colonnes \texttt{beta}, \texttt{gamma}, \texttt{OK} – grille log–log de $(\beta,\gamma)$ avec OK=1 stable, incluant explicitement le point de référence $(10^{-6},5\times10^{-9})$)
 \end{itemize}

@@ -139,11 +139,11 @@ La colonne \texttt{OK} (valeur 0 ou 1) dans chaque CSV signale la validation num
 \[
   f_{R}(R)>0,\quad f_{RR}(R)>0,\quad \frac{m_{s}^{2}(R)}{R_{0}}>0
 \]
-pour les redshifts considérés.
+pour les redshifts considérés.

 \subsection{Graphiques clés}

-Les deux figures suivantes sont générées par le script \texttt{13\_tracer\_stabilite\_fR.py}
+Les deux figures suivantes sont générées par le script \texttt{13\_tracer\_stabilite\_fR.py}
 à partir des mêmes sources CSV.

 \subsubsection*{Figure 02 : $f_{R}(R)$ et $f_{RR}(R)$ en fonction de $R/R_{0}$}
@@ -151,8 +151,8 @@ Les deux figures suivantes sont générées par le script \texttt{13\_tracer\_st
 \begin{figure}[htbp]
   \centering
   \includegraphics[width=0.75\textwidth]{03-stabilite-fR/fig_02_fR_fRR_vs_R.png}
-  \caption{Tracé en échelles log–log de $f_{R}(R)$ (ocre) et $f_{RR}(R)$ (orange)
-    en fonction de $R/R_{0}$ sur l’intervalle $[10^{-1},\,10^{1}]$, données issues de \texttt{03\_ricci\_fR\_exact.csv}.
+  \caption{Tracé en échelles log–log de $f_{R}(R)$ (ocre) et $f_{RR}(R)$ (orange)
+    en fonction de $R/R_{0}$ sur l’intervalle $[10^{-1},\,10^{1}]$, données issues de \texttt{03\_ricci\_fR\_exact.csv}.
     On constate que $f_{R}(R)\gtrsim1$ et que $f_{RR}(R)>0$ sur toute la plage considérée.}
   \label{fig:fR_fRR_vs_R}
 \end{figure}
@@ -162,10 +162,10 @@ Les deux figures suivantes sont générées par le script \texttt{13\_tracer\_st
 \begin{figure}[htbp]
   \centering
   \includegraphics[width=0.75\textwidth]{03-stabilite-fR/fig_03_ms2_vs_z.png}
-  \caption{Zoom sur l’évolution de $m_{s}^{2}(z)/R_{0}$ (ordonnée en échelle log)
-    pour $0\le z\le60$. Données interpolées à partir de \texttt{03\_ricci\_vs\_z.csv}
-    et \texttt{03\_ricci\_fR\_exact.csv}. On constate que $m_{s}^{2}/R_{0}$
-    reste strictement positif pour $z\le60$, puis décline rapidement à plus grand $z$,
+  \caption{Zoom sur l’évolution de $m_{s}^{2}(z)/R_{0}$ (ordonnée en échelle log)
+    pour $0\le z\le60$. Données interpolées à partir de \texttt{03\_ricci\_vs\_z.csv}
+    et \texttt{03\_ricci\_fR\_exact.csv}. On constate que $m_{s}^{2}/R_{0}$
+    reste strictement positif pour $z\le60$, puis décline rapidement à plus grand $z$,
     indiquant la transition vers des instabilités linéaires.}
   \label{fig:ms2_vs_z}
 \end{figure}
@@ -248,46 +248,46 @@ méritent approfondissement :
 \subsection*{Glossaire}

 \begin{itemize}
-  \item \(\,R(z)\) : scalaire de Ricci, fonction du redshift \(z\) donnée par
+  \item \(\,R(z)\) : scalaire de Ricci, fonction du redshift \(z\) donnée par
     \[
       R(z)
       =6\Bigl[2H^2(z)+(1+z)\,H(z)\,\frac{dH}{dz}\Bigr],
     \]
     avec \(H(z)\) la fonction de Hubble.
-  \item \(R_{0}\) : scalaire de Ricci au redshift \(z=0\), défini et       utilisé dans sa forme simplifiée pour \(\Lambda\)CDM sans              contribution radiation :
+  \item \(R_{0}\) : scalaire de Ricci au redshift \(z=0\), défini et       utilisé dans sa forme simplifiée pour \(\Lambda\)CDM sans              contribution radiation :
     \[
     R_{0} \;\equiv\; R(0) \;=\; 12\,H_{0}^{2},
     \]
-    ce qui correspond à l’approximation
-    \(\tfrac{dH}{dz}\bigl|_{z=0}\approx -\tfrac{3}{2}\,H_{0}\,\Omega_{m}\approx0\)
-    dans un univers dominé par la constante cosmologique et la matière.
+    ce qui correspond à l’approximation
+    \(\tfrac{dH}{dz}\bigl|_{z=0}\approx -\tfrac{3}{2}\,H_{0}\,\Omega_{m}\approx0\)
+    dans un univers dominé par la constante cosmologique et la matière.
     Cette valeur est employée de façon cohérente dans tous les calculs et CSV associés.
-  \item \(\beta,\;\gamma\) : paramètres fixes de l’extension \(f(R)\), choisis typiquement
+  \item \(\beta,\;\gamma\) : paramètres fixes de l’extension \(f(R)\), choisis typiquement
     \(\beta=10^{-6},\;\gamma=5\times10^{-9}.\)
-  \item \(f(R)\) : fonction gravitationnelle modifiée,
+  \item \(f(R)\) : fonction gravitationnelle modifiée,
     \[
       f(R)
       =R
       +\frac{\beta}{2\,R_{0}}\,(R-R_{0})^{2}
       +\frac{\gamma}{3\,R_{0}^{2}}\,(R-R_{0})^{3}.
     \]
-  \item \(f_{R}(R)\) : première dérivée de \(f\) par rapport à \(R\),
+  \item \(f_{R}(R)\) : première dérivée de \(f\) par rapport à \(R\),
     \[
       f_{R}(R)=\frac{df}{dR}.
     \]
-  \item \(f_{RR}(R)\) : seconde dérivée de \(f\),
+  \item \(f_{RR}(R)\) : seconde dérivée de \(f\),
     \[
       f_{RR}(R)=\frac{d^{2}f}{dR^{2}}.
     \]
-  \item \(m_{s}^{2}(R)\) : masse scalaire associée,
+  \item \(m_{s}^{2}(R)\) : masse scalaire associée,
     \[
       m_{s}^{2}(R)
       =\frac{f_{R}(R)-R\,f_{RR}(R)}{3\,f_{RR}(R)}.
     \]
   \item \(z\) : redshift, paramètre décrivant l’échelle cosmologique.
-  \item \(\mathrm{OK}\) : indicateur (0 ou 1) signalant que simultanément
+  \item \(\mathrm{OK}\) : indicateur (0 ou 1) signalant que simultanément
     \(f_{R}(R)>0\), \(f_{RR}(R)>0\) et \(m_{s}^{2}(R)/R_{0}>0\).
 \end{itemize}

 \bigskip
-\noindent\emph{Fin de la partie détaillée, Chapitre 3.}
\ No newline at end of file
+\noindent\emph{Fin de la partie détaillée, Chapitre 3.}
diff --git a/04-invariants-adimensionnels/04_invariants_adimensionnels_conceptuel.tex b/04-invariants-adimensionnels/04_invariants_adimensionnels_conceptuel.tex
index 71d05b5..380489f 100755
--- a/04-invariants-adimensionnels/04_invariants_adimensionnels_conceptuel.tex
+++ b/04-invariants-adimensionnels/04_invariants_adimensionnels_conceptuel.tex
@@ -13,44 +13,44 @@ Se reporter à la partie « détails » pour les données numériques et les vis
 \subsection{Définitions formelles des invariants}
 Dans le MCGT, on introduit trois invariants adimensionnels en fonction de l’âge cosmique $T$ :
 \[
-  I_{1}(T) \;=\; \frac{P(T)}{T},
+  I_{1}(T) \;=\; \frac{P(T)}{T},
   \qquad
-  I_{2}(T) \;=\; \kappa\,T^{2},
+  I_{2}(T) \;=\; \kappa\,T^{2},
   \qquad
   I_{3}(T) \;=\; F\bigl(R(T)\bigr)\;-\;1,
 \]
 avec :
 \begin{itemize}
-  \item $P(T)$ : la fonction propre du temps (définie au Chapitre 1),
-  \item $\kappa$ : constante adimensionnelle, de l’ordre de $10^{-35}$ (valeur précise donnée dans le fichier de données),
-  \item $F(R) = f_{R}(R)$ : dérivée première de l’extension $f(R)$ (voir Chapitre 3),
-  \item $R(T)$ : courbure scalaire en fonction de $T$ (voir Chapitre 3).
+  \item $P(T)$ : la fonction propre du temps (définie au Chapitre 1),
+  \item $\kappa$ : constante adimensionnelle, de l’ordre de $10^{-35}$ (valeur précise donnée dans le fichier de données),
+  \item $F(R) = f_{R}(R)$ : dérivée première de l’extension $f(R)$ (voir Chapitre 3),
+  \item $R(T)$ : courbure scalaire en fonction de $T$ (voir Chapitre 3).
 \end{itemize}

 \subsection{Ordres de grandeur et interprétations qualitatives}
 Les plages typiques observées pour chaque invariant sont :
 \[
-  I_{1}(T)\;\sim\;[10^{-3},\,2],
+  I_{1}(T)\;\sim\;[10^{-3},\,2],
   \qquad
-  I_{2}(T)\;\lesssim\;10^{-35},
+  I_{2}(T)\;\lesssim\;10^{-35},
   \qquad
   I_{3}(T)\;\lesssim\;10^{-6}.
 \]
 \begin{itemize}
-  \item \textbf{$I_{1}(T) = P(T)/T$} : mesure le rapport du temps propre au temps cosmique,
+  \item \textbf{$I_{1}(T) = P(T)/T$} : mesure le rapport du temps propre au temps cosmique,
     \begin{itemize}
-      \item lorsque $I_{1}\approx 2$ (autour de $T\sim0{,}3$ Gyr), la croissance des fluctuations de densité est maximisée,
-      \item pour $T\to 14$ Gyr, $I_{1}$ se stabilise proche de 1, ramenant MCGT vers $\Lambda$CDM.
+      \item lorsque $I_{1}\approx 2$ (autour de $T\sim0{,}3$ Gyr), la croissance des fluctuations de densité est maximisée,
+      \item pour $T\to 14$ Gyr, $I_{1}$ se stabilise proche de 1, ramenant MCGT vers $\Lambda$CDM.
     \end{itemize}
-  \item \textbf{$I_{2}(T) = \kappa\,T^{2}$} : reste extrêmement faible ($\lesssim10^{-35}$) sur toute la durée cosmique,
+  \item \textbf{$I_{2}(T) = \kappa\,T^{2}$} : reste extrêmement faible ($\lesssim10^{-35}$) sur toute la durée cosmique,
     \begin{itemize}
-      \item $\kappa$ a été déterminé de manière à garantir une influence négligeable sur l’évolution globale,
-      \item cet invariant permet de diagnostiquer la contribution purement adimensionnelle du paramétrage temporel.
+      \item $\kappa$ a été déterminé de manière à garantir une influence négligeable sur l’évolution globale,
+      \item cet invariant permet de diagnostiquer la contribution purement adimensionnelle du paramétrage temporel.
     \end{itemize}
-  \item \textbf{$I_{3}(T) = F(R) - 1$} : suit la déviation de la dérivée $f_{R}(R)$,
+  \item \textbf{$I_{3}(T) = F(R) - 1$} : suit la déviation de la dérivée $f_{R}(R)$,
     \begin{itemize}
-      \item $I_{3}\lesssim10^{-6}$ indique que $f(R)$ reste très proche de la forme linéaire $R$,
-      \item cette petite déviation reflète l’impact modifié du gravitationnel sur la composante temporelle.
+      \item $I_{3}\lesssim10^{-6}$ indique que $f(R)$ reste très proche de la forme linéaire $R$,
+      \item cette petite déviation reflète l’impact modifié du gravitationnel sur la composante temporelle.
     \end{itemize}
 \end{itemize}

@@ -66,14 +66,14 @@ Ci-dessous, un résumé compact des trois invariants adimensionnels définis pr
     \toprule
     Invariant & Formule & Plage typique & Interprétation clé \\
     \midrule
-    \(I_{1}(T)\) & \(\displaystyle \frac{P(T)}{T}\)
-                  & \([10^{-3},\,2]\)
+    \(I_{1}(T)\) & \(\displaystyle \frac{P(T)}{T}\)
+                  & \([10^{-3},\,2]\)
                   & Mesure le rapport du temps propre au temps cosmique. \\
-    \(I_{2}(T)\) & \(\displaystyle \kappa\,T^{2}\)
-                  & \(\lesssim 10^{-35}\)
+    \(I_{2}(T)\) & \(\displaystyle \kappa\,T^{2}\)
+                  & \(\lesssim 10^{-35}\)
                   & Quantifie la contribution purement adimensionnelle du paramétrage temporel. \\
-    \(I_{3}(T)\) & \(\displaystyle F(R(T)) - 1\)
-                  & \(\lesssim 10^{-6}\)
+    \(I_{3}(T)\) & \(\displaystyle F(R(T)) - 1\)
+                  & \(\lesssim 10^{-6}\)
                   & Exprime la déviation de la dérivée de l’extension gravitationnelle. \\
     \bottomrule
   \end{tabular}
@@ -88,9 +88,9 @@ Pour visualiser en un coup d’œil la dynamique relative des trois invariants a
 \begin{figure}[htbp]
   \centering
   \includegraphics[width=0.85\linewidth]{04-invariants-adimensionnels/fig_01_schema_invariants_adimensionnels.png}
-  \caption{Schéma conceptuel de l’évolution des invariants \(I_{1},I_{2},I_{3}\) en fonction de \(\log T\).
-    Les séparateurs verticaux délimitent trois phases clés de l’histoire cosmique,
-    et les lignes pointillées oranges indiquent respectivement les niveaux de référence
+  \caption{Schéma conceptuel de l’évolution des invariants \(I_{1},I_{2},I_{3}\) en fonction de \(\log T\).
+    Les séparateurs verticaux délimitent trois phases clés de l’histoire cosmique,
+    et les lignes pointillées oranges indiquent respectivement les niveaux de référence
     \(I_{2}\approx10^{-35}\) et \(I_{3}\approx10^{-6}\).}
   \label{fig:04_schema_invariants}
 \end{figure}
@@ -100,23 +100,23 @@ Le diagramme comporte :
 \begin{itemize}
   \item Trois \textbf{phases temporelles} mises en évidence par des lignes verticales :
     \begin{itemize}
-      \item \textbf{Précocité – Plateau bas}
+      \item \textbf{Précocité – Plateau bas}
         : \(I_{1}\ll1\), phase embryonnaire où la croissance des structures est naissante.
-      \item \textbf{Transition logistique – Pic de \(I_{1}\)}
+      \item \textbf{Transition logistique – Pic de \(I_{1}\)}
         : augmentation rapide de \(I_{1}\), signe de l’accélération des fluctuations.
-      \item \textbf{Régime tardif – Retour vers 1}
+      \item \textbf{Régime tardif – Retour vers 1}
         : \(I_{1}\to1\), convergence progressive du MCGT vers le comportement \(\Lambda\)CDM.
     \end{itemize}
   \item Deux \textbf{lignes horizontales pointillées} (en orange) :
     \begin{itemize}
-      \item \(I_{2}\approx10^{-35}\)  -- contribution adimensionnelle quasi-constante.
+      \item \(I_{2}\approx10^{-35}\)  -- contribution adimensionnelle quasi-constante.
       \item \(I_{3}\approx10^{-6}\)   -- déviation minimale de la dérivée \(f_{R}(R)\).
     \end{itemize}
-  \item Une flèche noire épaisse souligne l’avancée de \(\log T\)
-        de la gauche (univers primordial) vers la droite (univers tardif).
+  \item Une flèche noire épaisse souligne l’avancée de \(\log T\)
+        de la gauche (univers primordial) vers la droite (univers tardif).
 \end{itemize}

-Ce schéma, complété par le « Tableau synthétique des invariants », offre une vision pédagogique
+Ce schéma, complété par le « Tableau synthétique des invariants », offre une vision pédagogique
 de la manière dont chacun des trois invariants se comporte au cours de l’évolution cosmique.

 \subsection{Conclusion conceptuelle}
@@ -131,4 +131,4 @@ Pour les détails d’implémentation (scripts, données et figures), se reporte
 « Ces trois grandeurs adimensionnelles pourront servir de benchmarks pour comparer d’autres modèles modifiés ; en particulier, l’évolution de \(I_{1}(T)\) a un impact direct sur la croissance des structures, tandis que \(I_{3}(T)\) offre une jauge de la robustesse des extensions \(f(R)\). »
 \end{quote}

-\noindent\emph{Fin du volet conceptuel du Chapitre 4. La partie opérationnelle détaillée commence ci-dessous.}
\ No newline at end of file
+\noindent\emph{Fin du volet conceptuel du Chapitre 4. La partie opérationnelle détaillée commence ci-dessous.}
diff --git a/04-invariants-adimensionnels/04_invariants_adimensionnels_details.tex b/04-invariants-adimensionnels/04_invariants_adimensionnels_details.tex
index 2126205..d5cac27 100755
--- a/04-invariants-adimensionnels/04_invariants_adimensionnels_details.tex
+++ b/04-invariants-adimensionnels/04_invariants_adimensionnels_details.tex
@@ -6,7 +6,7 @@ Le fichier \texttt{04\_invariants\_adimensionnels\_complet.csv} contient 1400 po
 \[
   T,\;I_{1}(T),\;I_{2}(T),\;I_{3}(T).
 \]
-Les valeurs ont été générées par le script \texttt{13\_calculer\_invariants\_dimensionnels.py}.
+Les valeurs ont été générées par le script \texttt{13\_calculer\_invariants\_dimensionnels.py}.

 \medskip
 \noindent
@@ -37,10 +37,10 @@ Les valeurs ont été générées par le script \texttt{13\_calculer\_invariants

 \subsubsection*{Description du fichier}
 \begin{itemize}
-  \item \textbf{Nom :} \texttt{fig\_02\_histogramme\_invariants\_adimensionnels.png}
-  \item \textbf{Type :} image PNG (800 × 500 px)
-  \item \textbf{Génération :} script \texttt{13\_tracer\_invariants\_adimensionnels.py}, à partir de \texttt{04\_invariants\_adimensionnels\_complet.csv}
-  \item \textbf{Contenu :} histogramme normalisé des log–valeurs des invariants
+  \item \textbf{Nom :} \texttt{fig\_02\_histogramme\_invariants\_adimensionnels.png}
+  \item \textbf{Type :} image PNG (800 × 500 px)
+  \item \textbf{Génération :} script \texttt{13\_tracer\_invariants\_adimensionnels.py}, à partir de \texttt{04\_invariants\_adimensionnels\_complet.csv}
+  \item \textbf{Contenu :} histogramme normalisé des log–valeurs des invariants
     \(
       \log_{10}I_{2}
     \)
@@ -49,20 +49,20 @@ Les valeurs ont été générées par le script \texttt{13\_calculer\_invariants
       \log_{10}I_{3}
     \)
     \begin{itemize}
-      \item \textbf{Axe horizontal :} \(\log_{10}(\text{valeur de l’invariant})\).
-      \item \textbf{Axe vertical :} fréquence normalisée.
-      \item \textbf{Couleurs :}
+      \item \textbf{Axe horizontal :} \(\log_{10}(\text{valeur de l’invariant})\).
+      \item \textbf{Axe vertical :} fréquence normalisée.
+      \item \textbf{Couleurs :}
         \begin{itemize}
-          \item Jaune pour \(\log_{10}I_{2}\),
-          \item Orange pour \(\log_{10}I_{3}\).
+          \item Jaune pour \(\log_{10}I_{2}\),
+          \item Orange pour \(\log_{10}I_{3}\).
         \end{itemize}
     \end{itemize}
 \end{itemize}

 \subsubsection*{Interprétation quantitative}
 \begin{itemize}
-  \item \(\log_{10}I_{2}\) (jaune) : pic étroit autour de \(\approx -35\), reflétant la très faible amplitude et la quasi-constance de \(I_{2}\).
-  \item \(\log_{10}I_{3}\) (orange) : distribution centrée vers \(\approx -6\), indiquant que la déviation gravitationnelle reste faible mais plus variable que \(I_{2}\).
+  \item \(\log_{10}I_{2}\) (jaune) : pic étroit autour de \(\approx -35\), reflétant la très faible amplitude et la quasi-constance de \(I_{2}\).
+  \item \(\log_{10}I_{3}\) (orange) : distribution centrée vers \(\approx -6\), indiquant que la déviation gravitationnelle reste faible mais plus variable que \(I_{2}\).
 \end{itemize}

 \bigskip
@@ -71,40 +71,40 @@ Les valeurs ont été générées par le script \texttt{13\_calculer\_invariants
 \begin{figure}[htbp]
   \centering
   \includegraphics[width=0.85\linewidth]{04-invariants-adimensionnels/fig_03_i1_i2_i3_vs_t.png}
-  \caption{Invariants adimensionnels \(I_{1},I_{2},I_{3}\) en fonction de l’âge cosmique \(T\) (Gyr) sur échelle log–log.
-    — Jaune : \(I_{1}=P(T)/T\)
-    — Orange : \(I_{2}=\kappa\,T^{2}\)
+  \caption{Invariants adimensionnels \(I_{1},I_{2},I_{3}\) en fonction de l’âge cosmique \(T\) (Gyr) sur échelle log–log.
+    — Jaune : \(I_{1}=P(T)/T\)
+    — Orange : \(I_{2}=\kappa\,T^{2}\)
     — Rouge : \(I_{3}=F(R(T))-1\)}
   \label{fig:invariants_vs_t}
 \end{figure}

 \subsubsection*{Description du fichier}
 \begin{itemize}
-  \item \textbf{Nom :} \texttt{fig\_03\_i1\_i2\_i3\_vs\_t.png}
-  \item \textbf{Type :} PNG (800×500 px)
-  \item \textbf{Génération :} script \texttt{13\_tracer\_invariants\_adimensionnels.py}, à partir de \texttt{04\_invariants\_adimensionnels\_complet.csv}.
-  \item \textbf{Axes :}
+  \item \textbf{Nom :} \texttt{fig\_03\_i1\_i2\_i3\_vs\_t.png}
+  \item \textbf{Type :} PNG (800×500 px)
+  \item \textbf{Génération :} script \texttt{13\_tracer\_invariants\_adimensionnels.py}, à partir de \texttt{04\_invariants\_adimensionnels\_complet.csv}.
+  \item \textbf{Axes :}
     \begin{itemize}
-      \item Abscisse : \(T\) en Gyr (logarithmique).
-      \item Ordonnée : valeur adimensionnelle de l’invariant (logarithmique).
+      \item Abscisse : \(T\) en Gyr (logarithmique).
+      \item Ordonnée : valeur adimensionnelle de l’invariant (logarithmique).
     \end{itemize}
-  \item \textbf{Courbes :}
+  \item \textbf{Courbes :}
     \begin{itemize}
-      \item Jaune — \(I_{1}(T)=P(T)/T\)
-      \item Orange — \(I_{2}(T)=\kappa\,T^{2}\)
-      \item Rouge — \(I_{3}(T)=F\bigl(R(T)\bigr)-1\)
+      \item Jaune — \(I_{1}(T)=P(T)/T\)
+      \item Orange — \(I_{2}(T)=\kappa\,T^{2}\)
+      \item Rouge — \(I_{3}(T)=F\bigl(R(T)\bigr)-1\)
     \end{itemize}
 \end{itemize}

 \subsubsection*{Analyse des tendances}
 \begin{itemize}
-  \item \textbf{\(I_{1}\) (jaune)} :
+  \item \textbf{\(I_{1}\) (jaune)} :
     décroît de \(\sim10^{2}\) à \(\sim10^{1}\) avant de se stabiliser, traduisant la convergence tardive vers le comportement \(\Lambda\)CDM.
-  \item \textbf{\(I_{2}\) (orange)} :
+  \item \textbf{\(I_{2}\) (orange)} :
     augmente progressivement de \(\sim10^{-42}\) à \(\sim10^{-34}\), conformément à la loi \(\kappa\,T^{2}\).
-  \item \textbf{\(I_{3}\) (rouge)} :
+  \item \textbf{\(I_{3}\) (rouge)} :
     décroît fortement de l’ordre de \(10^{0}\) à \(10^{-10}\), reflétant la diminution rapide de la déviation gravitationnelle \(f_{R}(R)-1\).
-  \item \textbf{Synthèse :}
+  \item \textbf{Synthèse :}
     On observe une montée abrupte de \(I_{1}\) lors de la phase de transition logistique, suivie d’une stabilisation autour de 1, tandis que \(I_{2}\) et \(I_{3}\) varient sur plusieurs ordres de grandeur conformément à leurs définitions respectives.
 \end{itemize}

@@ -123,4 +123,4 @@ Les valeurs ont été générées par le script \texttt{13\_calculer\_invariants
 \end{description}

 \bigskip
-\noindent\emph{Fin de la partie détaillée, Chapitre 4.}
\ No newline at end of file
+\noindent\emph{Fin de la partie détaillée, Chapitre 4.}
diff --git a/05-nucleosynthese-primordiale/05_nucleosynthese_primordiale_conceptuel.tex b/05-nucleosynthese-primordiale/05_nucleosynthese_primordiale_conceptuel.tex
index 9591791..1e3011a 100755
--- a/05-nucleosynthese-primordiale/05_nucleosynthese_primordiale_conceptuel.tex
+++ b/05-nucleosynthese-primordiale/05_nucleosynthese_primordiale_conceptuel.tex
@@ -1,5 +1,5 @@
 \section*{Abstract}
-Cadre conceptuel de la nucléosynthèse primordiale (BBN) dans le MCGT : contexte et objectifs, définition de la fonction de coût \(\chi^2_{\rm BBN}(T)\) et workflow conceptuel.
+Cadre conceptuel de la nucléosynthèse primordiale (BBN) dans le MCGT : contexte et objectifs, définition de la fonction de coût \(\chi^2_{\rm BBN}(T)\) et workflow conceptuel.
 Pour le détail opérationnel (fichiers CSV, scripts, figures), se reporter à \texttt{05\_nucleosynthese\_primordiale\_details.tex}.

 \vspace{1em}
@@ -14,7 +14,7 @@ La nucléosynthèse primordiale se déroule lorsque l’âge cosmique est compri
 \]

 \begin{tcolorbox}[colback=gray!10,colframe=black,title=Fenêtre temporelle critique]
-BBN : \(T\in[10^{-5},\,10^{-3}]\;\mathrm{Gyr}\)
+BBN : \(T\in[10^{-5},\,10^{-3}]\;\mathrm{Gyr}\)
 (soit \(\sim0{,}01\)–\(10\) s).
 \end{tcolorbox}

@@ -87,11 +87,11 @@ Le processus conceptuel de validation de la BBN en MCGT s’articule en quatre g
 \subsection{Critères d’acceptation et conclusion conceptuelle}

 \begin{itemize}
-  \item \textbf{Concordance quantitative :}
-    exiger \(\chi^{2}_{\rm BBN}(T)\lesssim10^{-2}\) sur l’ensemble de l’intervalle critique.
-  \item \textbf{Concordance qualitative :}
-    les abondances modélisées doivent tomber à l’intérieur des barres d’erreur des observations pour \(Y_{p}\) et \(D/H\).
-  \item \textbf{Conclusion conceptuelle :}
+  \item \textbf{Concordance quantitative :}
+    exiger \(\chi^{2}_{\rm BBN}(T)\lesssim10^{-2}\) sur l’ensemble de l’intervalle critique.
+  \item \textbf{Concordance qualitative :}
+    les abondances modélisées doivent tomber à l’intérieur des barres d’erreur des observations pour \(Y_{p}\) et \(D/H\).
+  \item \textbf{Conclusion conceptuelle :}
     si ces critères sont remplis, le MCGT est compatible avec les abondances primordiales observées.
 \end{itemize}

@@ -103,4 +103,4 @@ Pour tous les aspects techniques (format des fichiers CSV, scripts Python, exemp
   \texttt{05\_nucleosynthese\_primordiale\_details.tex}
 \end{center}

-\noindent\emph{Fin du volet conceptuel du Chapitre 5. La partie opérationnelle détaillée commence ci-dessous.}
\ No newline at end of file
+\noindent\emph{Fin du volet conceptuel du Chapitre 5. La partie opérationnelle détaillée commence ci-dessous.}
diff --git a/05-nucleosynthese-primordiale/05_nucleosynthese_primordiale_details.tex b/05-nucleosynthese-primordiale/05_nucleosynthese_primordiale_details.tex
index 06b8cb8..3986589 100755
--- a/05-nucleosynthese-primordiale/05_nucleosynthese_primordiale_details.tex
+++ b/05-nucleosynthese-primordiale/05_nucleosynthese_primordiale_details.tex
@@ -1,7 +1,7 @@
 \subsection{Source des données \(\dot P(T)\)}

-La table \dot P(T) provient directement du Chapitre 2 :
-\texttt{02_pdot_tableau_plateau_fine.dat}, contenant 30 points \{T_i,\dot P(T_i)\} couvrant
+La table \dot P(T) provient directement du Chapitre 2 :
+\texttt{02_pdot_tableau_plateau_fine.dat}, contenant 30 points \{T_i,\dot P(T_i)\} couvrant
 \[
   T\in[0{,}01,\,10]\;\mathrm{s},
 \]
@@ -13,17 +13,17 @@ Ce fichier constitue l’entrée de base pour la conversion vers le format Alter
 Ce script prend en entrée la table \texttt{02\_pdot\_tableau\_plateau\_fine.dat} (déjà exprimée en secondes) et génère le fichier \texttt{bbn\_input.txt} au format requis par AlterBBN.

 \begin{itemize}
-  \item \textbf{Usage} :
+  \item \textbf{Usage} :
     \verb|python 13_executer_alterbbn_plateau.py|
-
-  \item \textbf{Fonctionnement} :
+
+  \item \textbf{Fonctionnement} :
     \begin{enumerate}
       \item Lit la table de points \(\{T_i,\dot P(T_i)\}\) au format deux colonnes, où \(T_i\) est déjà en secondes.
-      \item Calcule le ratio
+      \item Calcule le ratio
         \[
           r_i \;=\;\frac{1}{\dot P(T_i)}.
         \]
-      \item Vérifie que
+      \item Vérifie que
         \[
           r_i > 0
           \quad\text{et}\quad
@@ -34,13 +34,13 @@ Ce script prend en entrée la table \texttt{02\_pdot\_tableau\_plateau\_fine.dat
         \begin{itemize}
           \item Première ligne : commentaire
             \verb|# T(s) H_ratio|
-          \item Lignes suivantes :
-            \verb|<T_i> <r_i>|
+          \item Lignes suivantes :
+            \verb|<T_i> <r_i>|
             où <T_i> est la valeur de l’âge cosmique en secondes et <r_i> le ratio calculé.
         \end{itemize}
     \end{enumerate}

-  \item \textbf{Dépendances} :
+  \item \textbf{Dépendances} :
     Python 3.x, \texttt{numpy}.
 \end{itemize}

@@ -70,13 +70,13 @@ Le fichier \texttt{bbn\_input.txt} doit respecter le format suivant :
 \subsection{Exécution d’AlterBBN}

 \begin{itemize}
-  \item \textbf{Préparation} :
+  \item \textbf{Préparation} :
     copier \texttt{bbn\_input.txt} dans le répertoire principal d’AlterBBN, sans modifier les paramètres par défaut (\(\Omega_{b}h^{2},\,N_{\nu}\), etc.).
   \item \textbf{Lancement} :
     \begin{verbatim}
 alterbbn --input bbn_input.txt
     \end{verbatim}
-  \item \textbf{Sortie} :
+  \item \textbf{Sortie} :
   AlterBBN génère le fichier \texttt{05\_nucleosynthese\_resultats\_exact.csv}, structuré en trois colonnes :
   \[
     T\,[\mathrm{Gyr}],\quad Y_{p}^{\rm mod}(T),\quad (D/H)^{\rm mod}(T).
@@ -105,8 +105,8 @@ python 13_calculer_chi2_nucleosynthese.py
     \end{verbatim}
   \item \textbf{Étapes essentielles} :
     \begin{enumerate}
-      \item Lecture du CSV avec \texttt{pandas}.
-      \item Calcul de \(\chi^{2}_{\rm BBN}(T)\) pour chaque ligne comme défini précédemment.
+      \item Lecture du CSV avec \texttt{pandas}.
+      \item Calcul de \(\chi^{2}_{\rm BBN}(T)\) pour chaque ligne comme défini précédemment.
       \item Écriture du résultat sous la forme :
         \[
           T\,[\mathrm{Gyr}],\;\chi^{2}_{\rm BBN}(T).
@@ -207,13 +207,13 @@ Pour visualiser la qualité d’ajustement sur toute la plage de températures,
 Le répertoire comporte trois scripts principaux pour automatiser le workflow BBN :

 \begin{description}
-  \item[\texttt{13\_executer\_alterbbn\_plateau.py}]
+  \item[\texttt{13\_executer\_alterbbn\_plateau.py}]
     Convertit \texttt{02\_pdot\_tableau\_plateau\_fine.dat} en \texttt{bbn\_input.txt} (format AlterBBN), avec vérification de la validité numérique des ratios.

-  \item[\texttt{13\_calculer\_chi2\_nucleosynthese.py}]
+  \item[\texttt{13\_calculer\_chi2\_nucleosynthese.py}]
     Lit \texttt{05\_nucleosynthese\_resultats\_exact.csv}, calcule \(\chi^{2}_{\rm BBN}(T)\) selon la section « Définition de la fonction de coût \(\chi^{2}_{\rm BBN}(T)\) », et écrit \texttt{05\_chi2\_nucleosynthese\_vs\_t.csv}.

-  \item[\texttt{13\_tracer\_resultats\_nucleosynthese.py}]
+  \item[\texttt{13\_tracer\_resultats\_nucleosynthese.py}]
     Trace et sauve les figures comparatives (D/H, \(Y_{p}\), \(\chi^{2}\)) à partir des CSV générés.
 \end{description}

@@ -242,4 +242,3 @@ Le répertoire comporte trois scripts principaux pour automatiser le workflow BB

 \bigskip
 \noindent\emph{Fin de la partie détaillée, Chapitre 5.}
-
diff --git a/05-nucleosynthese-primordiale/CHAPTER05_GUIDE.txt b/05-nucleosynthese-primordiale/CHAPTER05_GUIDE.txt
index 5091bcd..bd05f3e 100755
--- a/05-nucleosynthese-primordiale/CHAPTER05_GUIDE.txt
+++ b/05-nucleosynthese-primordiale/CHAPTER05_GUIDE.txt
@@ -151,4 +151,3 @@ python zz-scripts/chapter05/plot_fig04_chi2_contre_T.py
 11) Compilation LaTeX
 pdflatex -jobname=chap5_conceptual  05-nucleosynthese-primordiale/05_primordial_nucleosynthesis_conceptual.tex
 pdflatex -jobname=chap5_details     05-nucleosynthese-primordiale/05_primordial_nucleosynthesis_details.tex
-
diff --git a/06-rayonnement-cmb/06_cmb_conceptuel.tex b/06-rayonnement-cmb/06_cmb_conceptuel.tex
index e0437f4..84877fb 100755
--- a/06-rayonnement-cmb/06_cmb_conceptuel.tex
+++ b/06-rayonnement-cmb/06_cmb_conceptuel.tex
@@ -3,18 +3,18 @@
 \subsection{Données essentielles et workflow global}
 Le test CMB du MCGT repose sur les éléments suivants :
 \begin{itemize}
-  \item \texttt{06-rayonnement-cmb/06\_hubble\_mcgt.dat} : table des rapports
+  \item \texttt{06-rayonnement-cmb/06\_hubble\_mcgt.dat} : table des rapports
         \(\{z,\;H_{MCGT}(z)/H_{\Lambda\mathrm{CDM}}(z)\}\).
-  \item \texttt{06-rayonnement-cmb/06\_cls\_lcdm\_spectre.dat} : spectre angulaire
+  \item \texttt{06-rayonnement-cmb/06\_cls\_lcdm\_spectre.dat} : spectre angulaire
         \(\{\ell,\;C_{\ell}^{\Lambda\mathrm{CDM}}\}\).
-  \item \texttt{06-rayonnement-cmb/06\_cls\_spectre.dat} : spectre angulaire
+  \item \texttt{06-rayonnement-cmb/06\_cls\_spectre.dat} : spectre angulaire
         \(\{\ell,\;C_{\ell}^{MCGT}\}\).
   \item \texttt{06-rayonnement-cmb/06\_delta\_rs\_scan.csv} : scan paramétrique des écarts acoustiques \(\Delta r_{s}/r_{s}\).
   \item \texttt{06-rayonnement-cmb/06\_delta\_rs\_scan\_complet.csv} : scan unidimensionnel des écarts \(\Delta r_{s}/r_{s}\) par variation de \(\alpha_{1}\), \(T_{c}\) ou \(\Delta\).
   \item \texttt{06-rayonnement-cmb/06\_cmb\_resultats\_complets.csv} : synthèse finale des métriques CMB (écarts acoustiques, résidus spectre, positions des pics, \(\Delta\chi^2_{\rm Planck}\)).
   \item \texttt{06-rayonnement-cmb/06\_cmb\_resultats\_scan\_chi2.csv} : scan paramétrique de \(\Delta\chi^2_{\mathrm{Planck}}\) pour chaque configuration \((\alpha_{1},T_{c},\Delta)\).
   \item \texttt{06-rayonnement-cmb/06\_planck\_likelihood.ini} : configuration CAMB (likelihood Planck TT/TE/EE/lensing).
-  \item \texttt{13-scripts-annexes/13\_generer\_spectres\_cmb.py} : script d’appel de CAMB pour générer les spectres
+  \item \texttt{13-scripts-annexes/13\_generer\_spectres\_cmb.py} : script d’appel de CAMB pour générer les spectres
         \texttt{06-rayonnement-cmb/06\_cls\_lcdm\_spectre.dat} et \texttt{06-rayonnement-cmb/06\_cls\_spectre.dat}.
   \item \texttt{13-scripts-annexes/13\_executer\_cmb\_complet.py} : orchestre l’ensemble du pipeline CMB, de la lecture des données à la production de \texttt{06\_cmb\_resultats\_complets.csv}.
 \end{itemize}
@@ -31,11 +31,11 @@ Cette figure illustre les différentes étapes automatisées du pipeline CMB sou
 \subsection{Figures clés et renvoi}

 \begin{itemize}
-  \item \texttt{fig\_02\_cls\_lcdm\_vs\_mcgt.png}
+  \item \texttt{fig\_02\_cls\_lcdm\_vs\_mcgt.png}
         superposition des spectres CMB \(\Lambda\)CDM (gris) et MCGT (bleu) pour \(\ell \le 2500\).
-  \item \texttt{fig\_03\_delta\_cls\_rel.png}
+  \item \texttt{fig\_03\_delta\_cls\_rel.png}
         tracé de \(\Delta C_{\ell}/C_{\ell}\) en fonction de \(\ell\) (jusqu’à \(\ell=2500\)).
-  \item \texttt{fig\_04\_delta\_rs\_vs\_params.png}
+  \item \texttt{fig\_04\_delta\_rs\_vs\_params.png}
         déviation \(\Delta r_{s}/r_{s}\) selon \((\alpha_{1},T_{c},\Delta)\).
 \end{itemize}

@@ -87,4 +87,4 @@ Le spectre CMB du MCGT est indiscernable de celui du \(\Lambda\)CDM au niveau de
 \caption{Résumé des écarts CMB pour les configurations explorées.}
 \end{table}

-\noindent\emph{Fin du volet conceptuel du Chapitre 6. La partie opérationnelle détaillée commence ci-dessous.}
\ No newline at end of file
+\noindent\emph{Fin du volet conceptuel du Chapitre 6. La partie opérationnelle détaillée commence ci-dessous.}
diff --git a/06-rayonnement-cmb/06_cmb_details.tex b/06-rayonnement-cmb/06_cmb_details.tex
index 9c312a2..c41040d 100755
--- a/06-rayonnement-cmb/06_cmb_details.tex
+++ b/06-rayonnement-cmb/06_cmb_details.tex
@@ -16,8 +16,8 @@
 \noindent
 Où :
 \begin{itemize}
-  \item \(C_{\ell}^{X}\) est le spectre CMB calculé pour le modèle \(X\).
-  \item \(r_{s}^{X}\) est l’échelle acoustique à recombinaison pour \(X\).
+  \item \(C_{\ell}^{X}\) est le spectre CMB calculé pour le modèle \(X\).
+  \item \(r_{s}^{X}\) est l’échelle acoustique à recombinaison pour \(X\).
   \item \(\chi^{2}_{\rm Planck}(X)\) est la vraisemblance Planck évaluée pour \(X\).
 \end{itemize}

@@ -34,7 +34,7 @@ Le fichier paramétrique complet contient trois colonnes, dans l’ordre :
 \texttt{delta\_rs\_over\_rs} & réel    & Écart relatif \(\Delta r_{s}/r_{s}\)                \\
 \bottomrule
 \end{tabular}
-\end{center}
+\end{center}

 \subsection{Calcul détaillé de $H_{\mathrm{MCGT}}/H_{\Lambda\mathrm{CDM}}$}
 \[
@@ -77,27 +77,27 @@ planck_likelihood     = plik_lite_TTTEEE.clik
 Le script \texttt{13\_executer\_cmb\_complet.py} orchestre l’ensemble du workflow CMB selon les étapes suivantes :

 \begin{enumerate}[label=\arabic*.]
-  \item \textbf{Chargement des données}
-    Lit le fichier \texttt{06\_hubble\_mcgt.dat} contenant
+  \item \textbf{Chargement des données}
+    Lit le fichier \texttt{06\_hubble\_mcgt.dat} contenant
     \(\{z,\;H_{\mathrm{MCGT}}(z)/H_{\Lambda\mathrm{CDM}}(z)\}\).
-  \item \textbf{Appel de CAMB}
-    Exécute CAMB deux fois :
+  \item \textbf{Appel de CAMB}
+    Exécute CAMB deux fois :
     \begin{itemize}
-      \item \verb|camb 06_planck_likelihood.ini --paramset=LCDM|
+      \item \verb|camb 06_planck_likelihood.ini --paramset=LCDM|
       \item \verb|camb 06_planck_likelihood.ini --paramset=MCGT|
     \end{itemize}
-  \item \textbf{Calcul des écarts spectres}
-    Calcule \(\Delta C_{\ell}/C_{\ell}\) pour chaque \(\ell\) en comparant
+  \item \textbf{Calcul des écarts spectres}
+    Calcule \(\Delta C_{\ell}/C_{\ell}\) pour chaque \(\ell\) en comparant
     \(\{C_{\ell}^{\Lambda\mathrm{CDM}}\}\) et \(\{C_{\ell}^{\mathrm{MCGT}}\}\).
-  \item \textbf{Extraction des pics acoustiques}
-    Détermine les indices \(\ell_{1},\ell_{2},\ell_{3}\) par interpolation
+  \item \textbf{Extraction des pics acoustiques}
+    Détermine les indices \(\ell_{1},\ell_{2},\ell_{3}\) par interpolation
     autour des maxima locaux de \(\ell(\ell+1)C_{\ell}\).
-  \item \textbf{Calcul de l’écart acoustique}
+  \item \textbf{Calcul de l’écart acoustique}
     Mesure \(\Delta r_{s}/r_{s}\) pour chaque configuration paramétrique.
-  \item \textbf{Évaluation de \(\Delta\chi^{2}_{\mathrm{Planck}}\)}
+  \item \textbf{Évaluation de \(\Delta\chi^{2}_{\mathrm{Planck}}\)}
     Compare les vraisemblances Planck pour LCDM et MCGT.
-  \item \textbf{Écriture du fichier résultat}
-    Concatène tous les résultats dans
+  \item \textbf{Écriture du fichier résultat}
+    Concatène tous les résultats dans
     \texttt{06\_cmb\_resultats\_complets.csv}.
 \end{enumerate}

@@ -106,52 +106,52 @@ Le script \texttt{13\_executer\_cmb\_complet.py} orchestre l’ensemble du workf
 Pour chaque spectre \(\{C_{\ell}^{\mathrm{MCGT}}\}\), le script procède ainsi :

 \begin{enumerate}[label=\arabic*.]
-  \item \textbf{Définition des fenêtres de recherche}
+  \item \textbf{Définition des fenêtres de recherche}
     Autour des positions théoriques des trois premiers pics :
     \(\ell \approx 220,\;545,\;800\), on fixe des intervalles \(\pm\Delta\ell\) (typiquement \(\pm10\)).
-  \item \textbf{Détection brute des maxima}
+  \item \textbf{Détection brute des maxima}
     Dans chaque intervalle, repérer l’indice \(\ell_{\max}^{(k)}\) où \(\ell(\ell+1)C_{\ell}\) est maximal.
-  \item \textbf{Interpolation quadratique}
+  \item \textbf{Interpolation quadratique}
     Considérer les points \(\ell_{\max}^{(k)} - 1,\;\ell_{\max}^{(k)},\;\ell_{\max}^{(k)} + 1\) et ajuster un polynôme du second degré pour affiner la position du pic \(\ell_{k}\) à une précision d’un demi-entier.
-  \item \textbf{Enregistrement}
+  \item \textbf{Enregistrement}
     Stocker \(\ell_{1},\,\ell_{2},\,\ell_{3}\) dans les colonnes dédiées de \texttt{06\_cmb\_resultats\_complets.csv}.
 \end{enumerate}

 \subsection{Calcul de \(\Delta r_{s}/r_{s}\)}
 L’échelle acoustique à la recombinaison \(r_{s}\) est donnée par :
 \[
-  r_{s}^{X}
-  =
+  r_{s}^{X}
+  =
   \int_{z_{\mathrm{rec}}}^{\infty} \frac{c_{s}(z)}{H^{X}(z)}\,dz,
   \quad
   z_{\mathrm{rec}}\approx1090,
 \]
-où \(X=\Lambda\mathrm{CDM}\) ou \(X=\mathrm{MCGT}\).
+où \(X=\Lambda\mathrm{CDM}\) ou \(X=\mathrm{MCGT}\).
 Pour chaque triple paramétrique \((\alpha_{1},\,T_{c},\,\Delta)\) :
 \begin{enumerate}
   \item Générer \(H_{\mathrm{MCGT}}(z)\) via CAMB (utilisant \texttt{06\_hubble\_mcgt.dat})
         ou, si disponible, via une évaluation analytique de \(\dot P[T(z)]\).
   \item Calculer numériquement
-        \(\displaystyle r_{s}^{\mathrm{MCGT}}
+        \(\displaystyle r_{s}^{\mathrm{MCGT}}
          = \int_{1090}^{\infty} c_{s}(z)\,/\,H_{\mathrm{MCGT}}(z)\,dz.\)
   \item Obtenir \(r_{s}^{\Lambda\mathrm{CDM}}\) à partir d’un appel CAMB en mode LCDM.
   \item Déterminer
         \(\displaystyle
           \Delta r_{s}/r_{s}
-          =
+          =
           \bigl(r_{s}^{\mathrm{MCGT}} - r_{s}^{\Lambda\mathrm{CDM}}\bigr)
 /\,r_{s}^{\Lambda\mathrm{CDM}}.\)
-  \item Enregistrer \((\alpha_{1},\,T_{c},\,\Delta,\;\Delta r_{s}/r_{s})\)
+  \item Enregistrer \((\alpha_{1},\,T_{c},\,\Delta,\;\Delta r_{s}/r_{s})\)
         dans \texttt{06\_delta\_rs\_scan.csv}.
 \end{enumerate}
-Le fichier \texttt{06\_delta\_rs\_scan.csv} est ensuite utilisé par
-\texttt{13\_tracer\_delta\_rs.py} pour tracer
+Le fichier \texttt{06\_delta\_rs\_scan.csv} est ensuite utilisé par
+\texttt{13\_tracer\_delta\_rs.py} pour tracer
 \texttt{fig\_03\_delta\_rs\_vs\_params.png}.

 \subsection{Calcul de \(\Delta\chi^{2}_{\mathrm{Planck}}\)}
 Après exécution de CAMB en mode LCDM et MCGT avec likelihood Planck activée, on récupère :
 \[
-  \chi^{2}_{\mathrm{Planck}}(\Lambda\mathrm{CDM}),
+  \chi^{2}_{\mathrm{Planck}}(\Lambda\mathrm{CDM}),
   \quad
   \chi^{2}_{\mathrm{Planck}}(\mathrm{MCGT}).
 \]
@@ -163,7 +163,7 @@ Le script \texttt{13\_executer\_cmb\_complet.py} calcule alors :
   \;-\;
   \chi^{2}_{\mathrm{Planck}}(\Lambda\mathrm{CDM}),
 \]
-et l’ajoute à la colonne correspondante de
+et l’ajoute à la colonne correspondante de
 \texttt{06\_cmb\_resultats\_complets.csv}.

 \subsection{Structure de \texttt{06\_cmb\_resultats\_complets.csv}}
@@ -195,19 +195,19 @@ Chaque ligne du fichier correspond à une configuration paramétrique \((\alpha_
 Pour enrichir l’analyse, deux fichiers de scan paramétrique sont désormais disponibles :

 \begin{itemize}
-  \item \texttt{06-rayonnement-cmb/06\_cmb\_resultats\_scan\_chi2.csv}
-        contient les colonnes \texttt{alpha1}, \texttt{T\_c}, \texttt{Delta} et \texttt{Delta\_chi2\_Planck}.
-        Chaque ligne donne la valeur de \(\Delta\chi^2_{\rm Planck}\) pour une configuration \((\alpha_{1},T_{c},\Delta)\),
+  \item \texttt{06-rayonnement-cmb/06\_cmb\_resultats\_scan\_chi2.csv}
+        contient les colonnes \texttt{alpha1}, \texttt{T\_c}, \texttt{Delta} et \texttt{Delta\_chi2\_Planck}.
+        Chaque ligne donne la valeur de \(\Delta\chi^2_{\rm Planck}\) pour une configuration \((\alpha_{1},T_{c},\Delta)\),
         et est générée par \texttt{13\_generer\_cmb\_scan\_chi2.py}.
-  \item \texttt{06-rayonnement-cmb/06\_delta\_rs\_scan\_complet.csv}
-        contient les colonnes \texttt{param\_name}, \texttt{param\_value} et \texttt{delta\_rs\_over\_rs}.
-        Permet d’étudier la dépendance de \(\Delta r_{s}/r_{s}\) à la variation unidimensionnelle de chacun des paramètres \(\alpha_{1}\), \(T_{c}\) ou \(\Delta\),
+  \item \texttt{06-rayonnement-cmb/06\_delta\_rs\_scan\_complet.csv}
+        contient les colonnes \texttt{param\_name}, \texttt{param\_value} et \texttt{delta\_rs\_over\_rs}.
+        Permet d’étudier la dépendance de \(\Delta r_{s}/r_{s}\) à la variation unidimensionnelle de chacun des paramètres \(\alpha_{1}\), \(T_{c}\) ou \(\Delta\),
         et est généré par \texttt{13\_generer\_delta\_rs\_scan\_complet.py}.
 \end{itemize}

 \subsection*{Lien vers les données Planck}

-Les données Planck utilisées (version 2018) sont accessibles à l’adresse suivante :
+Les données Planck utilisées (version 2018) sont accessibles à l’adresse suivante :
 \url{https://pla.esac.esa.int/pla/#home}

 \subsection{Analyse de sensibilité}
@@ -220,26 +220,26 @@ Les données Planck utilisées (version 2018) sont accessibles à l’adresse su
 \end{figure}

 \noindent
-La Fig.~\ref{fig:carte_chaleur_delta_chi2} met en évidence la région optimale du plan \((\alpha_{1},T_c)\) où \(\Delta\chi^2_{\rm Planck}\) est minimal (autour de \(\alpha_{1}\approx0{,}3\), \(T_c\approx0{,}3\) Gyr).
+La Fig.~\ref{fig:carte_chaleur_delta_chi2} met en évidence la région optimale du plan \((\alpha_{1},T_c)\) où \(\Delta\chi^2_{\rm Planck}\) est minimal (autour de \(\alpha_{1}\approx0{,}3\), \(T_c\approx0{,}3\) Gyr).

 \medskip
 \noindent
-Une analyse unidimensionnelle à partir de \texttt{06\_delta\_rs\_scan\_complet.csv} montre par exemple que,
-pour une augmentation de \(\alpha_{1}\) de +0,05 (passant de 0,30 à 0,35),
-l’écart acoustique \(\Delta r_{s}/r_{s}\) évolue d’environ \(-1\times10^{-5}\) à \(0\),
-soit une variation d’environ \(1\times10^{-5}\).
+Une analyse unidimensionnelle à partir de \texttt{06\_delta\_rs\_scan\_complet.csv} montre par exemple que,
+pour une augmentation de \(\alpha_{1}\) de +0,05 (passant de 0,30 à 0,35),
+l’écart acoustique \(\Delta r_{s}/r_{s}\) évolue d’environ \(-1\times10^{-5}\) à \(0\),
+soit une variation d’environ \(1\times10^{-5}\).

 \subsection*{Encadré : Limitations \& Perspectives}

 \begin{itemize}
-  \item Dépendance à l’interpolation de la fonction \(H_{\Lambda\mathrm{CDM}}(z)\),
-        qui peut introduire des erreurs de discrétisation aux hauts redshifts.
-  \item Absence d’effets non-linéaires dans le calcul des spectres (petits droits de lensing non inclus),
-        susceptibles d’affiner légèrement les positions et amplitudes des pics.
+  \item Dépendance à l’interpolation de la fonction \(H_{\Lambda\mathrm{CDM}}(z)\),
+        qui peut introduire des erreurs de discrétisation aux hauts redshifts.
+  \item Absence d’effets non-linéaires dans le calcul des spectres (petits droits de lensing non inclus),
+        susceptibles d’affiner légèrement les positions et amplitudes des pics.
   \item Perspectives d’extension :
     \begin{itemize}
-      \item Inclusion de la polarisation CMB (TE/EE) pour renforcer la contrainte sur \(\Delta\chi^2\).
-      \item Modélisation des effets de lentilles gravitationnelles sur le spectre à haut \(\ell\).
+      \item Inclusion de la polarisation CMB (TE/EE) pour renforcer la contrainte sur \(\Delta\chi^2\).
+      \item Modélisation des effets de lentilles gravitationnelles sur le spectre à haut \(\ell\).
       \item Exploration de couplages supplémentaires (e.g. neutrinos massifs, modifications de la recombinaison) pour tester la robustesse du MCGT.
     \end{itemize}
 \end{itemize}
@@ -276,4 +276,4 @@ Pour reproduire tous les résultats du test CMB, exécuter successivement :
 \end{description}

 \bigskip
-\noindent\emph{Fin de la partie détaillée, Chapitre 6.}
\ No newline at end of file
+\noindent\emph{Fin de la partie détaillée, Chapitre 6.}
diff --git a/07-perturbations-scalaires/07_perturbations_scalaires_conceptuel.tex b/07-perturbations-scalaires/07_perturbations_scalaires_conceptuel.tex
index 10ec99c..c8aada4 100755
--- a/07-perturbations-scalaires/07_perturbations_scalaires_conceptuel.tex
+++ b/07-perturbations-scalaires/07_perturbations_scalaires_conceptuel.tex
@@ -1,7 +1,7 @@
 \section{Chapitre 7 – Perturbations scalaires : Synthèse conceptuelle}

 \subsection{Rappel succinct des résultats du Chapitre 3}
-Les fonctions \(f_{R}(R)\) et \(f_{RR}(R)\) ont été extraites en Chapitre 3 via interpolation des données de Ricci (cf. Fig.~\ref{fig:fR_fRR_vs_R}) et l’analyse de stabilité (cf. Fig.~\ref{fig:ms2_vs_z}).
+Les fonctions \(f_{R}(R)\) et \(f_{RR}(R)\) ont été extraites en Chapitre 3 via interpolation des données de Ricci (cf. Fig.~\ref{fig:fR_fRR_vs_R}) et l’analyse de stabilité (cf. Fig.~\ref{fig:ms2_vs_z}).
 On y constate que \(f_{R}(R)>0\) et \(f_{RR}(R)>0\) sur tout l’intervalle \(0\le z\le1000\), garantissant ainsi la positivité de la masse scalaire \(m_{s}^{2}(R)\).

 \subsection{Objectif et portée}
@@ -24,7 +24,7 @@ En adoptant la **jauge comobile**, où la perturbation de la métrique se rédui
 avec
 \[
   z^{2}(a)
-  =
+  =
   \frac{3\,a^{2}\,f_{R}(R)}{\kappa\,H^{2}}\,
   \bigl(\dot f_{R} + 2\,f_{R}\,H\bigr)^{2},
   \quad
@@ -50,7 +50,7 @@ Les conditions de **stabilité linéaire** exigent pour tout \((k,a)\) du domain
 Physiquement, \(c_{s}^{2}(k,a)\) représente la vitesse de propagation des ondes scalaires dans le milieu, qui doit rester sub-luminale (\(<1\)) pour préserver la causalité, tandis que \(m_{s}^{2}(R)>0\) assure l’absence de modes fantômes (« ghosts ») et garantit la positivité de l’énergie cinétique des perturbations.

 \subsection{Conditions initiales}
-Les perturbations scalaires sont initialisées en état de Bunch–Davies dans le régime sous-horizon (\(k/a \gg H\)), garantissant un vide quantique minimal.
+Les perturbations scalaires sont initialisées en état de Bunch–Davies dans le régime sous-horizon (\(k/a \gg H\)), garantissant un vide quantique minimal.
 Cette condition impose les amplitudes de \(\zeta\) et de ses dérivées premières au début de l’intégration de l’équation de Mukhanov–Sasaki.

 \subsection{Domaines et critères de validité}
@@ -63,19 +63,19 @@ L’analyse linéaire des perturbations scalaires reste valide dans le domaine s
 \]
 Ces bornes garantissent :
 \begin{itemize}
-  \item \(k_{\max}=1\,h\,\mathrm{Mpc}^{-1}\) : on évite les régimes non linéaires de croissance des structures.
-  \item \(a_{\min}=0.1\) (soit \(z_{\max}=9\)) : on reste en phase matière-dominée, où l’équation de perturbations scalaires étudiée est applicable.
+  \item \(k_{\max}=1\,h\,\mathrm{Mpc}^{-1}\) : on évite les régimes non linéaires de croissance des structures.
+  \item \(a_{\min}=0.1\) (soit \(z_{\max}=9\)) : on reste en phase matière-dominée, où l’équation de perturbations scalaires étudiée est applicable.
 \end{itemize}

 \paragraph*{Renvois aux détails opérationnels}
 \begin{itemize}
-  \item \textbf{Construction de la grille \((k,a)\)}
+  \item \textbf{Construction de la grille \((k,a)\)}
         Voir la description complète dans \texttt{07\_perturbations\_scalaires\_details.tex}.
-  \item \textbf{Procédure d’interpolation de \(f_{R}(R)\) et \(f_{RR}(R)\)}
+  \item \textbf{Procédure d’interpolation de \(f_{R}(R)\) et \(f_{RR}(R)\)}
         Reportez-vous à \texttt{07\_perturbations\_scalaires\_details.tex}.
-  \item \textbf{Workflow pas-à-pas}
+  \item \textbf{Workflow pas-à-pas}
         Interpolation, calcul des coefficients, résolution de Mukhanov–Sasaki et génération des résultats : voir \texttt{07\_perturbations\_scalaires\_details.tex}.
-  \item \textbf{Scripts Python et dépendances}
+  \item \textbf{Scripts Python et dépendances}
         Liste complète dans \texttt{07\_perturbations\_scalaires\_details.tex}.
 \end{itemize}

@@ -87,25 +87,25 @@ Après exécution du scan complet sur la grille \((k,a)\), on obtient :
   \max_{(k,a)}\,\frac{\delta\varphi}{\varphi}(k,a)\;\approx\;10^{-4}\;\ll\;10^{-2}.
 \]

-Pour la visualisation détaillée :
-— carte de chaleur de \(c_{s}^{2}(k,a)\) (Fig.~\ref{fig_01_carte_chaleur_cs2_k_a}, Chapitre 7 – Détails),
+Pour la visualisation détaillée :
+— carte de chaleur de \(c_{s}^{2}(k,a)\) (Fig.~\ref{fig_01_carte_chaleur_cs2_k_a}, Chapitre 7 – Détails),
 — carte de chaleur de \(\delta\varphi/\varphi\) (Fig.~\ref{fig_02_carte_chaleur_delta_phi_k_a}, Chapitre 7 – Détails).

-Ces résultats confirment l’absence de modes de gradient instables (« ghosts ») et attestent de la validité de l’approximation linéaire sur tout le domaine considéré.
+Ces résultats confirment l’absence de modes de gradient instables (« ghosts ») et attestent de la validité de l’approximation linéaire sur tout le domaine considéré.

 \subsection{Discussion des résultats}
 La valeur minimale \(c_{s}^{2}\approx0.5\) reste largement sub-luminale, confirmant que la propagation des ondes scalaires respecte la causalité. De plus, l’amplitude maximale \(\delta\varphi/\varphi\lesssim10^{-4}\) est négligeable par rapport à l’échelle de fond, ce qui indique que les perturbations n’injectent pas de rétroaction significative sur la dynamique cosmologique. Ces résultats soulignent la robustesse de la stabilité linéaire du MCGT sur toute la grille considérée.

 \subsection*{Limites et perspectives conceptuelles}
-Cette analyse se concentre strictement sur la stabilité linéaire des perturbations scalaires pour \(k\le1\,h\,\mathrm{Mpc}^{-1}\) et \(z\le9\).
+Cette analyse se concentre strictement sur la stabilité linéaire des perturbations scalaires pour \(k\le1\,h\,\mathrm{Mpc}^{-1}\) et \(z\le9\).
 Au-delà de ces bornes, les effets non linéaires de croissance des structures deviennent significatifs et nécessitent une extension de la méthode, notamment :
 \begin{itemize}
-  \item étude non linéaire via simulations N-body ou codes Boltzmann étendus ;
-  \item inclusion des rétroactions couplage matière-énergie sombre à haut redshift ;
+  \item étude non linéaire via simulations N-body ou codes Boltzmann étendus ;
+  \item inclusion des rétroactions couplage matière-énergie sombre à haut redshift ;
   \item exploration du régime radiation-dominée (\(a<0.1\)), où la physique des perturbations diffère sensiblement.
 \end{itemize}
-Ces pistes permettront de tester la robustesse du MCGT au-delà de l’approximation linéaire.
-Ces résultats conceptuels, validés numériquement, constituent la base pour étudier l’incidence des couplages non linéaires dans les prochains chapitres.
+Ces pistes permettront de tester la robustesse du MCGT au-delà de l’approximation linéaire.
+Ces résultats conceptuels, validés numériquement, constituent la base pour étudier l’incidence des couplages non linéaires dans les prochains chapitres.

 \subsection{Conclusion conceptuelle}
 Le MCGT est linéairement stable pour toutes les perturbations scalaires \((k,a)\in[10^{-4},1]\times[0.1,1]\), car :
@@ -119,4 +119,4 @@ Pour consulter la structure détaillée des fichiers de sortie, les implémentat
   \texttt{07\_perturbations\_scalaires\_details.tex}
 \end{center}

-\noindent\emph{Fin du volet conceptuel du Chapitre 7. La partie opérationnelle détaillée commence ci-dessous.}
\ No newline at end of file
+\noindent\emph{Fin du volet conceptuel du Chapitre 7. La partie opérationnelle détaillée commence ci-dessous.}
diff --git a/07-perturbations-scalaires/07_perturbations_scalaires_details.tex b/07-perturbations-scalaires/07_perturbations_scalaires_details.tex
index 2662f6a..fc3e413 100755
--- a/07-perturbations-scalaires/07_perturbations_scalaires_details.tex
+++ b/07-perturbations-scalaires/07_perturbations_scalaires_details.tex
@@ -1,5 +1,5 @@
 \subsection{Grille numérique}
-On discrétise l’espace \((k,a)\) en posant
+On discrétise l’espace \((k,a)\) en posant
 \[
   k_{i} \;=\; 10^{-4 \;+\;\frac{i}{N_{k}-1}\,\log_{10}(10^{4})},
   \quad
@@ -24,71 +24,71 @@ Pour chaque facteur d’échelle \(a_{j}\) de la grille, on procède ainsi :
 Pour chaque nœud \(\bigl(k_{i},a_{j}\bigr)\) de la grille, on procède ainsi :

 \begin{enumerate}
-  \item \textbf{Interpolation des coefficients :}
+  \item \textbf{Interpolation des coefficients :}
     \[
       z_{j} = \frac{1}{a_{j}} - 1,
       \quad
       R_{\mathrm{norm}} = R(z_{j})/R_{0},
     \]
     puis on interpole \(f_{R}=f_{R}(R_{\mathrm{norm}})\) et \(f_{RR}=f_{RR}(R_{\mathrm{norm}})\) à partir des fichiers du Chapitre 3.
-
-  \item \textbf{Calcul de la masse scalaire :}
+
+  \item \textbf{Calcul de la masse scalaire :}
     \[
       m_{s}^{2}
       = \frac{f_{R} - R_{\mathrm{norm}}\,f_{RR}}{3\,f_{RR}}.
     \]

-  \item \textbf{Calcul de la vitesse effective :}
+  \item \textbf{Calcul de la vitesse effective :}
     \[
       c_{s}^{2}(k_{i},a_{j})
       = \frac{f_{R}}{3\,f_{RR}}
         \;\frac{\bigl(k_{i}/a_{j}\bigr)^{2}}{\bigl(k_{i}/a_{j}\bigr)^{2} + m_{s}^{2}}.
     \]

-  \item \textbf{Résolution de l’équation de Mukhanov–Sasaki modifiée :}
+  \item \textbf{Résolution de l’équation de Mukhanov–Sasaki modifiée :}
     \[
       \zeta'' + 2\frac{z'}{z}\,\zeta' + c_{s}^{2}(k_{i},a_{j})\,k_{i}^{2}\,\zeta = 0,
     \]
-    avec
-    \(\zeta(\eta_{\rm init}) = \frac{1}{\sqrt{2k_{i}}}\),
-    \(\zeta'(\eta_{\rm init}) = -i\,\sqrt{\tfrac{k_{i}}{2}}\)
-    (conditions de Bunch–Davies en sous-horizon),
+    avec
+    \(\zeta(\eta_{\rm init}) = \frac{1}{\sqrt{2k_{i}}}\),
+    \(\zeta'(\eta_{\rm init}) = -i\,\sqrt{\tfrac{k_{i}}{2}}\)
+    (conditions de Bunch–Davies en sous-horizon),
     intégrée jusqu’à l’échelle super-horizon.

-  \item \textbf{Calcul de la perturbation scalarisée :}
-    À l’issue de l’intégration, on note \(\zeta_{\rm fin}\) la valeur finale et on définit
+  \item \textbf{Calcul de la perturbation scalarisée :}
+    À l’issue de l’intégration, on note \(\zeta_{\rm fin}\) la valeur finale et on définit
     \[
       \frac{\delta\varphi}{\varphi}(k_{i},a_{j})
       = \frac{f_{R} - R_{\mathrm{norm}}\,f_{RR}}{f_{R}}\;\zeta_{\rm fin}.
     \]

-  \item \textbf{Enregistrement des résultats :}
-    Sauvegarde dans la table de sortie des valeurs
+  \item \textbf{Enregistrement des résultats :}
+    Sauvegarde dans la table de sortie des valeurs
     \(\{\,k_{i},\,a_{j},\,c_{s}^{2},\,m_{s}^{2},\,\delta\varphi/\varphi\}\).
 \end{enumerate}

 \subsection{Scripts et exécution numérique}
-Tous les scripts nécessaires se trouvent dans le dossier annexe \texttt{../13-scripts-annexes/}.
+Tous les scripts nécessaires se trouvent dans le dossier annexe \texttt{../13-scripts-annexes/}.

-\paragraph{Dépendances Python}
+\paragraph{Dépendances Python}
 \texttt{numpy}, \texttt{scipy}, \texttt{pandas}, \texttt{matplotlib}.

 \paragraph{Liste des scripts}
 \begin{itemize}
-  \item \texttt{13\_executer\_scan\_perturbations\_scalaires.py}
+  \item \texttt{13\_executer\_scan\_perturbations\_scalaires.py}
         Interpole \(f_{R}\) et \(f_{RR}\), balaie la grille \((k,a)\), calcule \(c_{s}^{2}\), \(m_{s}^{2}\) et \(\delta\varphi/\varphi\), puis génère :
-        \texttt{07\_cs2\_scan.csv},
-        \texttt{07\_delta\_phi\_phi\_scan.csv},
+        \texttt{07\_cs2\_scan.csv},
+        \texttt{07\_delta\_phi\_phi\_scan.csv},
         \texttt{07\_perturbations\_scalaires\_resultats.csv}.
-  \item \texttt{13\_extraire\_extrema.py}
-        Extrait et affiche
-        \(\min_{(k,a)}c_{s}^{2}\) et \(\max_{(k,a)}(\delta\varphi/\varphi)\)
+  \item \texttt{13\_extraire\_extrema.py}
+        Extrait et affiche
+        \(\min_{(k,a)}c_{s}^{2}\) et \(\max_{(k,a)}(\delta\varphi/\varphi)\)
         depuis \texttt{07\_perturbations\_scalaires\_resultats.csv}.
-  \item \texttt{13\_tracer\_carte\_chaleur\_cs2.py}
-        Trace et sauvegarde la carte de chaleur de \(c_{s}^{2}(k,a)\) sous
+  \item \texttt{13\_tracer\_carte\_chaleur\_cs2.py}
+        Trace et sauvegarde la carte de chaleur de \(c_{s}^{2}(k,a)\) sous
         \texttt{fig\_01\_carte\_chaleur\_cs2\_k\_a.png}.
-  \item \texttt{13\_tracer\_carte\_chaleur\_delta\_phi.py}
-        Trace et sauvegarde la carte de chaleur de \(\delta\varphi/\varphi\) sous
+  \item \texttt{13\_tracer\_carte\_chaleur\_delta\_phi.py}
+        Trace et sauvegarde la carte de chaleur de \(\delta\varphi/\varphi\) sous
         \texttt{fig\_02\_carte\_chaleur\_delta\_phi\_k\_a.png}.
 \end{itemize}

@@ -123,9 +123,9 @@ python ../13-scripts-annexes/13_tracer_carte_chaleur_delta_phi.py \

 \subsection{Structure détaillée des fichiers CSV}

-\paragraph{\texttt{07\_perturbations\_scalaires\_resultats.csv}}
+\paragraph{\texttt{07\_perturbations\_scalaires\_resultats.csv}}
 \begin{itemize}
-  \item \textbf{Colonnes (dans l’ordre exact, séparées par une virgule sans espace)} :
+  \item \textbf{Colonnes (dans l’ordre exact, séparées par une virgule sans espace)} :
   \texttt{k, a, c\_s\^2, m\_s\^2/R0, delta\_phi\_over\_phi}
   \item Il y a une ligne par nœud de la grille \((k_{i},a_{j})\) avec \(N_{k}=50\), \(N_{a}=50\), soit 2500 lignes.
   \item Format des valeurs :
@@ -145,7 +145,7 @@ k,a,cs2,ms2_over_R0,delta_phi_over_phi
     \end{verbatim}
 \end{itemize}

-\paragraph{\texttt{07\_cs2\_scan.csv}}
+\paragraph{\texttt{07\_cs2\_scan.csv}}
 \begin{itemize}
   \item \textbf{Colonnes} :
     \[
@@ -161,7 +161,7 @@ k,a,cs2,ms2_over_R0,delta_phi_over_phi
   \item Utilisé exclusivement pour générer la carte de chaleur de \(c_{s}^{2}(k,a)\).
 \end{itemize}

-\paragraph{\texttt{07\_delta\_phi\_phi\_scan.csv}}
+\paragraph{\texttt{07\_delta\_phi\_phi\_scan.csv}}
 \begin{itemize}
   \item \textbf{Colonnes} :
     \[
@@ -201,9 +201,9 @@ print(high_cs2[['k','a','c_s^2']].head())
   \centering
   \includegraphics[width=0.75\linewidth]
     {07-perturbations-scalaires/fig_01_carte_chaleur_cs2_k_a.png}
-  \caption{carte de chaleur de \(c_{s}^{2}(k,a)\) sur
-           \((k,a)\in[10^{-4},\,1]\times[0.1,\,1]\).
-           L’axe horizontal (log en \(k\)), l’axe vertical (linéaire en \(a\)).
+  \caption{carte de chaleur de \(c_{s}^{2}(k,a)\) sur
+           \((k,a)\in[10^{-4},\,1]\times[0.1,\,1]\).
+           L’axe horizontal (log en \(k\)), l’axe vertical (linéaire en \(a\)).
            On constate \(\min c_{s}^{2} \approx 0.50\), confirmant l’absence de modes instables.}
   \label{fig:carte_chaleur_cs2}
 \end{figure}
@@ -214,8 +214,8 @@ print(high_cs2[['k','a','c_s^2']].head())
   \centering
   \includegraphics[width=0.75\linewidth]
     {07-perturbations-scalaires/fig_02_carte_chaleur_delta_phi_k_a.png}
-  \caption{carte de chaleur de \(\delta\varphi/\varphi(k,a)\) sur la même grille
-           \((k,a)\in[10^{-4},\,1]\times[0.1,\,1]\).
+  \caption{carte de chaleur de \(\delta\varphi/\varphi(k,a)\) sur la même grille
+           \((k,a)\in[10^{-4},\,1]\times[0.1,\,1]\).
            Le maximum atteint \(\approx 10^{-4}\), bien en dessous du seuil \(10^{-2}\).}
   \label{fig:carte_chaleur_delta_phi}
 \end{figure}
@@ -229,7 +229,7 @@ montre clairement :
   \qquad
   \max_{(k,a)} \frac{\delta\varphi}{\varphi}(k,a) \simeq 10^{-4} \ll 10^{-2}.
 \]
-Ainsi, le MCGT est totalement exempt d’instabilités scalaires linéaires sur le domaine considéré.
+Ainsi, le MCGT est totalement exempt d’instabilités scalaires linéaires sur le domaine considéré.
 Les scripts et fichiers décrits garantissent la reproductibilité intégrale des résultats.

 \subsection*{Glossaire}
@@ -241,12 +241,12 @@ Les scripts et fichiers décrits garantissent la reproductibilité intégrale de
   \item[$R_{\mathrm{norm}}$] $R/R_{0}$, valeur normalisée de la courbure de Ricci ($R_{0}=12H_{0}^{2}$).
   \item[$f_{R}(R)$] Première dérivée de la fonction $f(R)$ par rapport à $R$.
   \item[$f_{RR}(R)$] Seconde dérivée de la fonction $f(R)$ par rapport à $R$.
-  \item[$m_{s}^{2}$] Masse scalaire définie par
+  \item[$m_{s}^{2}$] Masse scalaire définie par
     $\displaystyle m_{s}^{2}=\frac{f_{R}-R_{\mathrm{norm}}\,f_{RR}}{3\,f_{RR}}$.
-  \item[$c_{s}^{2}(k,a)$] Vitesse effective des perturbations scalaires :
+  \item[$c_{s}^{2}(k,a)$] Vitesse effective des perturbations scalaires :
     $\displaystyle c_{s}^{2}=\frac{f_{R}}{3f_{RR}}\frac{(k/a)^{2}}{(k/a)^{2}+m_{s}^{2}}$.
   \item[$\zeta(\eta,\mathbf{x})$] Variable de perturbation comobile en jauge comobile.
-  \item[$\delta\varphi/\varphi(k,a)$] Fraction de perturbation scalaire normalisée,
+  \item[$\delta\varphi/\varphi(k,a)$] Fraction de perturbation scalaire normalisée,
     $\displaystyle \frac{\delta\varphi}{\varphi}=\frac{f_{R}-R_{\mathrm{norm}}\,f_{RR}}{f_{R}}\,\zeta_{\rm fin}$.
   \item[Grille $(k_{i},a_{j})$] Ensemble de $N_{k}\times N_{a}=50\times50=2500$ nœuds pour $(k,a)$.
   \item[\texttt{07\_cs2\_scan.csv}] Fichier CSV contenant les valeurs de $c_{s}^{2}(k,a)$ pour chaque nœud.
@@ -256,7 +256,7 @@ Les scripts et fichiers décrits garantissent la reproductibilité intégrale de
   \item[\texttt{13\_extraire\_extrema.py}] Script Python pour extraire et afficher $\min c_{s}^{2}$ et $\max(\delta\varphi/\varphi)$.
   \item[\texttt{13\_tracer\_carte\_chaleur\_cs2.py}] Script Python pour tracer et sauvegarder la carte de chaleur de $c_{s}^{2}(k,a)$.
   \item[\texttt{13\_tracer\_carte\_chaleur\_delta\_phi.py}] Script Python pour tracer et sauvegarder la carte de chaleur de $\delta\varphi/\varphi(k,a)$.
-  \item[Département des dépendances]
+  \item[Département des dépendances]
     Bibliothèques Python requises : \texttt{numpy}, \texttt{scipy}, \texttt{pandas}, \texttt{matplotlib}.
 \end{description}

diff --git a/07-perturbations-scalaires/CHAPTER07_GUIDE.txt b/07-perturbations-scalaires/CHAPTER07_GUIDE.txt
index b6cdcb8..8259849 100755
--- a/07-perturbations-scalaires/CHAPTER07_GUIDE.txt
+++ b/07-perturbations-scalaires/CHAPTER07_GUIDE.txt
@@ -288,4 +288,3 @@ Constantes et tolérances clés :
 Notes :
   • Tous les chemins sont relatifs à la racine du projet.
   • Vérifier la présence des dépendances Python et l’accès aux fichiers INI/JSON.
-
diff --git a/08-couplage-sombre/08_couplage_sombre_conceptuel.tex b/08-couplage-sombre/08_couplage_sombre_conceptuel.tex
index be3755f..c6e42c0 100755
--- a/08-couplage-sombre/08_couplage_sombre_conceptuel.tex
+++ b/08-couplage-sombre/08_couplage_sombre_conceptuel.tex
@@ -2,7 +2,7 @@

 \subsection{Modèle d’interaction}

-Nous considérons, au niveau conceptuel, un couplage modéré entre matière sombre et énergie sombre, caractérisé par un unique paramètre \(Q_{0}\).
+Nous considérons, au niveau conceptuel, un couplage modéré entre matière sombre et énergie sombre, caractérisé par un unique paramètre \(Q_{0}\).

 Les densités aux temps présents sont fixées par :
 \[
@@ -23,19 +23,19 @@ Le paramètre d’interaction \(Q_{0}\) contrôle le transfert d’énergie entr
 Pour contraindre le paramètre \(Q_{0}\), on s’appuie sur trois ensembles de données :

 \begin{itemize}
-  \item \textbf{Supernovae Ia – Pantheon\,+ v2.0 (Scolnic et al. 2022)}
+  \item \textbf{Supernovae Ia – Pantheon\,+ v2.0 (Scolnic et al. 2022)}
     \begin{itemize}
-      \item Fichier : \texttt{08\_pantheon\_plus.csv}
-      \item Observables : module de distance \(\mu_{\rm obs}(z)\) et incertitude \(\sigma_{\mu}(z)\) pour 1550 SNIa.
+      \item Fichier : \texttt{08\_pantheon\_plus.csv}
+      \item Observables : module de distance \(\mu_{\rm obs}(z)\) et incertitude \(\sigma_{\mu}(z)\) pour 1550 SNIa.
     \end{itemize}

-  \item \textbf{Oscillations acoustiques baryoniques – BAO DR12/eBOSS (Alam et al. 2021)}
+  \item \textbf{Oscillations acoustiques baryoniques – BAO DR12/eBOSS (Alam et al. 2021)}
     \begin{itemize}
-      \item Fichier : \texttt{08\_donnees\_bao.csv}
-      \item Observables : distance volumétrique \(D_{V}^{\rm obs}(z)\) et incertitude \(\sigma_{D_{V}}(z)\) pour 8 mesures BAO.
+      \item Fichier : \texttt{08\_donnees\_bao.csv}
+      \item Observables : distance volumétrique \(D_{V}^{\rm obs}(z)\) et incertitude \(\sigma_{D_{V}}(z)\) pour 8 mesures BAO.
     \end{itemize}

-  \item \textbf{Contrainte locale sur la constante de Hubble}
+  \item \textbf{Contrainte locale sur la constante de Hubble}
     \[
       H_{0}^{\rm local} = 73.2 \pm 1.3 \;\mathrm{km/s/Mpc}
       \quad(\text{Riess et al. 2021}).
@@ -48,34 +48,34 @@ Pour contraindre le paramètre \(Q_{0}\), on s’appuie sur trois ensembles de d
 La chaîne de calcul repose sur trois scripts clés, dont l’exécution produit les fichiers de résultats listés dans ce chapitre :

 \begin{itemize}
-  \item \texttt{13\_resolution\_grille\_z.py}
+  \item \texttt{13\_resolution\_grille\_z.py}
     \begin{itemize}
-      \item Résout numériquement l’évolution de \(H(z;Q_{0})\) pour chaque valeur de \(Q_{0}\).
-      \item Génère la grille ASCII \texttt{08\_grille\_Hz\_q0.txt}, listant \(\{z_{i},\,H(z_{i};Q_{0})\}\).
+      \item Résout numériquement l’évolution de \(H(z;Q_{0})\) pour chaque valeur de \(Q_{0}\).
+      \item Génère la grille ASCII \texttt{08\_grille\_Hz\_q0.txt}, listant \(\{z_{i},\,H(z_{i};Q_{0})\}\).
     \end{itemize}

-  \item \texttt{13\_couplage\_energie\_sombre.py}
+  \item \texttt{13\_couplage\_energie\_sombre.py}
     \begin{itemize}
-      \item Parcourt l’intervalle \(Q_{0}\in[0,\,0.20]\).
-      \item Pour chaque \(Q_{0}\) :
+      \item Parcourt l’intervalle \(Q_{0}\in[0,\,0.20]\).
+      \item Pour chaque \(Q_{0}\) :
         \begin{itemize}
-          \item appelle \texttt{13\_resolution\_grille\_z.py},
-          \item calcule conceptuellement les distances \(D_{L}\) et \(D_{V}\) ainsi que les contributions \(\chi^{2}_{H_{0}}\), \(\chi^{2}_{\rm SNIa}\), \(\chi^{2}_{\rm BAO}\),
-          \item constitue la statistique globale \(\chi^{2}_{\rm tot}\),
-          \item écrit la ligne correspondante dans \texttt{08\_q0\_reel\_scan.csv}.
+          \item appelle \texttt{13\_resolution\_grille\_z.py},
+          \item calcule conceptuellement les distances \(D_{L}\) et \(D_{V}\) ainsi que les contributions \(\chi^{2}_{H_{0}}\), \(\chi^{2}_{\rm SNIa}\), \(\chi^{2}_{\rm BAO}\),
+          \item constitue la statistique globale \(\chi^{2}_{\rm tot}\),
+          \item écrit la ligne correspondante dans \texttt{08\_q0\_reel\_scan.csv}.
         \end{itemize}
-      \item Procède ensuite à une interpolation quadratique locale pour estimer
-        \(Q_{0}^{\star}\pm\sigma\) et sauvegarde ce résultat dans
-        \texttt{08\_q0\_meilleur\_ajustement.txt}.
+      \item Procède ensuite à une interpolation quadratique locale pour estimer
+        \(Q_{0}^{\star}\pm\sigma\) et sauvegarde ce résultat dans
+        \texttt{08\_q0\_meilleur\_ajustement.txt}.
     \end{itemize}

-  \item \texttt{13\_valider\_resultats\_couplage.py}
+  \item \texttt{13\_valider\_resultats\_couplage.py}
     \begin{itemize}
-      \item Lit \texttt{08\_q0\_reel\_scan.csv} et génère trois figures conceptuelles :
+      \item Lit \texttt{08\_q0\_reel\_scan.csv} et génère trois figures conceptuelles :
         \begin{itemize}
-          \item \texttt{fig\_01\_chi2\_total\_vs\_q0.png} : \(\chi^{2}_{\rm tot}(Q_{0})\),
-          \item \texttt{fig\_02\_dv\_vs\_z.png} : comparaison BAO \(D_{V}^{\rm obs}\) vs \(D_{V}^{\rm th}\),
-          \item \texttt{fig\_03\_mu\_vs\_z.png} : comparaison Pantheon+ \(\mu_{\rm obs}\) vs \(\mu_{\rm th}\).
+          \item \texttt{fig\_01\_chi2\_total\_vs\_q0.png} : \(\chi^{2}_{\rm tot}(Q_{0})\),
+          \item \texttt{fig\_02\_dv\_vs\_z.png} : comparaison BAO \(D_{V}^{\rm obs}\) vs \(D_{V}^{\rm th}\),
+          \item \texttt{fig\_03\_mu\_vs\_z.png} : comparaison Pantheon+ \(\mu_{\rm obs}\) vs \(\mu_{\rm th}\).
         \end{itemize}
     \end{itemize}
 \end{itemize}
@@ -85,19 +85,19 @@ La chaîne de calcul repose sur trois scripts clés, dont l’exécution produit
 La chaîne de calcul génère les fichiers et figures suivants, essentiels pour l’interprétation des résultats :

 \begin{itemize}
-  \item \texttt{08\_grille\_Hz\_q0.txt}
-    Grille de redshift et de taux de Hubble \(\{z_{i},H(z_{i};Q_{0})\}\) issue du script \texttt{13\_resolution\_grille\_z.py}.
-  \item \texttt{08\_q0\_reel\_scan.csv}
-    Tableau complet des statistiques partielles et globale
-    \((Q_{0},\chi^{2}_{H_{0}},\chi^{2}_{\rm SNIa},\chi^{2}_{\rm BAO},\chi^{2}_{\rm tot})\).
-  \item \texttt{08\_q0\_meilleur\_ajustement.txt}
-    Valeur optimale \(Q_{0}^{\star}\pm\sigma\) obtenue par interpolation locale autour du minimum de \(\chi^{2}_{\rm tot}\).
-  \item \texttt{fig\_01\_chi2\_total\_vs\_q0.png}
-    Trace de la courbe \(\chi^{2}_{\rm tot}(Q_{0})\), utilisée pour identifier le couplage optimal.
-  \item \texttt{fig\_02\_dv\_vs\_z.png}
-    Superposition du modèle et des données BAO pour la distance volumétrique \(D_{V}(z)\), vérifiant la concordance.
-  \item \texttt{fig\_03\_mu\_vs\_z.png}
-    Superposition du modèle et des données Pantheon\,+ pour le module de distance \(\mu(z)\), contrôlant l’accord avec les SN Ia.
+  \item \texttt{08\_grille\_Hz\_q0.txt}
+    Grille de redshift et de taux de Hubble \(\{z_{i},H(z_{i};Q_{0})\}\) issue du script \texttt{13\_resolution\_grille\_z.py}.
+  \item \texttt{08\_q0\_reel\_scan.csv}
+    Tableau complet des statistiques partielles et globale
+    \((Q_{0},\chi^{2}_{H_{0}},\chi^{2}_{\rm SNIa},\chi^{2}_{\rm BAO},\chi^{2}_{\rm tot})\).
+  \item \texttt{08\_q0\_meilleur\_ajustement.txt}
+    Valeur optimale \(Q_{0}^{\star}\pm\sigma\) obtenue par interpolation locale autour du minimum de \(\chi^{2}_{\rm tot}\).
+  \item \texttt{fig\_01\_chi2\_total\_vs\_q0.png}
+    Trace de la courbe \(\chi^{2}_{\rm tot}(Q_{0})\), utilisée pour identifier le couplage optimal.
+  \item \texttt{fig\_02\_dv\_vs\_z.png}
+    Superposition du modèle et des données BAO pour la distance volumétrique \(D_{V}(z)\), vérifiant la concordance.
+  \item \texttt{fig\_03\_mu\_vs\_z.png}
+    Superposition du modèle et des données Pantheon\,+ pour le module de distance \(\mu(z)\), contrôlant l’accord avec les SN Ia.
 \end{itemize}

 \subsection{Intervalle exploré pour \(Q_{0}\)}
@@ -114,9 +114,9 @@ La procédure conceptuelle se résume en quatre étapes clés :
 \begin{enumerate}
   \item \emph{Balayage de l’intervalle \(Q_{0}\).} Pour chaque valeur de \(Q_{0}\in[0,\,0.20]\), on résout numériquement l’évolution de \(H(z;Q_{0})\) sur la plage de redshift considérée.
   \item \emph{Calcul des distances cosmologiques.} À partir du profil \(H(z;Q_{0})\), on déduit les distances lumineuse et volumétrique \(D_{L}(z;Q_{0})\) et \(D_{V}(z;Q_{0})\).
-  \item \emph{Évaluation des statistiques de coût.} On compare ces quantités aux données pour calculer
-    \(\chi^{2}_{H_{0}},\,\chi^{2}_{\rm SNIa},\,\chi^{2}_{\rm BAO}\),
-    puis on constitue la statistique globale
+  \item \emph{Évaluation des statistiques de coût.} On compare ces quantités aux données pour calculer
+    \(\chi^{2}_{H_{0}},\,\chi^{2}_{\rm SNIa},\,\chi^{2}_{\rm BAO}\),
+    puis on constitue la statistique globale
     \[
       \chi^{2}_{\rm tot}(Q_{0})
       = \chi^{2}_{H_{0}}(Q_{0})
@@ -132,7 +132,7 @@ La procédure conceptuelle se résume en quatre étapes clés :
   \centering
   \includegraphics[width=0.75\linewidth]%
     {08-couplage-sombre/fig_01_chi2_total_vs_q0.png}
-  \caption{Variation de la statistique normalisée \(\chi^{2}_{\rm tot}(Q_{0})\) en fonction du paramètre de couplage.
+  \caption{Variation de la statistique normalisée \(\chi^{2}_{\rm tot}(Q_{0})\) en fonction du paramètre de couplage.
   Chaque point correspond à une valeur de \(Q_{0}\) dans \([0,0.20]\). Le minimum localisé à \(Q_{0}^{\star}=0{,}12\) donne \(\chi^{2}_{\rm tot}/N\approx1.00\).}
   \label{fig:chi2_total_vs_Q0}
 \end{figure}
@@ -143,7 +143,7 @@ La procédure conceptuelle se résume en quatre étapes clés :
     {08-couplage-sombre/fig_02_dv_vs_z.png}
   \caption{Comparaison de la distance volumétrique
     \(D_{V}(z)\) prédite par le modèle (courbe orange, \(Q_{0}^{\star}=0{,}12\))
-    aux mesures BAO (points verts avec barres d’erreur).
+    aux mesures BAO (points verts avec barres d’erreur).
     On observe une bonne concordance pour les quatre redshifts clés.}
   \label{fig:DV_z_p4}
 \end{figure}
@@ -152,19 +152,19 @@ La procédure conceptuelle se résume en quatre étapes clés :
   \centering
   \includegraphics[width=0.75\linewidth]%
     {08-couplage-sombre/fig_03_mu_vs_z.png}
-  \caption{Module de distance \(\mu(z)\) : modèle (courbe rouge, \(Q_{0}^{\star}=0{,}12\))
-    versus données Pantheon\,+ (points bleus).
+  \caption{Module de distance \(\mu(z)\) : modèle (courbe rouge, \(Q_{0}^{\star}=0{,}12\))
+    versus données Pantheon\,+ (points bleus).
     Le couplage sombre n’introduit pas de décalage systématique hors des barres d’erreur SN Ia.}
   \label{fig:mu_z_p4}
 \end{figure}

 \subsection{Conclusion conceptuelle}

-Le couplage optimal pour l’interaction matière sombre / énergie sombre est estimé à
+Le couplage optimal pour l’interaction matière sombre / énergie sombre est estimé à
 \[
   Q_{0}^{\star} \approx 0{,}12,
 \]
-avec une statistique globale normalisée \(\chi^{2}_{\rm tot}/N \approx 1\).
+avec une statistique globale normalisée \(\chi^{2}_{\rm tot}/N \approx 1\).

 Cette valeur de \(Q_{0}\) est pleinement compatible, au niveau conceptuel, avec les observations Supernovae Ia, BAO et la contrainte locale sur \(H_{0}\).

@@ -177,19 +177,19 @@ La modification MCGT de l’expansion précoce, via la calibration logistique et
 \]

 \begin{itemize}
-  \item Valeurs de référence en \(\Lambda\)CDM :
-    \(H_0^{\rm Planck} = 67.4 \pm 0.5\;\mathrm{km/s/Mpc}\)\;(\text{Planck 2018}),
-    \(H_0^{\rm local} = 73.2 \pm 1.3\;\mathrm{km/s/Mpc}\)\;(\text{Riess et al. 2021}),
+  \item Valeurs de référence en \(\Lambda\)CDM :
+    \(H_0^{\rm Planck} = 67.4 \pm 0.5\;\mathrm{km/s/Mpc}\)\;(\text{Planck 2018}),
+    \(H_0^{\rm local} = 73.2 \pm 1.3\;\mathrm{km/s/Mpc}\)\;(\text{Riess et al. 2021}),
     soit une tension de \(\sim6\%\).
-  \item Estimation MCGT :
-    en appliquant la correction logistique (voir Sect. 1.2) et le couplage sombre (Chap. 8), on calcule
+  \item Estimation MCGT :
+    en appliquant la correction logistique (voir Sect. 1.2) et le couplage sombre (Chap. 8), on calcule
     \(\delta H_0\) via l’évolution de \(H(z)\) autour du dernier scattering — détails dans la partie “Formules des distances cosmologiques” et “Formules des statistiques \(\chi^2\)”.
-  \item Résultat :
-    MCGT réduit la tension de l’ordre de quelques pour‐cents, ramenant partiellement \(H_0^{\rm rec}\) vers la valeur locale,
+  \item Résultat :
+    MCGT réduit la tension de l’ordre de quelques pour‐cents, ramenant partiellement \(H_0^{\rm rec}\) vers la valeur locale,
     mais sans l’éliminer complètement.
-  \item Perspectives :
-    l’ajout de couplages additionnels ou de nouveaux degrés de liberté (ex. couplage avec neutrinos, champs scalaires dynamiques)
+  \item Perspectives :
+    l’ajout de couplages additionnels ou de nouveaux degrés de liberté (ex. couplage avec neutrinos, champs scalaires dynamiques)
     pourrait amplifier \(\delta H_0\) et tendre vers une concordance complète.
 \end{itemize}

-\noindent\emph{Fin du volet conceptuel du Chapitre 8. La partie opérationnelle détaillée commence ci-dessous.}
\ No newline at end of file
+\noindent\emph{Fin du volet conceptuel du Chapitre 8. La partie opérationnelle détaillée commence ci-dessous.}
diff --git a/08-couplage-sombre/08_couplage_sombre_details.tex b/08-couplage-sombre/08_couplage_sombre_details.tex
index 79e1ce3..2c08c18 100755
--- a/08-couplage-sombre/08_couplage_sombre_details.tex
+++ b/08-couplage-sombre/08_couplage_sombre_details.tex
@@ -16,7 +16,7 @@ Le système détaillé décrivant l’échange d’énergie entre matière sombr

 Les densités aux temps présents sont fixées par :
 \[
-  \rho_{m}(0) = \rho_{c,0}\,\Omega_{m,0},
+  \rho_{m}(0) = \rho_{c,0}\,\Omega_{m,0},
   \quad
   \rho_{\phi}(0) = \rho_{c,0}\,\Omega_{\phi,0},
   \quad
@@ -68,15 +68,15 @@ Par exemple :
 \subsection{Vérifications et cohérence interne}

 \begin{itemize}
-  \item \textbf{Contrôle du rapport global}
-    Calculer
+  \item \textbf{Contrôle du rapport global}
+    Calculer
     \[
       \frac{\chi^{2}_{\rm tot}(Q_{0}^{\star})}{N_{\rm data}}
       \approx 1,
     \]
     où \(N_{\rm data} = N_{\rm SNIa} + N_{\rm BAO} + 1\) est le nombre total de points analysés.

-  \item \textbf{Validation de la contrainte locale \(H_{0}\)}
+  \item \textbf{Validation de la contrainte locale \(H_{0}\)}
     S’assurer que la covariance \(\sigma_{H_{0}} = 1.3\;\mathrm{km/s/Mpc}\) a été correctement appliquée :
     \[
       \chi^{2}_{H_{0}}
@@ -84,26 +84,26 @@ Par exemple :
              {\sigma_{H_{0}}^{2}}.
     \]

-  \item \textbf{Tests ponctuels}
+  \item \textbf{Tests ponctuels}
     \begin{itemize}
-      \item Sur un sous-ensemble de Supernovae Ia :
+      \item Sur un sous-ensemble de Supernovae Ia :
         \[
           \bigl|\mu_{\rm th}(z_i;Q_{0}^{\star}) - \mu_{\rm obs}(z_i)\bigr|
           < \sigma_{\mu,i}.
         \]
-      \item Sur un sous-ensemble de mesures BAO :
+      \item Sur un sous-ensemble de mesures BAO :
         \[
           \bigl|D_{V}(z_j;Q_{0}^{\star}) - D_{V}^{\rm obs}(z_j)\bigr|
           < \sigma_{D_{V},j}.
         \]
-      \item Récupération de \(\Lambda\)CDM (\(Q_{0}=0\)) :
+      \item Récupération de \(\Lambda\)CDM (\(Q_{0}=0\)) :
         \[
           \bigl|H(z;0) - H_{\Lambda\rm CDM}(z)\bigr|
           < 10^{-6}\,H(z).
         \]
     \end{itemize}

-  \item \textbf{Gestion des alertes}
+  \item \textbf{Gestion des alertes}
     Tout écart supérieur à \(3\sigma\) doit déclencher une notification au format :
     \begin{verbatim}
 [ALERTE] Donnée [type] à z=<valeur> hors 3σ : obs=<valeur_obs>, théorie=<valeur_th>
@@ -115,54 +115,54 @@ Par exemple :
 Les opérations s’effectuent via trois scripts, chaque script gérant à la fois son rôle, les options CLI, les sorties, et les étapes clés du workflow :

 \begin{description}
-  \item[\texttt{13\_resolution\_grille\_z.py}]
-    \textbf{Rôle :} résoudre numériquement l’évolution de \(H(z;Q_{0})\) conformément aux équations détaillées dans « Équations complètes du couplage ».
-    \textbf{Options CLI :}
-      \verb|--Q0 <float>|, \verb|--Nz <int>|, \verb|--rtol <float>|, \verb|--atol <float>|.
+  \item[\texttt{13\_resolution\_grille\_z.py}]
+    \textbf{Rôle :} résoudre numériquement l’évolution de \(H(z;Q_{0})\) conformément aux équations détaillées dans « Équations complètes du couplage ».
+    \textbf{Options CLI :}
+      \verb|--Q0 <float>|, \verb|--Nz <int>|, \verb|--rtol <float>|, \verb|--atol <float>|.
     \textbf{Déroulement :}
     \begin{enumerate}
-      \item Lecture de la valeur du paramètre \(Q_{0}\).
-      \item Intégration numérique de \(H(z)\) sur une grille de \(N_z\) points en \(z\).
-      \item Écriture du fichier de sortie \texttt{08\_grille\_Hz\_q0.txt}, listant les paires \(\{z_{i},\,H(z_{i};Q_{0})\}\).
+      \item Lecture de la valeur du paramètre \(Q_{0}\).
+      \item Intégration numérique de \(H(z)\) sur une grille de \(N_z\) points en \(z\).
+      \item Écriture du fichier de sortie \texttt{08\_grille\_Hz\_q0.txt}, listant les paires \(\{z_{i},\,H(z_{i};Q_{0})\}\).
     \end{enumerate}

-  \item[\texttt{13\_couplage\_energie\_sombre.py}]
-    \textbf{Rôle :} orchestrer le balayage de \(Q_{0}\in[0,0.20]\), calculer distances et \(\chi^2\), puis constituer le scan.
-    \textbf{Options CLI :}
-      \verb|--pantheon <fichier_csv>|,
-      \verb|--bao <fichier_csv>|,
-      \verb|--H0 <float>|,
-      \verb|--sigma_H0 <float>|,
-      \verb|--output_scan <fichier_csv>|,
-      \verb|--output_bestfit <fichier_txt>|.
+  \item[\texttt{13\_couplage\_energie\_sombre.py}]
+    \textbf{Rôle :} orchestrer le balayage de \(Q_{0}\in[0,0.20]\), calculer distances et \(\chi^2\), puis constituer le scan.
+    \textbf{Options CLI :}
+      \verb|--pantheon <fichier_csv>|,
+      \verb|--bao <fichier_csv>|,
+      \verb|--H0 <float>|,
+      \verb|--sigma_H0 <float>|,
+      \verb|--output_scan <fichier_csv>|,
+      \verb|--output_bestfit <fichier_txt>|.
     \textbf{Déroulement :}
     \begin{enumerate}
-      \item Lecture des fichiers \texttt{08\_pantheon\_plus.csv} et \texttt{08\_donnees\_bao.csv}.
+      \item Lecture des fichiers \texttt{08\_pantheon\_plus.csv} et \texttt{08\_donnees\_bao.csv}.
       \item Boucle pour chaque \(Q_{0}\) de la grille :
         \begin{itemize}
-          \item Appel à \texttt{13\_resolution\_grille\_z.py} (génère \texttt{08\_grille\_Hz\_q0.txt}).
-          \item Interpolation spline de \(H(z)\).
+          \item Appel à \texttt{13\_resolution\_grille\_z.py} (génère \texttt{08\_grille\_Hz\_q0.txt}).
+          \item Interpolation spline de \(H(z)\).
            \item Calcul des distances et des \(\chi^2\) selon les formules données dans les sections « Formules des distances cosmologiques » et « Formules des statistiques \(\chi^2\) ».
-          \item Écriture d’une ligne \(\{Q_{0},\chi^{2}_{H_{0}},\chi^{2}_{\rm SNIa},\chi^{2}_{\rm BAO},\chi^{2}_{\rm tot}\}\)
-                dans \texttt{08\_q0\_reel\_scan.csv}.
+          \item Écriture d’une ligne \(\{Q_{0},\chi^{2}_{H_{0}},\chi^{2}_{\rm SNIa},\chi^{2}_{\rm BAO},\chi^{2}_{\rm tot}\}\)
+                dans \texttt{08\_q0\_reel\_scan.csv}.
         \end{itemize}
-      \item Interpolation quadratique locale autour du minimum de \(\chi^{2}_{\rm tot}\)
-            pour estimer \(Q_{0}^{\star}\pm\sigma\), sauvegardé dans \texttt{08\_q0\_meilleur\_ajustement.txt}.
+      \item Interpolation quadratique locale autour du minimum de \(\chi^{2}_{\rm tot}\)
+            pour estimer \(Q_{0}^{\star}\pm\sigma\), sauvegardé dans \texttt{08\_q0\_meilleur\_ajustement.txt}.
     \end{enumerate}

-  \item[\texttt{13\_valider\_resultats\_couplage.py}]
-    \textbf{Rôle :} générer les diagnostics visuels à partir du scan complet.
-    \textbf{Options CLI :}
-      \verb|--input_scan <fichier_csv>|,
-      \verb|--output_chi2 <fichier_png>|,
-      \verb|--output_dv <fichier_png>|,
-      \verb|--output_mu <fichier_png>|.
+  \item[\texttt{13\_valider\_resultats\_couplage.py}]
+    \textbf{Rôle :} générer les diagnostics visuels à partir du scan complet.
+    \textbf{Options CLI :}
+      \verb|--input_scan <fichier_csv>|,
+      \verb|--output_chi2 <fichier_png>|,
+      \verb|--output_dv <fichier_png>|,
+      \verb|--output_mu <fichier_png>|.
     \textbf{Déroulement :}
     \begin{enumerate}
-      \item Lecture de \texttt{08\_q0\_reel\_scan.csv}.
-      \item Tracé de la courbe \(\chi^{2}_{\rm tot}(Q_{0})\) dans \texttt{fig\_01\_chi2\_total\_vs\_q0.png}.
-      \item Superposition modèle vs observations BAO dans \texttt{fig\_02\_dv\_vs\_z.png}.
-      \item Superposition modèle vs SNIa dans \texttt{fig\_03\_mu\_vs\_z.png}.
+      \item Lecture de \texttt{08\_q0\_reel\_scan.csv}.
+      \item Tracé de la courbe \(\chi^{2}_{\rm tot}(Q_{0})\) dans \texttt{fig\_01\_chi2\_total\_vs\_q0.png}.
+      \item Superposition modèle vs observations BAO dans \texttt{fig\_02\_dv\_vs\_z.png}.
+      \item Superposition modèle vs SNIa dans \texttt{fig\_03\_mu\_vs\_z.png}.
     \end{enumerate}
 \end{description}

@@ -171,14 +171,14 @@ Les opérations s’effectuent via trois scripts, chaque script gérant à la fo
 Les opérations génèrent deux types de fichiers de résultats :

 \begin{itemize}
-  \item \textbf{Fichier de grille de H(z)}
+  \item \textbf{Fichier de grille de H(z)}
     \texttt{08\_grille\_Hz\_q0.txt} : chaque ligne contient deux colonnes séparées par un espace :
     \[
-      z_i
+      z_i
       \quad H(z_i;Q_{0})\,.
     \]

-  \item \textbf{Fichier de scan statistique}
+  \item \textbf{Fichier de scan statistique}
     \texttt{08\_q0\_reel\_scan.csv} : fichier CSV à cinq colonnes, séparées par des virgules, avec en-tête :
     \begin{verbatim}
 Q0,chi2_H0,chi2_SNIa,chi2_BAO,chi2_tot
@@ -197,53 +197,53 @@ En complément du fichier de scan statistique, nous stockons :
   \item \texttt{08\_mu\_theorie\_q0star.csv} : valeurs de \(\mu_{\rm th}(z_i;Q_0^\star)\) pour chaque redshift \(z_i\).
 \end{itemize}

-Ces deux fichiers servent à tracer les figures
+Ces deux fichiers servent à tracer les figures
 \texttt{fig\_02\_dv\_vs\_z.png} et \texttt{fig\_03\_mu\_vs\_z.png}.

 \subsection{Notes d’implémentation}

-Le résumé conceptuel (08\_couplage\_sombre\_conceptuel.tex) présente formules, définitions, sources de données, figures et résultats chiffrés.
+Le résumé conceptuel (08\_couplage\_sombre\_conceptuel.tex) présente formules, définitions, sources de données, figures et résultats chiffrés.
 Ce document détaille en revanche :

 \begin{itemize}
-  \item l’arborescence et le rôle des scripts (\texttt{13\_resolution\_grille\_z.py}, \texttt{13\_couplage\_energie\_sombre.py}, \texttt{13\_valider\_resultats\_couplage.py}),
-  \item les formats de fichiers produits (\texttt{08\_grille\_Hz\_q0.txt}, \texttt{08\_q0\_reel\_scan.csv}, \texttt{08\_q0\_meilleur\_ajustement.txt}),
+  \item l’arborescence et le rôle des scripts (\texttt{13\_resolution\_grille\_z.py}, \texttt{13\_couplage\_energie\_sombre.py}, \texttt{13\_valider\_resultats\_couplage.py}),
+  \item les formats de fichiers produits (\texttt{08\_grille\_Hz\_q0.txt}, \texttt{08\_q0\_reel\_scan.csv}, \texttt{08\_q0\_meilleur\_ajustement.txt}),
   \item l’organisation technique des boucles, interpolations et alertes.
 \end{itemize}

-Pour ajuster l’intervalle de balayage \(Q_{0}\), le maillage en \(z\) ou les tolérances ODE, modifier les constantes en tête de \texttt{13\_resolution\_grille\_z.py} et \texttt{13\_couplage\_energie\_sombre.py}.
+Pour ajuster l’intervalle de balayage \(Q_{0}\), le maillage en \(z\) ou les tolérances ODE, modifier les constantes en tête de \texttt{13\_resolution\_grille\_z.py} et \texttt{13\_couplage\_energie\_sombre.py}.

 \subsection{Informations complémentaires utiles}

 \begin{itemize}
-  \item \textbf{Environnement et dépendances}
-    Les scripts ont été testés sous Python 3.10.
+  \item \textbf{Environnement et dépendances}
+    Les scripts ont été testés sous Python 3.10.
     Les principales bibliothèques requises sont listées dans \texttt{13\_requirements.txt}, notamment :
     \begin{itemize}
-      \item \texttt{numpy}
-      \item \texttt{scipy}
-      \item \texttt{pandas}
-      \item \texttt{matplotlib}
+      \item \texttt{numpy}
+      \item \texttt{scipy}
+      \item \texttt{pandas}
+      \item \texttt{matplotlib}
     \end{itemize}

-  \item \textbf{Performance / durée}
-    Le balayage complet de \(Q_{0}\) (201 valeurs, résolution ODE sur 1000 points)
+  \item \textbf{Performance / durée}
+    Le balayage complet de \(Q_{0}\) (201 valeurs, résolution ODE sur 1000 points)
     nécessite environ 3 minutes sur un processeur quad-core standard.

-  \item \textbf{Reproductibilité}
+  \item \textbf{Reproductibilité}
     Pour exécuter l’intégralité de la chaîne de calcul en une seule commande, on peut utiliser un script shell dédié, par exemple :
     \begin{verbatim}
 bash run_all_coupling.sh
     \end{verbatim}
     qui appelle successivement :
     \begin{enumerate}
-      \item \texttt{13\_resolution\_grille\_z.py}
-      \item \texttt{13\_couplage\_energie\_sombre.py}
+      \item \texttt{13\_resolution\_grille\_z.py}
+      \item \texttt{13\_couplage\_energie\_sombre.py}
       \item \texttt{13\_valider\_resultats\_couplage.py}
     \end{enumerate}

-  \item \textbf{Répertoire de travail}
-    Les scripts doivent être lancés depuis la racine du projet pour que les chemins relatifs
+  \item \textbf{Répertoire de travail}
+    Les scripts doivent être lancés depuis la racine du projet pour que les chemins relatifs
     (\texttt{08-couplage-sombre/...} et \texttt{13-scripts-annexes/...}) soient correctement résolus.
 \end{itemize}

@@ -253,13 +253,13 @@ bash run_all_coupling.sh
   \item[$Q_{0}$] Paramètre d’interaction matière sombre–énergie sombre, détermine le transfert d’énergie entre \(\rho_{m}\) et \(\rho_{\phi}\).
   \item[$\rho_{m}(0),\;\rho_{\phi}(0)$] Densités initiales aujourd’hui, fixées par \(\rho_{c,0}\,\Omega_{m,0}\) et \(\rho_{c,0}\,\Omega_{\phi,0}\).
   \item[$H_{0}^{\rm local}$] Contrainte locale de la constante de Hubble, \(73.2\pm1.3\)\,km/s/Mpc.
-  \item[$D_{L}(z)$] Distance lumineuse :
+  \item[$D_{L}(z)$] Distance lumineuse :
     \(\displaystyle D_{L}(z) = (1+z)\int_{0}^{z}\frac{c\,dz'}{H(z')}\).
   \item[$D_{V}(z)$] Distance volumétrique :
     \(\displaystyle D_{V}(z) = \Bigl[\frac{c\,z}{H(z)}\Bigl(\int_{0}^{z}\frac{c\,dz'}{H(z')}\Bigr)^{2}\Bigr]^{1/3}.\)
-  \item[$\chi^{2}_{H_{0}}$] Contribution du terme \(H_{0}\) :
+  \item[$\chi^{2}_{H_{0}}$] Contribution du terme \(H_{0}\) :
     \(\displaystyle \frac{[H(0)-H_{0}^{\rm local}]^{2}}{\sigma_{H_{0}}^{2}}.\)
-  \item[$\chi^{2}_{\rm SNIa}$] Statistique Supernovae Ia :
+  \item[$\chi^{2}_{\rm SNIa}$] Statistique Supernovae Ia :
     \(\displaystyle \sum_{i}\frac{[\mu_{\rm th}(z_{i})-\mu_{\rm obs}(z_{i})]^{2}}{\sigma_{\mu,i}^{2}}.\)
   \item[$\chi^{2}_{\rm BAO}$] Statistique BAO :
     \(\displaystyle \sum_{j}\frac{[D_{V}(z_{j})-D_{V}^{\rm obs}(z_{j})]^{2}}{\sigma_{D_{V},j}^{2}}.\)
@@ -276,4 +276,4 @@ bash run_all_coupling.sh
 \end{description}

 \bigskip
-\noindent\emph{Fin de la partie détaillée, Chapitre 8.}
\ No newline at end of file
+\noindent\emph{Fin de la partie détaillée, Chapitre 8.}
diff --git a/09-phase-ondes-gravitationnelles/09_phase_ondes_grav_conceptuel.tex b/09-phase-ondes-gravitationnelles/09_phase_ondes_grav_conceptuel.tex
index 068fbac..776797b 100755
--- a/09-phase-ondes-gravitationnelles/09_phase_ondes_grav_conceptuel.tex
+++ b/09-phase-ondes-gravitationnelles/09_phase_ondes_grav_conceptuel.tex
@@ -30,16 +30,16 @@ Pour relier la fréquence \(f\) à l’âge chirp \(T\), on utilise l’inversio
   \quad\Longrightarrow\quad
   T = T(f)\,,
 \]
-où \(\dot F(f)\) provient de la modélisation IMRPhenomD.
+où \(\dot F(f)\) provient de la modélisation IMRPhenomD.

 \subsection{Paramètres astrophysiques et instrumentaux}
 \begin{itemize}
-  \item \textbf{Source :} binaire de trous noirs, masse chirp
-        \(\mathcal{M}=30\,M_{\odot}\), rapport de masses \(\eta=0.25\), à
+  \item \textbf{Source :} binaire de trous noirs, masse chirp
+        \(\mathcal{M}=30\,M_{\odot}\), rapport de masses \(\eta=0.25\), à
         \(d = 1\,\mathrm{Gpc}\).
   \item \textbf{Plage fréquentielle :}
         \[
-          f_{\min} = 10^{-4}\,\mathrm{Hz},
+          f_{\min} = 10^{-4}\,\mathrm{Hz},
           \quad
           f_{\max} = 1\,\mathrm{Hz}.
         \]
@@ -49,10 +49,10 @@ où \(\dot F(f)\) provient de la modélisation IMRPhenomD.
 \subsection{Schéma conceptuel du phasing modifié}
 Le phasing corrigé s’obtient en deux étapes conceptuelles :
 \begin{enumerate}
-  \item Inversion numérique phase \(\to\) âge chirp \(T(f)\)
+  \item Inversion numérique phase \(\to\) âge chirp \(T(f)\)
         (cf. § « Contexte et définitions »).
-  \item Calcul du retard \(\Delta\tau(f)\) et déphasage
-        \(\Delta\varphi(f)=2\pi\,f\,\Delta\tau(f)\)
+  \item Calcul du retard \(\Delta\tau(f)\) et déphasage
+        \(\Delta\varphi(f)=2\pi\,f\,\Delta\tau(f)\)
         (cf. § « Contexte et définitions »).
 \end{enumerate}

@@ -60,38 +60,38 @@ Le phasing corrigé s’obtient en deux étapes conceptuelles :
 La chaîne conceptuelle se décline en sept étapes principales :

 \begin{enumerate}
-  \item \textbf{Génération de la grille fréquentielle}
+  \item \textbf{Génération de la grille fréquentielle}
     Création d’une grille log-linéaire de fréquences $f\in[10^{-4},1]\,$Hz
     pour assurer la reproductibilité du calcul.

-  \item \textbf{Simulation de la PSD brute}
+  \item \textbf{Simulation de la PSD brute}
     Calcul de la densité spectrale de bruit (PSD) de LISA sur cette grille.

-  \item \textbf{Construction de la PSD simplifiée}
+  \item \textbf{Construction de la PSD simplifiée}
     Nettoyage et interpolation spline de la PSD brute, en garantissant
     $S_n(f)>0$ sur toute la bande.

-  \item \textbf{Calcul des retards et des phases}
+  \item \textbf{Calcul des retards et des phases}
     Pour chaque fréquence, inversion $f\to T$ puis évaluation de
     \(\Delta\tau(f)\) et \(\Delta\varphi(f)=2\pi f\,\Delta\tau(f)\).

-  \item \textbf{Évaluation de la matrice de Fisher}
+  \item \textbf{Évaluation de la matrice de Fisher}
     Construction et inversion de la matrice de Fisher pour extraire les
     incertitudes sur \(\mathcal{M}\) et \(\eta\).

-  \item \textbf{Vérification et journalisation des anomalies}
+  \item \textbf{Vérification et journalisation des anomalies}
     Contrôle systématique de :
     \begin{itemize}
-      \item \(S_n(f)>0\),
-      \item \(0 \le \Delta\tau(f)\le 10^{-1}\,\mathrm{s}\),
+      \item \(S_n(f)>0\),
+      \item \(0 \le \Delta\tau(f)\le 10^{-1}\,\mathrm{s}\),
       \item \(\det C \neq 0\) (matrice de covariance).
     \end{itemize}
     Toutes les violations sont consignées dans un journal d’anomalies.

-  \item \textbf{Extraction et visualisation des résultats}
+  \item \textbf{Extraction et visualisation des résultats}
     \begin{itemize}
-      \item Extraction du déphasage optimal pour la valeur choisie de \(Q_0\).
-      \item Tracé des ellipses de confiance (68 % & 95 %).
+      \item Extraction du déphasage optimal pour la valeur choisie de \(Q_0\).
+      \item Tracé des ellipses de confiance (68 % & 95 %).
       \item (Optionnel) Validation comparative de la différence de phase.
     \end{itemize}
 \end{enumerate}
@@ -125,7 +125,7 @@ Les principaux fichiers produits par la chaîne de traitement sont :
   \item \texttt{fig\_04\_validation\_delta\_phase.png} : validation de \(\Delta\varphi(f)\) sur la bande complète LISA.
 \end{itemize}

-Pour le format exact des colonnes, les tolérances et les exemples de commandes, voir la section « Détails opérationnels ».
+Pour le format exact des colonnes, les tolérances et les exemples de commandes, voir la section « Détails opérationnels ».

 \subsection{Résultats numériques clés}
 Les ordres de grandeur obtenus pour la configuration considérée sont :
@@ -138,10 +138,10 @@ Les ordres de grandeur obtenus pour la configuration considérée sont :
   \quad
   \frac{\Delta\eta}{\eta} \sim 2 \times 10^{-3}.
 \]
-Ces résultats indiquent que, même pour un binaire de masse chirp
-\(\mathcal{M}=30\,M_{\odot}\) et \(\eta=0.25\), l’effet du MCGT sur la phase
-des ondes gravitationnelles reste potentiellement détectable par un instrument
-de type LISA (voir § « Détails opérationnels » pour les valeurs exactes et
+Ces résultats indiquent que, même pour un binaire de masse chirp
+\(\mathcal{M}=30\,M_{\odot}\) et \(\eta=0.25\), l’effet du MCGT sur la phase
+des ondes gravitationnelles reste potentiellement détectable par un instrument
+de type LISA (voir § « Détails opérationnels » pour les valeurs exactes et
 leurs tableaux associés).

 \subsection{Figures principales}
@@ -177,10 +177,10 @@ La figure ci-dessous présente la validation du déphasage \(\Delta\varphi(f)\)
   \label{fig:validation_delta_phase_full}
 \end{figure}

-Ce tracé confirme que, même si le retard \(\Delta\tau(f)\) diminue à haute fréquence, le produit \(f\,\Delta\tau(f)\) croît suffisamment pour rendre le déphasage plus marqué près de \(1\) Hz.
+Ce tracé confirme que, même si le retard \(\Delta\tau(f)\) diminue à haute fréquence, le produit \(f\,\Delta\tau(f)\) croît suffisamment pour rendre le déphasage plus marqué près de \(1\) Hz.

 \subsection{Matrice de Fisher et incertitudes}
-La matrice de Fisher pour les paramètres
+La matrice de Fisher pour les paramètres
 \(\theta = (\mathcal{M},\,\eta)\) se définit formellement par :
 \[
   F_{ij}
@@ -199,9 +199,9 @@ Les incertitudes s’extraient via l’inversion de \(\mathbf{F}\) :
   \Delta\eta
   = \sqrt{\bigl(\mathbf{F}^{-1}\bigr)_{22}}.
 \]
-Pour la configuration considérée, on obtient
-\(\Delta\mathcal{M}/\mathcal{M}\sim10^{-3}\) et
-\(\Delta\eta/\eta\sim2\times10^{-3}\)
+Pour la configuration considérée, on obtient
+\(\Delta\mathcal{M}/\mathcal{M}\sim10^{-3}\) et
+\(\Delta\eta/\eta\sim2\times10^{-3}\)
 (voir § « Détails opérationnels » pour la procédure numérique complète).

 \subsection{Références}
@@ -213,28 +213,28 @@ Pour la configuration considérée, on obtient

 \subsection{Limites et perspectives}
 \begin{itemize}
-  \item \textbf{Principales hypothèses}
+  \item \textbf{Principales hypothèses}
     Phasing quadratique (IMRPhenomD), PSD simplifiée, absence de bruit de confusion galactique.
-  \item \textbf{Axes d’amélioration}
+  \item \textbf{Axes d’amélioration}
     \begin{itemize}
-      \item Tester des phasings plus précis (PhenomX, calibrations numériquement ajustées).
-      \item Intégrer le bruit de confusion galactique et des contributions astrophysiques additionnelles.
+      \item Tester des phasings plus précis (PhenomX, calibrations numériquement ajustées).
+      \item Intégrer le bruit de confusion galactique et des contributions astrophysiques additionnelles.
       \item Étendre l’analyse à d’autres infrastructures (Einstein Telescope, Cosmic Explorer).
     \end{itemize}
 \end{itemize}

 \subsubsection*{Sensibilité minimale}
-Le seuil de détection de LISA est de l’ordre
+Le seuil de détection de LISA est de l’ordre
 \[
   \Delta\varphi_{\min}\sim10^{-3}\,\mathrm{rad}.
 \]
-Nos déphasages calculés \(\mathcal{O}(10^{-3})\) rad se situent donc juste au seuil de détectabilité ;
+Nos déphasages calculés \(\mathcal{O}(10^{-3})\) rad se situent donc juste au seuil de détectabilité ;
 tout effet plus faible exigerait un rapport signal-à-bruit plus élevé ou un réseau conjoint de détecteurs.

 \subsection{Conclusion conceptuelle}
 Pour un binaire de masse chirp \(\mathcal{M}=30\,M_{\odot}\), \(\eta=0.25\), à 1 Gpc et SNR = 20, on obtient :
 \[
-  \Delta\tau \sim 10^{-4}\!-\!10^{-2}\,\mathrm{s},
+  \Delta\tau \sim 10^{-4}\!-\!10^{-2}\,\mathrm{s},
   \quad
   \Delta\varphi \sim 10^{-3}\,\mathrm{rad},
   \quad
@@ -244,4 +244,4 @@ Pour un binaire de masse chirp \(\mathcal{M}=30\,M_{\odot}\), \(\eta=0.25\), à
 \]
 Ces ordres de grandeur placent l’effet MCGT au seuil de ce qu’un instrument LISA-like peut mesurer, confirmant la faisabilité expérimentale de ce test cosmologique.

-\noindent\emph{Fin du volet conceptuel du Chapitre 9. La partie opérationnelle détaillée commence ci-dessous.}
\ No newline at end of file
+\noindent\emph{Fin du volet conceptuel du Chapitre 9. La partie opérationnelle détaillée commence ci-dessous.}
diff --git a/09-phase-ondes-gravitationnelles/09_phase_ondes_grav_details.tex b/09-phase-ondes-gravitationnelles/09_phase_ondes_grav_details.tex
index 66f750c..a4b8619 100755
--- a/09-phase-ondes-gravitationnelles/09_phase_ondes_grav_details.tex
+++ b/09-phase-ondes-gravitationnelles/09_phase_ondes_grav_details.tex
@@ -16,9 +16,9 @@ Le calcul complet s’appuie sur dix scripts principaux et un fichier de dépen

   \item \texttt{13\_generer\_journal\_anomalies.py} : vérifie :
     \begin{itemize}
-      \item $S_n(f)>0$,
-      \item $0 \le \Delta\tau(f)\le10^{-1}\,\mathrm{s}$,
-      \item $\det\mathbf{F}\neq0$,
+      \item $S_n(f)>0$,
+      \item $0 \le \Delta\tau(f)\le10^{-1}\,\mathrm{s}$,
+      \item $\det\mathbf{F}\neq0$,
     \end{itemize}
     et consigne chaque anomalie dans \texttt{09\_anomalies.log}.

@@ -80,49 +80,49 @@ Le calcul complet s’appuie sur dix scripts principaux et un fichier de dépen
 \end{verbatim}

 \begin{itemize}
-  \item \textbf{Version Python et dépendances :}
-    Python \(\ge3.8\) (voir \texttt{13\_requirements.txt}).
+  \item \textbf{Version Python et dépendances :}
+    Python \(\ge3.8\) (voir \texttt{13\_requirements.txt}).
     Packages : \texttt{numpy}, \texttt{scipy}, \texttt{pandas}, \texttt{matplotlib}, \texttt{lalsuite}, \texttt{lalsimulation}.

-  \item \textbf{Grille fréquentielle :}
+  \item \textbf{Grille fréquentielle :}
     \[
       N_{f} = 10\,000,\quad
       f_{i} = 10^{\log_{10}(f_{\min}) + \frac{i}{N_{f}-1}\bigl(\log_{10}(f_{\max})-\log_{10}(f_{\min})\bigr)},
       \]
-    avec \(f_{\min}=10^{-4}\,\mathrm{Hz}\), \(f_{\max}=1\,\mathrm{Hz}\).
+    avec \(f_{\min}=10^{-4}\,\mathrm{Hz}\), \(f_{\max}=1\,\mathrm{Hz}\).
     Générée par \texttt{13\_generer\_grille\_frequentielle.py} et stockée dans \texttt{12\_grille\_frequentielle.csv} (colonne unique \texttt{f} en Hz).

   \item \textbf{Tolérances d’intégration :}
     \begin{itemize}
-      \item \(\varphi\to T\) (inversion numérique) : \(\mathrm{rtol}=\mathrm{atol}=10^{-10}\).
+      \item \(\varphi\to T\) (inversion numérique) : \(\mathrm{rtol}=\mathrm{atol}=10^{-10}\).
       \item Calcul de \(\Delta\tau\) (trapèzes en \(\ln f\)) : pas variable en \(\ln f\).
     \end{itemize}

   \item \textbf{Fichiers PSD :}
     \begin{itemize}
-      \item \texttt{12\_psd\_lisa\_brute.csv} (CSV, entête \texttt{f,S\_n}) :
-        \texttt{f} [Hz], \texttt{S\_n} [Hz\(^{-1}\)].
-      \item \texttt{12\_psd\_lisa\_simplifiee.dat} (ASCII, deux colonnes séparées par un espace) :
+      \item \texttt{12\_psd\_lisa\_brute.csv} (CSV, entête \texttt{f,S\_n}) :
+        \texttt{f} [Hz], \texttt{S\_n} [Hz\(^{-1}\)].
+      \item \texttt{12\_psd\_lisa\_simplifiee.dat} (ASCII, deux colonnes séparées par un espace) :
         \texttt{f} [Hz], \texttt{S\_n} [Hz\(^{-1}\)], interpolée en spline cubique et strictement positive sur la bande.
     \end{itemize}

-  \item \textbf{Tableau \(\dot P(T)\) :}
-    \texttt{02\_pdot\_tableau\_plateau\_fine.dat} (deux colonnes \texttt{T, Pdot}).
+  \item \textbf{Tableau \(\dot P(T)\) :}
+    \texttt{02\_pdot\_tableau\_plateau\_fine.dat} (deux colonnes \texttt{T, Pdot}).
     Utilisé par \texttt{13\_generer\_retards\_phases.py} pour interpoler \(\dot P\).

-  \item \textbf{Format de \texttt{09\_gw\_resultats.csv} :}
+  \item \textbf{Format de \texttt{09\_gw\_resultats.csv} :}
     CSV, entête \texttt{f,DeltaTau,DeltaPhase} :
     \begin{itemize}
-      \item \texttt{f} [Hz] (scientifique),
-      \item \texttt{DeltaTau} [s] (\(\Delta\tau(f)\), \texttt{\%e}),
+      \item \texttt{f} [Hz] (scientifique),
+      \item \texttt{DeltaTau} [s] (\(\Delta\tau(f)\), \texttt{\%e}),
       \item \texttt{DeltaPhase} [rad] (\(\Delta\varphi(f)\), \texttt{\%e}).
     \end{itemize}

-  \item \textbf{Format de \texttt{09\_matrice\_fisher.csv} :}
-    CSV sans entête, quatre valeurs \(\{F_{11},F_{12},F_{21},F_{22}\}\) séparées par « , »,
+  \item \textbf{Format de \texttt{09\_matrice\_fisher.csv} :}
+    CSV sans entête, quatre valeurs \(\{F_{11},F_{12},F_{21},F_{22}\}\) séparées par « , »,
     avec \(F_{12}=F_{21}\), unités \(\mathrm{Hz}^0\).

-  \item \textbf{Seuils de validation} (journal \texttt{09\_anomalies.log}) :
+  \item \textbf{Seuils de validation} (journal \texttt{09\_anomalies.log}) :
     Vérifications par \texttt{13\_generer\_journal\_anomalies.py} :
     \[
       S_{n}(f)>0,\quad
@@ -130,13 +130,13 @@ Le calcul complet s’appuie sur dix scripts principaux et un fichier de dépen
       \det F\neq0.
     \]

-  \item \textbf{Options communes des scripts :}
-    \verb|--freq-grid| (grille CSV),
-    \verb|--psd-file| (PSD simplifiée),
-    \verb|--pdot-file| (\(\dot P\) DAT),
-    \verb|--snr| (rapport SNR, ex.~20),
-    \verb|--nf| (points grille, ex.~10000),
-    \verb|--out_csv| (sortie CSV),
+  \item \textbf{Options communes des scripts :}
+    \verb|--freq-grid| (grille CSV),
+    \verb|--psd-file| (PSD simplifiée),
+    \verb|--pdot-file| (\(\dot P\) DAT),
+    \verb|--snr| (rapport SNR, ex.~20),
+    \verb|--nf| (points grille, ex.~10000),
+    \verb|--out_csv| (sortie CSV),
     \verb|--out_log| (journal anomalies).
 \end{itemize}

@@ -145,11 +145,11 @@ Python (≥ 3.8) et les bibliothèques suivantes sont nécessaires :
 \[
   \texttt{numpy},\;\texttt{scipy},\;\texttt{pandas},\;\texttt{matplotlib},\;\texttt{lalsuite},
 \]
-ainsi que \texttt{lalsimulation} (LALSuite v6.10+) pour IMRPhenomD.
+ainsi que \texttt{lalsimulation} (LALSuite v6.10+) pour IMRPhenomD.
 Il est recommandé d’utiliser un environnement isolé (virtualenv ou Conda).

 \subsection{Paramètres astrophysiques et instrumentaux}
-Pour la description complète de ces paramètres, voir § « Paramètres astrophysiques et instrumentaux » du volet conceptuel.
+Pour la description complète de ces paramètres, voir § « Paramètres astrophysiques et instrumentaux » du volet conceptuel.
 Ici, on se contente de rappeler les valeurs numériques utilisées :

 \begin{itemize}
@@ -285,11 +285,11 @@ Chaque anomalie détectée est consignée, ligne par ligne, dans \texttt{09\_ano
 Cette étape s’appuie désormais sur deux scripts complémentaires :

 \begin{itemize}
-  \item \textbf{Génération du CSV complet des phases}
-    \texttt{13\_generer\_phases\_completes.py} : produit le fichier
-    \texttt{09\_phases\_gw.csv} contenant, pour chaque fréquence, la phase
-    IMRPhenomD (\(\varphi_{\Lambda\mathrm{CDM}}\)) et la phase MCGT
-    (\(\varphi_{\mathrm{MCGT}}\)).
+  \item \textbf{Génération du CSV complet des phases}
+    \texttt{13\_generer\_phases\_completes.py} : produit le fichier
+    \texttt{09\_phases\_gw.csv} contenant, pour chaque fréquence, la phase
+    IMRPhenomD (\(\varphi_{\Lambda\mathrm{CDM}}\)) et la phase MCGT
+    (\(\varphi_{\mathrm{MCGT}}\)).
     \textbf{Usage :}
     \begin{verbatim}
 python 13_generer_phases_completes.py \
@@ -299,10 +299,10 @@ python 13_generer_phases_completes.py \
   --out-csv   09_phases_gw.csv
     \end{verbatim}

-  \item \textbf{Vérification comparative des phases}
-    \texttt{13\_valider\_phase\_gw.py} : lit \texttt{09\_phases\_gw.csv}, calcule
-    \(\Delta\varphi(f)=\varphi_{\mathrm{MCGT}}(f)-\varphi_{\Lambda\mathrm{CDM}}(f)\)
-    et trace son évolution.
+  \item \textbf{Vérification comparative des phases}
+    \texttt{13\_valider\_phase\_gw.py} : lit \texttt{09\_phases\_gw.csv}, calcule
+    \(\Delta\varphi(f)=\varphi_{\mathrm{MCGT}}(f)-\varphi_{\Lambda\mathrm{CDM}}(f)\)
+    et trace son évolution.
     \textbf{Usage :}
     \begin{verbatim}
 python 13_valider_phase_gw.py \
@@ -337,11 +337,11 @@ Le fichier \texttt{09\_anomalies.log}, produit automatiquement par \texttt{13\_g

 \noindent Plus précisément :
 \begin{itemize}
-  \item \texttt{[PSD]}      \, f=\<valeur\> Hz \(\to\) S\_n=\<valeur\>
-        (lorsque \(S_{n}(f)\le 0\)).
-  \item \texttt{[DeltaTau]} \, f=\<valeur\> Hz \(\to\) DeltaTau=\<valeur\> s
-        (lorsque \(\Delta\tau(f)<0\) ou \(\Delta\tau(f)>10^{-1}\) s).
-  \item \texttt{[Fisher]}   \, det(C)=\<valeur\>
+  \item \texttt{[PSD]}      \, f=\<valeur\> Hz \(\to\) S\_n=\<valeur\>
+        (lorsque \(S_{n}(f)\le 0\)).
+  \item \texttt{[DeltaTau]} \, f=\<valeur\> Hz \(\to\) DeltaTau=\<valeur\> s
+        (lorsque \(\Delta\tau(f)<0\) ou \(\Delta\tau(f)>10^{-1}\) s).
+  \item \texttt{[Fisher]}   \, det(C)=\<valeur\>
         (lorsque \(\det C=0\), matrice singulière).
 \end{itemize}

@@ -359,11 +359,11 @@ Chaque valeur numérique est affichée en notation scientifique, exemple :
 La mise en œuvre numérique suit ces étapes :

 \begin{enumerate}
-  \item \textbf{Chargement de la PSD et des paramètres :}
+  \item \textbf{Chargement de la PSD et des paramètres :}
   lire \texttt{12\_psd\_lisa\_simplifiee.dat} (\(f,S_{n}(f)\)), fixer la SNR (20).

-  \item \textbf{Intégration numérique :}
-    calculer
+  \item \textbf{Intégration numérique :}
+    calculer
     \[
       F_{ij}
       =4\,\mathrm{SNR}^{2}
@@ -375,18 +375,18 @@ La mise en œuvre numérique suit ces étapes :
     \]
     par quadrature adaptée (trapèzes en \(\log f\) ou méthode de Simpson).

-  \item \textbf{Construction et inversion de la matrice de Fisher :}
-  calculer directement
-  \(\;F_{ij}=4\,\mathrm{SNR}^{2}\!\int\!\cdots\), imposer \(F_{12}=F_{21}\)
+  \item \textbf{Construction et inversion de la matrice de Fisher :}
+  calculer directement
+  \(\;F_{ij}=4\,\mathrm{SNR}^{2}\!\int\!\cdots\), imposer \(F_{12}=F_{21}\)
   puis inverser la matrice de Fisher \(\mathbf{F}\) pour obtenir \(\mathbf{F}^{-1}\).

-  \item \textbf{Extraction des incertitudes :}
-    calculer
-    \(\Delta\mathcal{M}=\sqrt{(\mathbf{F}^{-1})_{11}}\) et
+  \item \textbf{Extraction des incertitudes :}
+    calculer
+    \(\Delta\mathcal{M}=\sqrt{(\mathbf{F}^{-1})_{11}}\) et
     \(\Delta\eta=\sqrt{(\mathbf{F}^{-1})_{22}}\).

-  \item \textbf{Vérifications :}
-  vérifier \(\det\mathbf{F}\neq0\) (matrice non singulière) et, en cas d’échec,
+  \item \textbf{Vérifications :}
+  vérifier \(\det\mathbf{F}\neq0\) (matrice non singulière) et, en cas d’échec,
   consigner l’anomalie dans \texttt{09\_anomalies.log}.
 \end{enumerate}

@@ -394,34 +394,34 @@ La mise en œuvre numérique suit ces étapes :
 Pour garantir la robustesse et la cohérence de l’ensemble du pipeline, les vérifications suivantes sont effectuées et toutes anomalies consignées dans \texttt{09\_anomalies.log} :

 \begin{itemize}
-  \item \textbf{PSD strictement positive}
-    Vérifier que la densité spectrale de bruit simplifiée
-    \(\,S_{n}(f)>0\) pour tout
-    \(f\in[10^{-4},\,1]\)\;Hz.
-    Toute valeur \(S_{n}(f)\le0\) déclenche une entrée
+  \item \textbf{PSD strictement positive}
+    Vérifier que la densité spectrale de bruit simplifiée
+    \(\,S_{n}(f)>0\) pour tout
+    \(f\in[10^{-4},\,1]\)\;Hz.
+    Toute valeur \(S_{n}(f)\le0\) déclenche une entrée
     \verb|[PSD] f=<valeur> Hz -> S_n=<valeur>|.

-  \item \textbf{Cas \(\Lambda\)CDM (\(Q_{0}=0\))}
-    En forçant \(\dot P(T)=1\) dans le script de génération des retards,
-    s’assurer que \(\Delta\tau(f)=0\) sur toute la bande.
+  \item \textbf{Cas \(\Lambda\)CDM (\(Q_{0}=0\))}
+    En forçant \(\dot P(T)=1\) dans le script de génération des retards,
+    s’assurer que \(\Delta\tau(f)=0\) sur toute la bande.
     Toute déviation est signalée comme \verb|[DeltaTau] f=<valeur> Hz -> DeltaTau≠0|.

-  \item \textbf{Bornes sur \(\Delta\tau\)}
-    Contrôler que
+  \item \textbf{Bornes sur \(\Delta\tau\)}
+    Contrôler que
     \[
       0 \;\le\;\Delta\tau(f)\;\le\;10^{-1}\,\mathrm{s}
       \quad\forall\,f\in[10^{-4},\,1]\;\mathrm{Hz}.
     \]
-    Toute valeur hors plage génère une entrée
+    Toute valeur hors plage génère une entrée
     \verb|[DeltaTau] f=<valeur> Hz -> DeltaTau=<valeur> s > 1e-1 s|.

-  \item \textbf{Invertibilité de la matrice de Fisher}
-    Après construction de
-    \(\mathbf{F}\) selon
-    \(\;F_{ij}=4\,\mathrm{SNR}^{2}\int\cdots\),
-    imposer la symétrie \(F_{12}=F_{21}\) puis vérifier
-    \(\det\mathbf{F}\neq0\).
-    En cas de singularité, consigner
+  \item \textbf{Invertibilité de la matrice de Fisher}
+    Après construction de
+    \(\mathbf{F}\) selon
+    \(\;F_{ij}=4\,\mathrm{SNR}^{2}\int\cdots\),
+    imposer la symétrie \(F_{12}=F_{21}\) puis vérifier
+    \(\det\mathbf{F}\neq0\).
+    En cas de singularité, consigner
     \verb|[Fisher] det(F)=0 -> matrice singulière|.
 \end{itemize}

@@ -435,56 +435,56 @@ Pour garantir la robustesse et la cohérence de l’ensemble du pipeline, les v
 \subsection*{Glossaire}

 \begin{description}
-  \item[\texttt{12\_grille\_frequentielle.csv}]
+  \item[\texttt{12\_grille\_frequentielle.csv}]
     Grille log‐linéaire de fréquences $f\in[10^{-4},1]\,$Hz, générée par \texttt{13\_generer\_grille\_frequentielle.py} (colonne unique \texttt{f} en Hz).

-  \item[$N_{f}$]
+  \item[$N_{f}$]
     Nombre de points de la grille fréquentielle (ici $N_{f}=10\,000$).

-  \item[\texttt{12\_psd\_lisa\_brute.csv}]
+  \item[\texttt{12\_psd\_lisa\_brute.csv}]
     PSD brute simulée de LISA, deux colonnes \{\texttt{f},\texttt{S\_n}\}, générée par \texttt{13\_calculer\_psd\_brute.py}.

-  \item[\texttt{12\_psd\_lisa\_simplifiee.dat}]
+  \item[\texttt{12\_psd\_lisa\_simplifiee.dat}]
     PSD simplifiée (strictement positive), deux colonnes \{\texttt{f},\texttt{S\_n}\}, interpolée en spline cubique par \texttt{13\_construire\_psd\_simplifiee.py}.

-  \item[\texttt{02\_pdot\_tableau\_plateau\_fine.dat}]
+  \item[\texttt{02\_pdot\_tableau\_plateau\_fine.dat}]
     Tableau $\{T_i,\dot P(T_i)\}$ utilisé pour interpoler $\dot P$ dans \texttt{13\_generer\_retards\_phases.py}.

-  \item[$\Delta\tau(f)$]
-    Retard de propagation induit par MCGT :
+  \item[$\Delta\tau(f)$]
+    Retard de propagation induit par MCGT :
     $\displaystyle \Delta\tau(f)
       =\int_{f_{\min}}^{f}\bigl[1 - 1/\dot P(T(f'))\bigr]\,\mathrm{d}\ln f'$.

-  \item[$\Delta\varphi(f)$]
-    Déphasage associé :
+  \item[$\Delta\varphi(f)$]
+    Déphasage associé :
     $\Delta\varphi(f)=2\pi\,f\,\Delta\tau(f)$.

-  \item[\texttt{09\_gw\_resultats.csv}]
+  \item[\texttt{09\_gw\_resultats.csv}]
     Résultats des retards et déphasages, colonnes \texttt{f,DeltaTau,DeltaPhase}.

-  \item[\texttt{09\_matrice\_fisher.csv}]
-    Matrice de Fisher $F_{ij}$ (sans entête), format
+  \item[\texttt{09\_matrice\_fisher.csv}]
+    Matrice de Fisher $F_{ij}$ (sans entête), format
     \texttt{F11,F12,F21,F22} avec $F_{12}=F_{21}$.

-  \item[\texttt{09\_phases\_gw.csv}]
+  \item[\texttt{09\_phases\_gw.csv}]
     CSV complet des phases, colonnes \texttt{f,phi\_LCDM,phi\_MCGT}, produit par \texttt{13\_generer\_phases\_completes.py}.

-  \item[\texttt{09\_phase\_diff\_q0star.csv}]
+  \item[\texttt{09\_phase\_diff\_q0star.csv}]
     Colonne unique \texttt{DeltaPhase} pour la valeur optimale $Q_{0}^{\star}$, extraite par \texttt{13\_extraire\_dephasage\_optimal.py}.

-  \item[\texttt{09\_anomalies.log}]
+  \item[\texttt{09\_anomalies.log}]
     Journal des anomalies (PSD non‐positive, retards hors bornes, Fisher singulière) consigné par \texttt{13\_generer\_journal\_anomalies.py}.

-  \item[\texttt{fig\_03\_fisher\_contour.png}]
+  \item[\texttt{fig\_03\_fisher\_contour.png}]
     Ellipses de confiance (68 \% & 95 \%) dans le plan $(\mathcal{M},\eta)$, tracées par \texttt{13\_tracer\_ellipses\_fisher.py}.

-  \item[\texttt{fig\_04\_validation\_delta\_phase.png}]
+  \item[\texttt{fig\_04\_validation\_delta\_phase.png}]
     Validation du déphasage sur la bande complète LISA, générée par \texttt{13\_valider\_phase\_gw.py}.

-  \item[\texttt{--freq-grid}, \texttt{--psd-file}, \texttt{--pdot-file}, …]
+  \item[\texttt{--freq-grid}, \texttt{--psd-file}, \texttt{--pdot-file}, …]
     Principaux flags de ligne de commande pour les scripts du Chapitre 9 (voir § « Paramètres numériques et configuration »).
 \end{description}


 \bigskip
-\noindent\emph{Fin de la partie détaillée, Chapitre 9.}
\ No newline at end of file
+\noindent\emph{Fin de la partie détaillée, Chapitre 9.}
diff --git a/09-phase-ondes-gravitationnelles/CHAPTER09_GUIDE.txt b/09-phase-ondes-gravitationnelles/CHAPTER09_GUIDE.txt
index 63ecabc..631a012 100755
--- a/09-phase-ondes-gravitationnelles/CHAPTER09_GUIDE.txt
+++ b/09-phase-ondes-gravitationnelles/CHAPTER09_GUIDE.txt
@@ -107,58 +107,58 @@ Métadonnées : `09_jalons_comparaison.meta.json` (catalogue, règles d’incert
 3) SCRIPTS D’EXÉCUTION (dossier `zz-scripts/chapter09/`)
 --------------------------------------------------------

-- extract_phenom_phase.py
-  Extrait la phase GR de référence (IMRPhenomD) sur la grille f_Hz.
+- extract_phenom_phase.py
+  Extrait la phase GR de référence (IMRPhenomD) sur la grille f_Hz.
   Sorties : `09_phases_imrphenom.csv`, `09_phases_imrphenom.meta.json`.

-- generate_mcgt_raw_phase.py  (optionnel)
-  Reconstruit φ_mcgt “brute” (avant correction) pour diagnostic.
+- generate_mcgt_raw_phase.py  (optionnel)
+  Reconstruit φ_mcgt “brute” (avant correction) pour diagnostic.
   Sorties : `09_phases_mcgt_prepoly.csv` (colonnes `phi_mcgt_raw`, `phi_mcgt_cal` si calage dispo).

-- generate_data_chapter09.py
-  Pipeline de base : produit la série **pré-correction** et le résidu standardisé.
-  Entrées : `09_phases_imrphenom.csv`.
+- generate_data_chapter09.py
+  Pipeline de base : produit la série **pré-correction** et le résidu standardisé.
+  Entrées : `09_phases_imrphenom.csv`.
   Sorties : `09_phases_mcgt_prepoly.csv`, `09_phase_diff.csv` (|Δφ|_principal).

-- apply_poly_unwrap_rebranch.py
-  Applique le fit polynomiale, unwrap et rebranch (k) ; écrit la série finale.
-  Entrées : `09_phases_mcgt_prepoly.csv`.
+- apply_poly_unwrap_rebranch.py
+  Applique le fit polynomiale, unwrap et rebranch (k) ; écrit la série finale.
+  Entrées : `09_phases_mcgt_prepoly.csv`.
   Sorties : `09_phases_mcgt.csv`, mise à jour `09_metrics_phase.json`.

-- opt_poly_rebranch.py
-  Recherche (base, degré, k) minimisant p95 (20–300 Hz) ; exporte les meilleurs paramètres.
-  Entrées : `09_phases_mcgt_prepoly.csv`.
+- opt_poly_rebranch.py
+  Recherche (base, degré, k) minimisant p95 (20–300 Hz) ; exporte les meilleurs paramètres.
+  Entrées : `09_phases_mcgt_prepoly.csv`.
   Sorties : `09_phases_mcgt.csv`, `09_best_params.json`, `09_metrics_phase.json`.

-- check_p95_methods.py
+- check_p95_methods.py
   Contrôle méthodologique : compare p95 (raw/unwrap/principal), teste k±1, vérifie l’effet d’un ancrage visuel,
-  et peut mettre à jour les métriques.
-  Sorties : `zz-figures/chapter09/p95_check_control.png`, logs.
+  et peut mettre à jour les métriques.
+  Sorties : `zz-figures/chapter09/p95_check_control.png`, logs.
   (Peut aussi servir à **ré-écrire** `metrics_active` à partir du “principal” si demandé.)

-- plot_fig01_phase_overlay.py
-  Superpose φ_ref & φ_mcgt (log-x) avec encart du résidu |Δφ| ; options d’ancrage visuel.
-  Entrées : `09_phases_mcgt.csv`, `09_metrics_phase.json`.
+- plot_fig01_phase_overlay.py
+  Superpose φ_ref & φ_mcgt (log-x) avec encart du résidu |Δφ| ; options d’ancrage visuel.
+  Entrées : `09_phases_mcgt.csv`, `09_metrics_phase.json`.
   Sorties : `fig_01_phase_overlay.png`.

-- plot_fig02_residual_phase.py
-  Tracé |Δφ|_principal par bandes (20–300, 300–1000, 1000–2000 Hz) avec p95 par panneau.
-  Entrées : `09_phases_mcgt.csv`, `09_metrics_phase.json`.
+- plot_fig02_residual_phase.py
+  Tracé |Δφ|_principal par bandes (20–300, 300–1000, 1000–2000 Hz) avec p95 par panneau.
+  Entrées : `09_phases_mcgt.csv`, `09_metrics_phase.json`.
   Sorties : `fig_02_residual_phase.png`.

-- plot_fig03_hist_absdphi_20_300.py
-  Histogramme de |Δφ| (20–300 Hz), échelle log recommandée, lignes moyenne/médiane/p95/max.
-  Entrées : `09_phase_diff.csv` (prioritaire) ou `09_phases_mcgt.csv`.
+- plot_fig03_hist_absdphi_20_300.py
+  Histogramme de |Δφ| (20–300 Hz), échelle log recommandée, lignes moyenne/médiane/p95/max.
+  Entrées : `09_phase_diff.csv` (prioritaire) ou `09_phases_mcgt.csv`.
   Sorties : `fig_03_hist_absdphi_20_300.png`.

-- plot_fig04_absdphi_milestones_vs_f.py
-  |Δφ|(f) aux jalons GWTC-3 avec barres d’erreur (±σ) et fond optionnel de résidu.
-  Entrées : `09_jalons_comparaison.csv` (+ `09_phase_diff.csv` / `09_metrics_phase.json` facultatifs).
+- plot_fig04_absdphi_milestones_vs_f.py
+  |Δφ|(f) aux jalons GWTC-3 avec barres d’erreur (±σ) et fond optionnel de résidu.
+  Entrées : `09_jalons_comparaison.csv` (+ `09_phase_diff.csv` / `09_metrics_phase.json` facultatifs).
   Sorties : `fig_04_absdphi_milestones_vs_f.png`.

-- plot_fig05_scatter_phi_at_fpeak.py
-  Nuage φ_ref(f_peak) vs φ_mcgt(f_peak) coloré par classe ; barres d’erreur ±σ.
-  Entrées : `09_jalons_comparaison.csv`.
+- plot_fig05_scatter_phi_at_fpeak.py
+  Nuage φ_ref(f_peak) vs φ_mcgt(f_peak) coloré par classe ; barres d’erreur ±σ.
+  Entrées : `09_jalons_comparaison.csv`.
   Sorties : `fig_05_scatter_phi_at_fpeak.png`.


@@ -376,4 +376,3 @@ pdflatex -interaction=nonstopmode 09_phase_ondes_grav_details.tex
 - Mettre à jour `CHANGELOG.md` (entrée “Chapitre 9 – Première version”)
 - Créer un tag Git : `git tag -a v9.0 -m "Chapitre 9 – Phase et ondes gravitationnelles"`
 - Pousser le tag : `git push --tags`
-
diff --git a/10-monte-carlo-global-8d/10_monte_carlo_global_conceptuel.tex b/10-monte-carlo-global-8d/10_monte_carlo_global_conceptuel.tex
index 390609f..eb1e36b 100755
--- a/10-monte-carlo-global-8d/10_monte_carlo_global_conceptuel.tex
+++ b/10-monte-carlo-global-8d/10_monte_carlo_global_conceptuel.tex
@@ -3,7 +3,7 @@
 \subsection{10.1 – Définition du vecteur de paramètres \(\Theta\)}
 On considère un vecteur à huit composantes :
 \[
-  \Theta \;=\;
+  \Theta \;=\;
   \bigl(\alpha_{1},\,T_{c},\,\Delta,\,T_{p},\,\Delta_{p},\,\beta,\,
            \Omega_{m},\,H_{0}\bigr).
 \]
@@ -13,10 +13,10 @@ Les six premiers paramètres sont propres au MCGT, les deux derniers proviennent
 Pour les tirages MCGT, chaque paramètre suit une loi normale centrée sur sa valeur optimale :
 \[
   \begin{aligned}
-    \alpha_{1} &\sim \mathcal{N}\bigl(0.40,\;0.01^{2}\bigr),
+    \alpha_{1} &\sim \mathcal{N}\bigl(0.40,\;0.01^{2}\bigr),
     &\quad
     T_{c} &\sim \mathcal{N}\bigl(0.25\,\text{Gyr},\;0.01^{2}\bigr), \\
-    \Delta &\sim \mathcal{N}\bigl(3.00,\;0.50^{2}\bigr),
+    \Delta &\sim \mathcal{N}\bigl(3.00,\;0.50^{2}\bigr),
     &\quad
     T_{p} &\sim \mathcal{N}\bigl(0.05\,\text{Gyr},\;0.01^{2}\bigr), \\
     \Delta_{p} &\sim \mathcal{N}\bigl(0.10\,\text{Gyr},\;0.02^{2}\bigr),
@@ -46,16 +46,16 @@ On obtient ainsi \(N_{\rm MC}\) vecteurs complets \(\Theta^{(i)} \in \mathbb{R}^
 \subsection{10.5 – Formule de la covariance empirique}
 La matrice de covariance empirique \(C_{8\times8}\) de l’échantillon \(\{\Theta^{(i)}\}\) est définie par :
 \[
-  C_{jk}
-  \;=\;
+  C_{jk}
+  \;=\;
   \frac{1}{N_{\rm MC} - 1}
   \sum_{i=1}^{N_{\rm MC}}
     \Bigl(\Theta_{j}^{(i)} - \overline{\Theta}_{j}\Bigr)
     \Bigl(\Theta_{k}^{(i)} - \overline{\Theta}_{k}\Bigr),
   \quad
-  \overline{\Theta}_{j}
-  \;=\;
-  \frac{1}{N_{\rm MC}}
+  \overline{\Theta}_{j}
+  \;=\;
+  \frac{1}{N_{\rm MC}}
   \sum_{i=1}^{N_{\rm MC}} \Theta_{j}^{(i)}.
 \]
 Cette matrice symétrique 8 × 8 est sauvegardée au format binaire NumPy sous le nom \texttt{10\_global\_covariance\_real.npy}.
@@ -63,18 +63,18 @@ Cette matrice symétrique 8 × 8 est sauvegardée au format binaire NumPy sous l
 \subsection{10.6 – Figures à produire}

 \begin{itemize}
-  \item \textbf{Heatmap de la matrice de covariance}
-    \texttt{fig\_01\_covariance\_heatmap.png} :
-    représentation colorée de \(C_{8\times8}\) avec annotations numériques.
+  \item \textbf{Heatmap de la matrice de covariance}
+    \texttt{fig\_01\_covariance\_heatmap.png} :
+    représentation colorée de \(C_{8\times8}\) avec annotations numériques.
     Permet de visualiser corrélations et anticorrélations entre les huit paramètres.

-  \item \textbf{Densités KDE unidimensionnelles}
-    \texttt{fig\_02\_distributions\_unidimensionnelles.png} :
+  \item \textbf{Densités KDE unidimensionnelles}
+    \texttt{fig\_02\_distributions\_unidimensionnelles.png} :
     pour chaque composante \(\Theta_{j}\), tracer la densité de probabilité estimée (KDE) et indiquer l’écart‐type empirique \(\sqrt{C_{jj}}\).

-  \item \textbf{Ellipse de confiance \((\Omega_{m},\,H_{0})\)}
-    \texttt{fig\_03\_ellipse\_Omega\_m\_H0.png} :
-    ellipses 68 \% (Δχ² = 2,30) et 95 \% (Δχ² = 5,99) basées sur la sous‐matrice covariante extraite de \((\Omega_{m},H_{0})\).
+  \item \textbf{Ellipse de confiance \((\Omega_{m},\,H_{0})\)}
+    \texttt{fig\_03\_ellipse\_Omega\_m\_H0.png} :
+    ellipses 68 \% (Δχ² = 2,30) et 95 \% (Δχ² = 5,99) basées sur la sous‐matrice covariante extraite de \((\Omega_{m},H_{0})\).
     Le centre correspond à la moyenne Planck \(\bigl(\overline{\Omega}_{m},\,\overline{H}_{0}\bigr)\).

 \end{itemize}
@@ -82,9 +82,9 @@ Cette matrice symétrique 8 × 8 est sauvegardée au format binaire NumPy sous l
 \subsection{10.7 – Choix numériques et organisation}

 \begin{itemize}
-  \item \(\mathbf{N_{\rm MC} = 10^{5}}\) : compromis entre précision statistique et temps de calcul (quelques minutes sur CPU standard).
-  \item \(\mathbf{np.random.seed(0)}\) : garantit la même suite de tirages MCGT à chaque exécution.
-  \item Ordre des huit paramètres :
+  \item \(\mathbf{N_{\rm MC} = 10^{5}}\) : compromis entre précision statistique et temps de calcul (quelques minutes sur CPU standard).
+  \item \(\mathbf{np.random.seed(0)}\) : garantit la même suite de tirages MCGT à chaque exécution.
+  \item Ordre des huit paramètres :
     \[
       (\alpha_{1},\,T_{c},\,\Delta,\,T_{p},\,\Delta_{p},\,\beta,\,\Omega_{m},\,H_{0}),
     \]
@@ -96,4 +96,4 @@ Cette matrice symétrique 8 × 8 est sauvegardée au format binaire NumPy sous l
 *****Chapitre 10 (Monte-Carlo global 8D)
 Dans la discussion des contraintes combinées (section des résultats globaux), rappelez que sous MCGT, la distribution postérieure de H 0 se décale légèrement vers les valeurs locales grâce à δH0 ∼1%.****

-\noindent\emph{Fin du volet conceptuel du Chapitre 10. La partie opérationnelle détaillée commence ci-dessous.}
\ No newline at end of file
+\noindent\emph{Fin du volet conceptuel du Chapitre 10. La partie opérationnelle détaillée commence ci-dessous.}
diff --git a/10-monte-carlo-global-8d/10_monte_carlo_global_details.tex b/10-monte-carlo-global-8d/10_monte_carlo_global_details.tex
index 1eeafc3..1ed19c0 100755
--- a/10-monte-carlo-global-8d/10_monte_carlo_global_details.tex
+++ b/10-monte-carlo-global-8d/10_monte_carlo_global_details.tex
@@ -3,7 +3,7 @@
   \item Charger la chaîne MCMC Planck (\texttt{planck2018\_chain.csv}) contenant les colonnes \texttt{omegabh2}, \texttt{omegach2} et \texttt{H0}.
   \item Pour chaque ligne \(i\) :
     \[
-      h_{i} = \frac{H_{0}^{(i)}}{100},
+      h_{i} = \frac{H_{0}^{(i)}}{100},
       \quad
       \Omega_{m}^{(i)} = \frac{\omega_{b}h^{2} + \omega_{c}h^{2}}{h^{2}},
       \quad
@@ -14,8 +14,8 @@

 \subsection{10.9 – Génération des tirages MCGT}
 \begin{enumerate}
-  \item Fixer la graine aléatoire :
-    \verb|np.random.seed(0)|
+  \item Fixer la graine aléatoire :
+    \verb|np.random.seed(0)|

   \item Définir les distributions normales :
     \begin{align*}
@@ -39,18 +39,18 @@
   \item Initialiser un tableau \texttt{samples} de forme \((N_{\rm MC},\,8)\).
   \item Pour chaque \(i=0,\dots,N_{\rm MC}-1\) :
     \begin{itemize}
-      \item Sélectionner l’indice Planck \(j = i \bmod N_{\rm Planck}\).
-      \item Extraire \(\Omega_{m}^{(j)},\,H_{0}^{(j)}\) depuis le tableau Planck.
+      \item Sélectionner l’indice Planck \(j = i \bmod N_{\rm Planck}\).
+      \item Extraire \(\Omega_{m}^{(j)},\,H_{0}^{(j)}\) depuis le tableau Planck.
       \item Affecter :
         \[
-          \texttt{samples}[i,\,0:6] = \texttt{mcgt\_samples}[i,\,0:6],
+          \texttt{samples}[i,\,0:6] = \texttt{mcgt\_samples}[i,\,0:6],
           \quad
-          \texttt{samples}[i,\,6] = \Omega_{m}^{(j)},
+          \texttt{samples}[i,\,6] = \Omega_{m}^{(j)},
           \quad
           \texttt{samples}[i,\,7] = H_{0}^{(j)}.
         \]
     \end{itemize}
-  \item Au terme de la boucle, \texttt{samples[i]} contient
+  \item Au terme de la boucle, \texttt{samples[i]} contient
     \(\bigl[\alpha_{1}^{(i)},\,T_{c}^{(i)},\,\Delta^{(i)},\,T_{p}^{(i)},\,\Delta_{p}^{(i)},\,\beta^{(i)},\,\Omega_{m}^{(j)},\,H_{0}^{(j)}\bigr]\).
 \end{enumerate}

@@ -60,10 +60,10 @@
     \[
       C_{8\times8} = \texttt{np.cov(samples, rowvar=False)}.
     \]
-  \item Vérifier la symétrie :
-    \texttt{assert np.allclose(cov8, cov8.T)}
+  \item Vérifier la symétrie :
+    \texttt{assert np.allclose(cov8, cov8.T)}
   \item Vérifier que toutes les variances diagonales sont strictement positives :
-    \texttt{assert np.all(np.diag(cov8) > 0)}
+    \texttt{assert np.all(np.diag(cov8) > 0)}
   \item Sauvegarder au format binaire NumPy :
     \[
       \texttt{np.save('10\_global\_covariance\_real.npy',\,C_{8\times8})}.
@@ -174,14 +174,14 @@ plt.close()

 \subsection{10.15 – Dépendances et installation}
 \begin{itemize}
-  \item Python 3.8 ou supérieur
-  \item Packages :
+  \item Python 3.8 ou supérieur
+  \item Packages :
     \begin{itemize}
-      \item \texttt{numpy} (manipulation de tableaux, calcul de covariance)
-      \item \texttt{scipy} (estimation KDE)
-      \item \texttt{matplotlib} (tracés)
+      \item \texttt{numpy} (manipulation de tableaux, calcul de covariance)
+      \item \texttt{scipy} (estimation KDE)
+      \item \texttt{matplotlib} (tracés)
     \end{itemize}
-  \item Installation recommandée :
+  \item Installation recommandée :
     \begin{verbatim}
     pip install numpy scipy matplotlib
     \end{verbatim}
@@ -189,16 +189,16 @@ plt.close()

 \subsection{10.16 – Validations post‐calcul}
 \begin{itemize}
-  \item Vérifier la symétrie de la matrice :
-    \verb|assert np.allclose(cov8, cov8.T)|
-  \item Vérifier la positivité des variances :
-    \verb|assert np.all(np.diag(cov8) > 0)|
-  \item Contrôler que les écarts‐types reportés dans les KDE correspondent
-        à \(\sqrt{C_{jj}}\) pour chaque \(j\).
-  \item S’assurer que l’ellipse \((\Omega_{m},H_{0})\) est centrée sur
-        \(\bigl(\overline{\Omega}_{m},\,\overline{H}_{0}\bigr)\)
+  \item Vérifier la symétrie de la matrice :
+    \verb|assert np.allclose(cov8, cov8.T)|
+  \item Vérifier la positivité des variances :
+    \verb|assert np.all(np.diag(cov8) > 0)|
+  \item Contrôler que les écarts‐types reportés dans les KDE correspondent
+        à \(\sqrt{C_{jj}}\) pour chaque \(j\).
+  \item S’assurer que l’ellipse \((\Omega_{m},H_{0})\) est centrée sur
+        \(\bigl(\overline{\Omega}_{m},\,\overline{H}_{0}\bigr)\)
         et reflète la corrélation de \(\text{sub_cov}\).
 \end{itemize}

 \bigskip
-\noindent\emph{Fin de la partie détaillée, Chapitre 10.}
\ No newline at end of file
+\noindent\emph{Fin de la partie détaillée, Chapitre 10.}
diff --git a/10-monte-carlo-global-8d/CHAPTER10_GUIDE.txt b/10-monte-carlo-global-8d/CHAPTER10_GUIDE.txt
index e7375cf..de9fc54 100755
--- a/10-monte-carlo-global-8d/CHAPTER10_GUIDE.txt
+++ b/10-monte-carlo-global-8d/CHAPTER10_GUIDE.txt
@@ -469,4 +469,3 @@ pdflatex -interaction=nonstopmode 10_monte_carlo_global_details.tex
 --------------------------------------------------------------------
 FIN – CHAPITRE 10
 --------------------------------------------------------------------
-
diff --git a/README-REPRO.md b/README-REPRO.md
index 7d82426..9c9b773 100755
--- a/README-REPRO.md
+++ b/README-REPRO.md
@@ -60,38 +60,38 @@ Ce document explique comment (re)générer les données, figures et diagnostics

 6.1 Chapitre 09 — Phase d’ondes gravitationnelles
 Étape 0 — (si besoin) extraire/régénérer la référence
-python zz-scripts/chapter09/extract\_phenom\_phase.py
+python zz-scripts/chapter09/extract\_phenom\_phase.py
 --out zz-data/chapter09/09\_phases\_imrphenom.csv
 Étape 1 — Générer prétraitement + résidus
-python zz-scripts/chapter09/generate\_data\_chapter09.py
---ref zz-data/chapter09/09\_phases\_imrphenom.csv
---out-prepoly zz-data/chapter09/09\_phases\_mcgt\_prepoly.csv
---out-diff    zz-data/chapter09/09\_phase\_diff.csv
+python zz-scripts/chapter09/generate\_data\_chapter09.py
+--ref zz-data/chapter09/09\_phases\_imrphenom.csv
+--out-prepoly zz-data/chapter09/09\_phases\_mcgt\_prepoly.csv
+--out-diff    zz-data/chapter09/09\_phase\_diff.csv
 --log-level INFO
 Étape 2 — Optimiser (base, degré) + rebranch k, écrire la série finale
-python zz-scripts/chapter09/opt\_poly\_rebranch.py
---csv zz-data/chapter09/09\_phases\_mcgt\_prepoly.csv
---meta zz-data/chapter09/09\_metrics\_phase.json
---fit-window 30 250 --metrics-window 20 300
---degrees 3 4 5 --bases log10 hz --k-range -10 10
---out-csv  zz-data/chapter09/09\_phases\_mcgt.csv
---out-best zz-data/chapter09/09\_best\_params.json
+python zz-scripts/chapter09/opt\_poly\_rebranch.py
+--csv zz-data/chapter09/09\_phases\_mcgt\_prepoly.csv
+--meta zz-data/chapter09/09\_metrics\_phase.json
+--fit-window 30 250 --metrics-window 20 300
+--degrees 3 4 5 --bases log10 hz --k-range -10 10
+--out-csv  zz-data/chapter09/09\_phases\_mcgt.csv
+--out-best zz-data/chapter09/09\_best\_params.json
 --backup --log-level INFO
 Étape 3 — Figures
-python zz-scripts/chapter09/plot\_fig01\_phase\_overlay.py
---csv  zz-data/chapter09/09\_phases\_mcgt.csv
---meta zz-data/chapter09/09\_metrics\_phase.json
---out  zz-figures/chapter09/fig\_01\_phase\_overlay.png
+python zz-scripts/chapter09/plot\_fig01\_phase\_overlay.py
+--csv  zz-data/chapter09/09\_phases\_mcgt.csv
+--meta zz-data/chapter09/09\_metrics\_phase.json
+--out  zz-figures/chapter09/fig\_01\_phase\_overlay.png
 --shade 20 300 --show-residual --dpi 300
-python zz-scripts/chapter09/plot\_fig02\_residual\_phase.py
---csv  zz-data/chapter09/09\_phases\_mcgt.csv
---meta zz-data/chapter09/09\_metrics\_phase.json
---out  zz-figures/chapter09/fig\_02\_residual\_phase.png
+python zz-scripts/chapter09/plot\_fig02\_residual\_phase.py
+--csv  zz-data/chapter09/09\_phases\_mcgt.csv
+--meta zz-data/chapter09/09\_metrics\_phase.json
+--out  zz-figures/chapter09/fig\_02\_residual\_phase.png
 --bands 20 300 300 1000 1000 2000 --dpi 300
-python zz-scripts/chapter09/plot\_fig03\_hist\_absdphi\_20\_300.py
---csv  zz-data/chapter09/09\_phases\_mcgt.csv
---meta zz-data/chapter09/09\_metrics\_phase.json
---out  zz-figures/chapter09/fig\_03\_hist\_absdphi\_20\_300.png
+python zz-scripts/chapter09/plot\_fig03\_hist\_absdphi\_20\_300.py
+--csv  zz-data/chapter09/09\_phases\_mcgt.csv
+--meta zz-data/chapter09/09\_metrics\_phase.json
+--out  zz-figures/chapter09/fig\_03\_hist\_absdphi\_20\_300.png
 --mode principal --bins 50 --window 20 300 --xscale log --dpi 300
 Étape 4 — Jalons (catalogue GWTC-3 confident)

@@ -99,50 +99,50 @@ Préparez « zz-data/chapter09/09\_comparison\_milestones.csv »

 puis flaguez selon sigma/classe :

-python zz-scripts/chapter09/flag\_jalons.py
---csv  zz-data/chapter09/09\_comparison\_milestones.csv
---meta zz-data/chapter09/09\_comparison\_milestones.meta.json
---out-csv zz-data/chapter09/09\_comparison\_milestones.flagged.csv
+python zz-scripts/chapter09/flag\_jalons.py
+--csv  zz-data/chapter09/09\_comparison\_milestones.csv
+--meta zz-data/chapter09/09\_comparison\_milestones.meta.json
+--out-csv zz-data/chapter09/09\_comparison\_milestones.flagged.csv
 --write-meta

 Figures jalons :

-python zz-scripts/chapter09/plot\_fig04\_absdphi\_milestones\_vs\_f.py
---diff   zz-data/chapter09/09\_phase\_diff.csv
---csv    zz-data/chapter09/09\_phases\_mcgt.csv
---meta   zz-data/chapter09/09\_metrics\_phase.json
---jalons zz-data/chapter09/09\_comparison\_milestones.csv
---out    zz-figures/chapter09/fig\_04\_absdphi\_milestones\_vs\_f.png
+python zz-scripts/chapter09/plot\_fig04\_absdphi\_milestones\_vs\_f.py
+--diff   zz-data/chapter09/09\_phase\_diff.csv
+--csv    zz-data/chapter09/09\_phases\_mcgt.csv
+--meta   zz-data/chapter09/09\_metrics\_phase.json
+--jalons zz-data/chapter09/09\_comparison\_milestones.csv
+--out    zz-figures/chapter09/fig\_04\_absdphi\_milestones\_vs\_f.png
 --window 20 300 --with\_errorbar --dpi 300
-python zz-scripts/chapter09/plot\_fig05\_scatter\_phi\_at\_fpeak.py
---jalons zz-data/chapter09/09\_comparison\_milestones.csv
+python zz-scripts/chapter09/plot\_fig05\_scatter\_phi\_at\_fpeak.py
+--jalons zz-data/chapter09/09\_comparison\_milestones.csv
 --out    zz-figures/chapter09/fig\_05\_scatter\_phi\_at\_fpeak.png

 6.2 Chapitre 10 — Monte Carlo global 8D
 Étape 1 — Config (paramètres/prior/nuisances)
 cat zz-data/chapter10/10\_mc\_config.json   # vérifiez les bornes, seed, n
 Étape 2 — Génération/évaluation principale
-python zz-scripts/chapter10/generate\_data\_chapter10.py
---config zz-data/chapter10/10\_mc\_config.json
---out-results zz-data/chapter10/10\_mc\_results.csv
---out-results-circ zz-data/chapter10/10\_mc\_results.circ.csv
---out-samples zz-data/chapter10/10\_mc\_samples.csv
+python zz-scripts/chapter10/generate\_data\_chapter10.py
+--config zz-data/chapter10/10\_mc\_config.json
+--out-results zz-data/chapter10/10\_mc\_results.csv
+--out-results-circ zz-data/chapter10/10\_mc\_results.circ.csv
+--out-samples zz-data/chapter10/10\_mc\_samples.csv
 --log-level INFO
 Étape 3 — Diagnostics complémentaires

 Ajout de φ(f\_peak) et QA circulaire

-python zz-scripts/chapter10/add\_phi\_at\_fpeak.py
---results zz-data/chapter10/10\_mc\_results.circ.csv
+python zz-scripts/chapter10/add\_phi\_at\_fpeak.py
+--results zz-data/chapter10/10\_mc\_results.circ.csv
 --out     zz-data/chapter10/10\_mc\_results.circ.with\_fpeak.csv
-python zz-scripts/chapter10/inspect\_topk\_residuals.py
---results zz-data/chapter10/10\_mc\_results.csv
---jalons  zz-data/chapter10/10\_mc\_milestones\_eval.csv
+python zz-scripts/chapter10/inspect\_topk\_residuals.py
+--results zz-data/chapter10/10\_mc\_results.csv
+--jalons  zz-data/chapter10/10\_mc\_milestones\_eval.csv
 --out-dir zz-data/chapter10/topk\_residuals
-python zz-scripts/chapter10/bootstrap\_topk\_p95.py
---results zz-data/chapter10/10\_mc\_results.csv
---topk-json zz-data/chapter10/10\_mc\_best.json
---out-json  zz-data/chapter10/10\_mc\_best\_bootstrap.json
+python zz-scripts/chapter10/bootstrap\_topk\_p95.py
+--results zz-data/chapter10/10\_mc\_results.csv
+--topk-json zz-data/chapter10/10\_mc\_best.json
+--out-json  zz-data/chapter10/10\_mc\_best\_bootstrap.json
 --B 1000 --seed 12345
 Étape 4 — Figures de synthèse
 python zz-scripts/chapter10/plot\_fig01\_iso\_p95\_maps.py        --out zz-figures/chapter10/fig\_01\_iso\_p95\_maps.png
@@ -170,16 +170,16 @@ python zz-scripts/chapter10/plot\_fig07\_synthesis.py            --out zz-figure
    make jsoncheck-strict
 8. MANIFESTES \& PUBLICATION
    Vérifier et rapporter l’inventaire
-   python zz-manifests/diag\_consistency.py
-   zz-manifests/manifest\_publication.json
+   python zz-manifests/diag\_consistency.py
+   zz-manifests/manifest\_publication.json
    --report md > zz-manifests/manifest\_report.md
    Corriger/compléter (optionnel, prudence)
-   python zz-manifests/diag\_consistency.py
-   zz-manifests/manifest\_master.json
+   python zz-manifests/diag\_consistency.py
+   zz-manifests/manifest\_master.json
    --fix --normalize-paths --strip-internal --sha256-out
    Préparer une archive livrable (exemple)
-   tar czf MCGT\_artifacts\_$(date +%Y%m%d).tar.gz
-   zz-data/ zz-figures/ zz-manifests/ zz-schemas/
+   tar czf MCGT\_artifacts\_$(date +%Y%m%d).tar.gz
+   zz-data/ zz-figures/ zz-manifests/ zz-schemas/
    convention.md README-REPRO.md RUNBOOK.md
 9. ARBORESCENCE MINIMALE ATTENDUE
    MCGT/
diff --git a/RUNBOOK.md b/RUNBOOK.md
index ae3e9ef..3da61ea 100755
--- a/RUNBOOK.md
+++ b/RUNBOOK.md
@@ -269,7 +269,7 @@ MCGT/
 │  ├─ validate_json.py
 │  └─ validate_csv_table.py
 ├─ zz-data/
-│  ├─ chapter01/ ...
+│  ├─ chapter01/ ...
 │  ├─ chapter09/
 │  │  ├─ 09_phases_imrphenom.csv
 │  │  ├─ 09_phases_imrphenom.meta.json
@@ -312,4 +312,3 @@ python zz-manifests/diag_consistency.py zz-manifests/manifest_master.json --repo
 * Fichiers manquants parce que `chapter10` vs `chapitre10` → rester cohérent (anglais)
 * `p95_*` mélangé (linéaire/circulaire) → vérifier les suffixes (`_circ`, `_recalc`) et l’unité (radian)
 * `*.meta.json` incomplets (pas de `generated_at`/`git_hash`) → compléter avant remise
-
diff --git a/arborescence.txt b/arborescence.txt
index 32dbbc8..5692f8e 100755
--- a/arborescence.txt
+++ b/arborescence.txt
@@ -10,67 +10,67 @@
 |   requirements.txt
 |   RUNBOOK.md
 |   setup.py
-|
+|
 +---01-introduction-applications
 |       01_applications_calibration_conceptuel.tex
 |       01_introduction_conceptuel.tex
 |       CHAPTER01_GUIDE.txt
-|
+|
 +---02-validation-chronologique
 |       02_validation_chronologique_conceptuel.tex
 |       02_validation_chronologique_details.tex
 |       CHAPTER02_GUIDE.txt
-|
+|
 +---03-stabilite-fR
 |       03_stabilite_fR_conceptuel.tex
 |       03_stabilite_fR_details.tex
 |       CHAPTER03_GUIDE.txt
-|
+|
 +---04-invariants-adimensionnels
 |       04_invariants_adimensionnels_conceptuel.tex
 |       04_invariants_adimensionnels_details.tex
 |       CHAPTER04_GUIDE.txt
-|
+|
 +---05-nucleosynthese-primordiale
 |       05_nucleosynthese_primordiale_conceptuel.tex
 |       05_nucleosynthese_primordiale_details.tex
 |       CHAPTER05_GUIDE.txt
-|
+|
 +---06-rayonnement-cmb
 |       06_cmb_conceptuel.tex
 |       06_cmb_details.tex
 |       CHAPTER06_GUIDE.txt
-|
+|
 +---07-perturbations-scalaires
 |       07_perturbations_scalaires_conceptuel.tex
 |       07_perturbations_scalaires_details.tex
 |       CHAPTER07_GUIDE.txt
-|
+|
 +---08-couplage-sombre
 |       08_couplage_sombre_conceptuel.tex
 |       08_couplage_sombre_details.tex
 |       CHAPTER08_GUIDE.txt
-|
+|
 +---09-phase-ondes-gravitationnelles
 |       09_phase_ondes_grav_conceptuel.tex
 |       09_phase_ondes_grav_details.tex
 |       CHAPTER09_GUIDE.txt
-|
+|
 +---10-monte-carlo-global-8d
 |       10_monte_carlo_global_conceptuel.tex
 |       10_monte_carlo_global_details.tex
 |       CHAPTER10_GUIDE.txt
-|
+|
 +---mcgt
 |   |   CHANGELOG.md
 |   |   phase.py
 |   |   pyproject.toml
 |   |   scalar_perturbations.py
 |   |   __init__.py
-|   |
+|   |
 |   \---backends
 |           ref_phase.py
-|
+|
 +---zz-checklists
 |       CHAPTER01_CHECKLIST.txt
 |       CHAPTER02_CHECKLIST.txt
@@ -82,7 +82,7 @@
 |       CHAPTER08_CHECKLIST.txt
 |       CHAPTER09_CHECKLIST.txt
 |       CHAPTER10_CHECKLIST.txt
-|
+|
 +---zz-configuration
 |       camb_exact_plateau.ini
 |       GWTC-3-confident-events.json
@@ -93,7 +93,7 @@
 |       pdot_plateau_vs_z.dat
 |       README.md
 |       scalar_perturbations.ini
-|
+|
 +---zz-data
 |   +---chapter01
 |   |       01_dimensionless_invariants.csv
@@ -106,7 +106,7 @@
 |   |       01_P_vs_T.dat
 |   |       01_relative_error_timeline.csv
 |   |       01_timeline_milestones.csv
-|   |
+|   |
 |   +---chapter02
 |   |       02_As_ns_vs_alpha.csv
 |   |       02_FG_series.csv
@@ -118,7 +118,7 @@
 |   |       02_P_vs_T_grid_data.dat
 |   |       02_relative_error_timeline.csv
 |   |       02_timeline_milestones.csv
-|   |
+|   |
 |   +---chapter03
 |   |       03_fR_stability_boundary.csv
 |   |       03_fR_stability_data.csv
@@ -127,11 +127,11 @@
 |   |       03_ricci_fR_milestones.csv
 |   |       03_ricci_fR_vs_T.csv
 |   |       03_ricci_fR_vs_z.csv
-|   |
+|   |
 |   +---chapter04
 |   |       04_dimensionless_invariants.csv
 |   |       04_P_vs_T.dat
-|   |
+|   |
 |   +---chapter05
 |   |       05_bbn_data.csv
 |   |       05_bbn_grid.csv
@@ -140,7 +140,7 @@
 |   |       05_bbn_params.json
 |   |       05_chi2_bbn_vs_T.csv
 |   |       05_dchi2_vs_T.csv
-|   |
+|   |
 |   +---chapter06
 |   |       01_P_vs_T.dat
 |   |       06_alpha_evolution.csv
@@ -156,7 +156,7 @@
 |   |       06_delta_Tm_scan.csv
 |   |       06_hubble_mcgt.dat
 |   |       06_params_cmb.json
-|   |
+|   |
 |   +---chapter07
 |   |       07_cs2_matrix.csv
 |   |       07_dcs2_vs_k.csv
@@ -170,7 +170,7 @@
 |   |       07_phase_run.csv
 |   |       07_scalar_invariants.csv
 |   |       07_scalar_perturbations_results.csv
-|   |
+|   |
 |   +---chapter08
 |   |       08_bao_data.csv
 |   |       08_chi2_derivative.csv
@@ -183,7 +183,7 @@
 |   |       08_mu_theory_q0star.csv
 |   |       08_mu_theory_z.csv
 |   |       08_pantheon_data.csv
-|   |
+|   |
 |   +---chapter09
 |   |       09_best_params.json
 |   |       09_comparison_milestones.csv
@@ -196,7 +196,7 @@
 |   |       09_phases_mcgt_prepoly.csv
 |   |       09_phase_diff.csv
 |   |       gwtc3_confident_parameters.json
-|   |
+|   |
 |   \---chapter10
 |           10_mc_best.json
 |           10_mc_best_bootstrap.json
@@ -208,7 +208,7 @@
 |           10_mc_results.circ.with_fpeak.csv
 |           10_mc_results.csv
 |           10_mc_samples.csv
-|
+|
 +---zz-figures
 |   +---chapter01
 |   |       fig_01_early_plateau.png
@@ -217,7 +217,7 @@
 |   |       fig_04_P_vs_T_evolution.png
 |   |       fig_05_I1_vs_T.png
 |   |       fig_06_P_derivative_comparison.png
-|   |
+|   |
 |   +---chapter02
 |   |       fig_00_spectrum.png
 |   |       fig_01_P_vs_T_evolution.png
@@ -226,7 +226,7 @@
 |   |       fig_04_pipeline_diagram.png
 |   |       fig_05_FG_series.png
 |   |       fig_06_fit_alpha.png
-|   |
+|   |
 |   +---chapter03
 |   |       fig_01_fR_stability_domain.png
 |   |       fig_02_fR_fRR_vs_R.png
@@ -236,26 +236,26 @@
 |   |       fig_06_grid_quality.png
 |   |       fig_07_ricci_fR_vs_z.png
 |   |       fig_08_ricci_fR_vs_T.png
-|   |
+|   |
 |   +---chapter04
 |   |       fig_01_invariants_schematic.png
 |   |       fig_02_invariants_histogram.png
 |   |       fig_03_invariants_vs_T.png
 |   |       fig_04_relative_deviations.png
-|   |
+|   |
 |   +---chapter05
 |   |       fig_01_bbn_reaction_network.png
 |   |       fig_02_dh_model_vs_obs.png
 |   |       fig_03_yp_model_vs_obs.png
 |   |       fig_04_chi2_vs_T.png
-|   |
+|   |
 |   +---chapter06
 |   |       fig_01_cmb_dataflow_diagram.png
 |   |       fig_02_cls_lcdm_vs_mcgt.png
 |   |       fig_03_delta_cls_relative.png
 |   |       fig_04_delta_rs_vs_params.png
 |   |       fig_05_delta_chi2_heatmap.png
-|   |
+|   |
 |   +---chapter07
 |   |       fig_00_loglog_sampling_test.png
 |   |       fig_01_cs2_heatmap_k_a.png
@@ -265,7 +265,7 @@
 |   |       fig_05_ddelta_phi_dk_vs_k.png
 |   |       fig_06_comparison.png
 |   |       fig_07_invariant_I2.png
-|   |
+|   |
 |   +---chapter08
 |   |       fig_01_chi2_total_vs_q0.png
 |   |       fig_02_dv_vs_z.png
@@ -274,7 +274,7 @@
 |   |       fig_05_residuals.png
 |   |       fig_06_pulls.png
 |   |       fig_07_chi2_profile.png
-|   |
+|   |
 |   +---chapter09
 |   |   |   fig_01_phase_overlay.png
 |   |   |   fig_02_residual_phase.png
@@ -282,7 +282,7 @@
 |   |   |   fig_04_absdphi_milestones_vs_f.png
 |   |   |   fig_05_scatter_phi_at_fpeak.png
 |   |   |   p95_check_control.png
-|   |   |
+|   |   |
 |   |   \---p95_methods
 |   |           fig03_raw_bins30.png
 |   |           fig03_raw_bins50.png
@@ -293,7 +293,7 @@
 |   |           fig03_unwrap_bins30.png
 |   |           fig03_unwrap_bins50.png
 |   |           fig03_unwrap_bins80.png
-|   |
+|   |
 |   \---chapter10
 |           fig_01_iso_p95_maps.png
 |           fig_02_scatter_phi_at_fpeak.png
@@ -303,7 +303,7 @@
 |           fig_05_hist_cdf_metrics.png
 |           fig_06_heatmap_absdp95_m1m2.png
 |           fig_07_summary_comparison.png
-|
+|
 +---zz-manifests
 |   |   add_to_manifest.py
 |   |   diag_consistency.py
@@ -314,7 +314,7 @@
 |   |   meta_template.json
 |   |   migration_map.json
 |   |   README_manifest.md
-|   |
+|   |
 |   +---chapters
 |   |       chapter_manifest_01.json
 |   |       chapter_manifest_02.json
@@ -326,7 +326,7 @@
 |   |       chapter_manifest_08.json
 |   |       chapter_manifest_09.json
 |   |       chapter_manifest_10.json
-|   |
+|   |
 |   \---reports
 +---zz-schemas
 |       02_optimal_parameters.schema.json
@@ -352,7 +352,7 @@
 |       validate_csv_table.py
 |       validate_json.py
 |       validation_globals.json
-|
+|
 +---zz-scripts
 |   +---chapter01
 |   |       generate_data_chapter01.py
@@ -363,7 +363,7 @@
 |   |       plot_fig05_I1_vs_T.py
 |   |       plot_fig06_P_derivative_comparison.py
 |   |       requirements.txt
-|   |
+|   |
 |   +---chapter02
 |   |       extract_sympy_FG.ipynb
 |   |       generate_data_chapter02.py
@@ -376,7 +376,7 @@
 |   |       plot_fig06_alpha_fit.py
 |   |       primordial_spectrum.py
 |   |       requirements.txt
-|   |
+|   |
 |   +---chapter03
 |   |   |   generate_data_chapter03.py
 |   |   |   plot_fig01_fR_stability_domain.py
@@ -388,11 +388,11 @@
 |   |   |   plot_fig07_ricci_fR_vs_z.py
 |   |   |   plot_fig08_ricci_fR_vs_T.py
 |   |   |   requirements.txt
-|   |   |
+|   |   |
 |   |   \---utils
 |   |           03_ricci_fR_milestones_enhanced.csv
 |   |           convert_milestones.py
-|   |
+|   |
 |   +---chapter04
 |   |       generate_data_chapter04.py
 |   |       plot_fig01_invariants_schematic.py
@@ -400,7 +400,7 @@
 |   |       plot_fig03_invariants_vs_T.py
 |   |       plot_fig04_relative_deviations.py
 |   |       requirements.txt
-|   |
+|   |
 |   +---chapter05
 |   |       generate_data_chapter05.py
 |   |       plot_fig01_bbn_reaction_network.py
@@ -408,7 +408,7 @@
 |   |       plot_fig03_yp_model_vs_obs.py
 |   |       plot_fig04_chi2_vs_T.py
 |   |       requirements.txt
-|   |
+|   |
 |   +---chapter06
 |   |       generate_data_chapter06.py
 |   |       generate_pdot_plateau_vs_z.py
@@ -419,7 +419,7 @@
 |   |       plot_fig05_delta_chi2_heatmap.py
 |   |       requirements.txt
 |   |       run_camb_chapter06.bat
-|   |
+|   |
 |   +---chapter07
 |   |   |   generate_data_chapter07.py
 |   |   |   launch_scalar_perturbations_solver.py
@@ -432,14 +432,14 @@
 |   |   |   plot_fig06_comparison.py
 |   |   |   plot_fig07_invariant_I2.py
 |   |   |   requirements.txt
-|   |   |
+|   |   |
 |   |   +---tests
 |   |   |       test_chapter07.py
-|   |   |
+|   |   |
 |   |   \---utils
 |   |           test_kgrid.py
 |   |           toy_model.py
-|   |
+|   |
 |   +---chapter08
 |   |   |   generate_coupling_milestones.py
 |   |   |   generate_data_chapter08.py
@@ -451,7 +451,7 @@
 |   |   |   plot_fig06_normalized_residuals_distribution.py
 |   |   |   plot_fig07_chi2_profile.py
 |   |   |   requirements.txt
-|   |   |
+|   |   |
 |   |   \---utils
 |   |           cosmo.py
 |   |           coupling_example_model.py
@@ -459,7 +459,7 @@
 |   |           extract_pantheon_plus_data.py
 |   |           generate_coupling_milestones.py
 |   |           verify_z_grid.py
-|   |
+|   |
 |   +---chapter09
 |   |       apply_poly_unwrap_rebranch.py
 |   |       check_p95_methods.py
@@ -475,7 +475,7 @@
 |   |       plot_fig04_absdphi_milestones_vs_f.py
 |   |       plot_fig05_scatter_phi_at_fpeak.py
 |   |       requirements.txt
-|   |
+|   |
 |   +---chapter10
 |   |       add_phi_at_fpeak.py
 |   |       bootstrap_topk_p95.py
@@ -497,18 +497,18 @@
 |   |       regen_fig05_using_circp95.py
 |   |       requirements.txt
 |   |       update_manifest_with_hashes.py
-|   |
+|   |
 |   \---manifest_tools
 |           populate_manifest.py
 |           verify_manifest.py
-|
+|
 +---zz-tests
 |       pytest.ini
 |       test_manifest.py
 |       test_schemas.py
-|
+|
 \---zz-workflows
         ci.yml
         README.md
         release.yml
-
+
diff --git a/conventions.md b/conventions.md
index 8761e65..aa5573d 100755
--- a/conventions.md
+++ b/conventions.md
@@ -1,6 +1,6 @@
 # CONVENTION MCGT

-Version : 1.1
+Version : 1.1
 Portée : ce document définit les **conventions de données, de métadonnées, d’unités, de nommage et de validation** applicables à l’ensemble des 10 chapitres MCGT (01–10). Il accompagne `zz-configuration/mcgt-global-config.ini` et les schémas dans `zz-schemas/`.

 ---
@@ -337,4 +337,3 @@ $$
 * Validation JSON seule : `make validate-json`
 * Validation CSV (tables) : `make validate-csv`
 * Rapport manifest : `make manifests-md`
-
diff --git a/mcgt/CHANGELOG.md b/mcgt/CHANGELOG.md
index 198cbb1..6268a23 100755
--- a/mcgt/CHANGELOG.md
+++ b/mcgt/CHANGELOG.md
@@ -33,6 +33,3 @@ Toutes les modifications notables de ce projet seront documentées ici.
 \### Corrigé

 \- Petites incohérences de normalisation de classes (`primaire`/`ordre2`) désormais cartographiées via le fichier de migration.
-
-
-
diff --git a/mcgt/__init__.py b/mcgt/__init__.py
index be9f66d..7373297 100755
--- a/mcgt/__init__.py
+++ b/mcgt/__init__.py
@@ -11,6 +11,7 @@ Ce fichier expose :

 But : rester non-intrusif (pas d'I/O obligatoire à l'import).
 """
+
 from __future__ import annotations

 __all__ = [
@@ -27,6 +28,7 @@ __version__ = "0.1.1"  # bump mineur après refactor des noms de fichiers

 # --- logging minimal ---
 import logging
+
 logger = logging.getLogger("mcgt")
 if not logger.handlers:
     handler = logging.NullHandler()
@@ -108,6 +110,7 @@ def list_backends(package: str = "mcgt.backends") -> List[str]:
         return []
     return [name for _, name, _ in pkgutil.iter_modules(mod.__path__)]

+
 # --- lazy imports pour commodité ---
 def _lazy_import(name: str):
     try:
diff --git a/mcgt/backends/ref_phase.py b/mcgt/backends/ref_phase.py
index db2809a..39a2ea7 100755
--- a/mcgt/backends/ref_phase.py
+++ b/mcgt/backends/ref_phase.py
@@ -24,12 +24,14 @@ _have_lal = False
 _have_filelock = False
 try:
     from filelock import FileLock
+
     _have_filelock = True
 except Exception:
     FileLock = None

 try:
     from pycbc.waveform import get_fd_waveform
+
     _have_pyc = True
 except Exception:
     _have_pyc = False
@@ -37,6 +39,7 @@ except Exception:
 try:
     import lal
     import lalsimulation as lalsim
+
     _have_lal = True
 except Exception:
     _have_lal = False
@@ -45,7 +48,9 @@ except Exception:
 logger = logging.getLogger("mcgt.ref_phase")
 if not logger.handlers:
     handler = logging.StreamHandler()
-    handler.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s"))
+    handler.setFormatter(
+        logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s")
+    )
     logger.addHandler(handler)
 logger.setLevel(logging.INFO)

@@ -151,7 +156,9 @@ def _acquire_lock(path: str, timeout: float = LOCK_TIMEOUT):
                     return self
                 except FileExistsError:
                     if (time.time() - t0) > self.timeout:
-                        raise TimeoutError(f"Timeout acquiring simple lock: {self.lockdir}")
+                        raise TimeoutError(
+                            f"Timeout acquiring simple lock: {self.lockdir}"
+                        )
                     time.sleep(0.1)

         def __exit__(self, exc_type, exc, tb):
@@ -164,7 +171,9 @@ def _acquire_lock(path: str, timeout: float = LOCK_TIMEOUT):


 # ------------------------- Backends de calcul ------------------------- #
-def _phi_ref_via_pyc(f_Hz: np.ndarray, m1: float, m2: float, approximant: str = "IMRPhenomD") -> np.ndarray:
+def _phi_ref_via_pyc(
+    f_Hz: np.ndarray, m1: float, m2: float, approximant: str = "IMRPhenomD"
+) -> np.ndarray:
     """Calcul via PyCBC/get_fd_waveform. Retourne la phase (radians) sur f_Hz."""
     if not _have_pyc:
         raise RuntimeError("PyCBC indisponible")
@@ -202,7 +211,9 @@ def _phi_ref_via_pyc(f_Hz: np.ndarray, m1: float, m2: float, approximant: str =
         try:
             f_src = np.asarray(hp.sample_frequencies(), dtype=np.float64)
         except Exception:
-            raise RuntimeError("REF_COMPUTE_FAIL: impossible d'extraire sample_frequencies de PyCBC waveform")
+            raise RuntimeError(
+                "REF_COMPUTE_FAIL: impossible d'extraire sample_frequencies de PyCBC waveform"
+            )

     data = np.asarray(hp.data, dtype=np.complex128)
     phase_src = np.unwrap(np.angle(data))
@@ -210,7 +221,9 @@ def _phi_ref_via_pyc(f_Hz: np.ndarray, m1: float, m2: float, approximant: str =
     return phi_on_grid


-def _phi_ref_via_lalsim(f_Hz: np.ndarray, m1: float, m2: float, approximant: str = "IMRPhenomD") -> np.ndarray:
+def _phi_ref_via_lalsim(
+    f_Hz: np.ndarray, m1: float, m2: float, approximant: str = "IMRPhenomD"
+) -> np.ndarray:
     """Calcul via LALSimulation (API indicative ; peut nécessiter adaptation selon version)."""
     if not _have_lal:
         raise RuntimeError("LALSuite/lalsimulation indisponible")
@@ -227,11 +240,18 @@ def _phi_ref_via_lalsim(f_Hz: np.ndarray, m1: float, m2: float, approximant: str

         # NB : Interface indicative — peut varier suivant la version.
         hp_fd = lalsim.SimInspiralChooseFDWaveform(
-            m1_si, m2_si,
-            0.0, 0.0, 0.0,   # spins
-            0.0, 1.0, 1.0,   # orientation/distance placeholders
-            f_Hz[0], f_Hz[-1],
-            0.0, approx_enum
+            m1_si,
+            m2_si,
+            0.0,
+            0.0,
+            0.0,  # spins
+            0.0,
+            1.0,
+            1.0,  # orientation/distance placeholders
+            f_Hz[0],
+            f_Hz[-1],
+            0.0,
+            approx_enum,
         )
         # Extraction (indicative) — adapter si nécessaire
         f_src = np.arange(len(hp_fd.data), dtype=float)
@@ -310,7 +330,9 @@ def compute_phi_ref(
                         _memcache.popitem(last=False)
                     return phi
         except Exception as e:
-            logger.warning("Cache disque illisible (%s), on recalcule : %s", cache_path, e)
+            logger.warning(
+                "Cache disque illisible (%s), on recalcule : %s", cache_path, e
+            )

     # 3) calcul (avec verrou pour éviter courses multiples)
     with _acquire_lock(cache_path):
@@ -346,13 +368,21 @@ def compute_phi_ref(

         if phi_on_grid is None:
             if not (_have_pyc or _have_lal):
-                raise RuntimeError("REF_BACKEND_MISSING: aucun backend (PyCBC/LALSuite) disponible")
-            raise RuntimeError(f"REF_COMPUTE_FAIL: backends disponibles ont échoué: {last_exc}")
+                raise RuntimeError(
+                    "REF_BACKEND_MISSING: aucun backend (PyCBC/LALSuite) disponible"
+                )
+            raise RuntimeError(
+                f"REF_COMPUTE_FAIL: backends disponibles ont échoué: {last_exc}"
+            )

         # Écriture cache disque + LRU mémoire
         try:
-            _atomic_write_npz(cache_path, {"phi_on_grid": np.asarray(phi_on_grid, dtype=np.float64)})
-            _evict_cache_if_needed(cache_dir=cache_dir, quota_bytes=CACHE_DISK_QUOTA_BYTES)
+            _atomic_write_npz(
+                cache_path, {"phi_on_grid": np.asarray(phi_on_grid, dtype=np.float64)}
+            )
+            _evict_cache_if_needed(
+                cache_dir=cache_dir, quota_bytes=CACHE_DISK_QUOTA_BYTES
+            )
         except Exception as e:
             logger.warning("Écriture cache disque impossible (%s) : %s", cache_path, e)

@@ -393,7 +423,9 @@ def ref_cache_info(cache_dir: Optional[str] = None) -> dict:
                 st = os.stat(p)
                 info["n_files"] += 1
                 info["total_bytes"] += st.st_size
-                info["entries"].append({"file": fn, "size": st.st_size, "mtime": st.st_mtime})
+                info["entries"].append(
+                    {"file": fn, "size": st.st_size, "mtime": st.st_mtime}
+                )
             except Exception:
                 continue
     return info
@@ -403,7 +435,9 @@ def ref_cache_info(cache_dir: Optional[str] = None) -> dict:
 if __name__ == "__main__":
     import argparse

-    parser = argparse.ArgumentParser(description="Test rapide du backend φ_ref (PyCBC/LALSuite) et cache.")
+    parser = argparse.ArgumentParser(
+        description="Test rapide du backend φ_ref (PyCBC/LALSuite) et cache."
+    )
     parser.add_argument("--fmin", type=float, default=10.0)
     parser.add_argument("--fmax", type=float, default=2041.7379)
     parser.add_argument("--dlog10", type=float, default=0.01)
@@ -416,9 +450,25 @@ if __name__ == "__main__":
     # Construire grille log10
     n = int(np.ceil((np.log10(args.fmax) - np.log10(args.fmin)) / args.dlog10)) + 1
     fgrid = np.logspace(np.log10(args.fmin), np.log10(args.fmax), n)
-    logger.info("Test compute_phi_ref: grille %d points, m1=%s m2=%s", fgrid.size, args.m1, args.m2)
+    logger.info(
+        "Test compute_phi_ref: grille %d points, m1=%s m2=%s",
+        fgrid.size,
+        args.m1,
+        args.m2,
+    )
     try:
-        phi = compute_phi_ref(fgrid, args.m1, args.m2, approximant=args.approximant, cache_dir=args.cache_dir)
-        logger.info("Phase calculée, %d points (min/max) = (%g, %g)", phi.size, float(np.min(phi)), float(np.max(phi)))
+        phi = compute_phi_ref(
+            fgrid,
+            args.m1,
+            args.m2,
+            approximant=args.approximant,
+            cache_dir=args.cache_dir,
+        )
+        logger.info(
+            "Phase calculée, %d points (min/max) = (%g, %g)",
+            phi.size,
+            float(np.min(phi)),
+            float(np.max(phi)),
+        )
     except Exception as e:
         logger.exception("Échec compute_phi_ref (test) : %s", e)
diff --git a/mcgt/phase.py b/mcgt/phase.py
index bd834d3..5b439d5 100755
--- a/mcgt/phase.py
+++ b/mcgt/phase.py
@@ -15,6 +15,7 @@ Fonctions exposées
 - corr_phase()           : correcteur analytique δφ(f) = ∫δt(f) df
 - solve_mcgt()           : phase MCGT = φ_GR − δφ
 """
+
 from __future__ import annotations

 from dataclasses import dataclass
@@ -35,13 +36,13 @@ __all__ = [
 # ----------------------------------------------------------------------#
 @dataclass
 class PhaseParams:
-    m1: float              # masse primaire [M☉]
-    m2: float              # masse secondaire [M☉]
-    q0star: float          # amplitude du correcteur MCGT
-    alpha: float           # exposant α (param2)
-    phi0: float = 0.0      # phase initiale à fmin [rad]
-    tc: float = 0.0        # temps de coalescence t_c [s]
-    tol: float = 1e-8      # tolérance numérique
+    m1: float  # masse primaire [M☉]
+    m2: float  # masse secondaire [M☉]
+    q0star: float  # amplitude du correcteur MCGT
+    alpha: float  # exposant α (param2)
+    phi0: float = 0.0  # phase initiale à fmin [rad]
+    tc: float = 0.0  # temps de coalescence t_c [s]
+    tol: float = 1e-8  # tolérance numérique


 # ----------------------------------------------------------------------#
@@ -78,11 +79,13 @@ def check_log_spacing(grid: np.ndarray, atol: float = 1e-12) -> bool:
 # ----------------------------------------------------------------------#
 _CPN = {
     0: 1.0,
-    2: (3715 / 756 + 55 / 9),         # coefficients simplifiés
+    2: (3715 / 756 + 55 / 9),  # coefficients simplifiés
     3: -16 * np.pi,
     4: (15293365 / 508032 + 27145 / 504 + 3085 / 72),
     5: np.pi * (38645 / 756 - 65 / 9) * (1 + 3 * np.log(np.pi)),
-    6: (11583231236531 / 4694215680 - 640 / 3 * np.pi**2 - 6848 / 21 * np.log(4 * np.pi)),
+    6: (
+        11583231236531 / 4694215680 - 640 / 3 * np.pi**2 - 6848 / 21 * np.log(4 * np.pi)
+    ),
     7: np.pi * (77096675 / 254016 + 378515 / 1512),
 }

@@ -116,9 +119,9 @@ def phi_gr(freqs: np.ndarray, p: PhaseParams) -> np.ndarray:
         raise ValueError("La grille freqs doit être 1D et strictement croissante.")

     # Conversion masse solaire → secondes (G = c = 1)
-    M_s = (p.m1 + p.m2) * 4.925490947e-6     # masse totale (s)
+    M_s = (p.m1 + p.m2) * 4.925490947e-6  # masse totale (s)
     eta = _symmetric_eta(p.m1, p.m2)
-    v = (np.pi * M_s * freqs) ** (1 / 3)     # vitesse PN
+    v = (np.pi * M_s * freqs) ** (1 / 3)  # vitesse PN

     # Série PN
     series = np.zeros_like(freqs)
@@ -132,7 +135,9 @@ def phi_gr(freqs: np.ndarray, p: PhaseParams) -> np.ndarray:
 # ----------------------------------------------------------------------#
 # 4. Correcteur analytique δφ (δt(f) = q0★ · f^(−α))
 # ----------------------------------------------------------------------#
-def corr_phase(freqs: np.ndarray, fmin: float, q0star: float, alpha: float) -> np.ndarray:
+def corr_phase(
+    freqs: np.ndarray, fmin: float, q0star: float, alpha: float
+) -> np.ndarray:
     """
     Correction δφ(f) induite par un décalage temporel δt(f)=q0★·f^(−α).

@@ -142,13 +147,17 @@ def corr_phase(freqs: np.ndarray, fmin: float, q0star: float, alpha: float) -> n
     freqs = np.asarray(freqs, dtype=float)
     if np.isclose(alpha, 1.0):
         return 2 * np.pi * q0star * np.log(freqs / fmin)
-    return (2 * np.pi * q0star / (1 - alpha)) * (freqs ** (1 - alpha) - fmin ** (1 - alpha))
+    return (2 * np.pi * q0star / (1 - alpha)) * (
+        freqs ** (1 - alpha) - fmin ** (1 - alpha)
+    )


 # ----------------------------------------------------------------------#
 # 5. Solveur global MCGT
 # ----------------------------------------------------------------------#
-def solve_mcgt(freqs: np.ndarray, p: PhaseParams, fmin: float | None = None) -> np.ndarray:
+def solve_mcgt(
+    freqs: np.ndarray, p: PhaseParams, fmin: float | None = None
+) -> np.ndarray:
     """
     Calcule la phase MCGT sur `freqs` :

diff --git a/mcgt/scalar_perturbations.py b/mcgt/scalar_perturbations.py
index 23acf36..46ed54c 100755
--- a/mcgt/scalar_perturbations.py
+++ b/mcgt/scalar_perturbations.py
@@ -15,6 +15,7 @@ Remarques :
 - Les chemins et noms de fichiers doivent être en anglais côté I/O ; seul
   le contenu textuel (commentaires/docstrings) reste en français.
 """
+
 from __future__ import annotations

 from dataclasses import dataclass
@@ -36,6 +37,7 @@ __all__ = [
     "_default_params",
 ]

+
 # -----------------------------------------------------------------------------#
 # 1) Dataclass des paramètres
 # -----------------------------------------------------------------------------#
@@ -65,9 +67,9 @@ class PertParams:
     k_split: float

     # Dynamique Φ & gel progressif
-    a_eq: float              # facteur d’échelle à l’égalité rad−mat
-    freeze_scale: float      # ~1e5 (plus petit ⇒ gel plus doux)
-    Phi0: float              # Φ(k≈0, a≪a_eq)
+    a_eq: float  # facteur d’échelle à l’égalité rad−mat
+    freeze_scale: float  # ~1e5 (plus petit ⇒ gel plus doux)
+    Phi0: float  # Φ(k≈0, a≪a_eq)

     # Champs optionnels
     alpha: Optional[float] = None
@@ -153,7 +155,7 @@ def compute_cs2(k_vals: np.ndarray, a_vals: np.ndarray, p: PertParams) -> np.nda
     cs2_a = PchipInterpolator(a_vals, cs2_a, extrapolate=True)(a_vals)

     # Filtre gaussien en k + amplitude globale
-    T = np.exp(-(K / p.k0) ** 2)
+    T = np.exp(-((K / p.k0) ** 2))
     cs2 = T * cs2_a[np.newaxis, :] * p.cs2_param

     # Contrôle physique strict
@@ -182,7 +184,7 @@ def _kg_eq(a: float, y: np.ndarray, k: float, p: PertParams) -> np.ndarray:
     # potentiel métrique Φ(k,a)
     Phi0 = p.Phi0
     a_eq = p.a_eq
-    Phi = Phi0 * np.exp(-(k / p.k_split) ** 2) / (1.0 + (a / a_eq) ** 3)
+    Phi = Phi0 * np.exp(-((k / p.k_split) ** 2)) / (1.0 + (a / a_eq) ** 3)

     # source
     φ0 = float(phi0_of_a(a, p))
@@ -196,7 +198,9 @@ def _kg_eq(a: float, y: np.ndarray, k: float, p: PertParams) -> np.ndarray:
 # -----------------------------------------------------------------------------#
 # 5) δφ/φ(k,a)
 # -----------------------------------------------------------------------------#
-def compute_delta_phi(k_vals: np.ndarray, a_vals: np.ndarray, p: PertParams) -> np.ndarray:
+def compute_delta_phi(
+    k_vals: np.ndarray, a_vals: np.ndarray, p: PertParams
+) -> np.ndarray:
     """
     Intègre δφ/φ(k,a) sur la grille (n_k, n_a) via solve_ivp (Radau).

@@ -223,7 +227,9 @@ def compute_delta_phi(k_vals: np.ndarray, a_vals: np.ndarray, p: PertParams) ->
         # gel progressif continu (suppression des modes profonds sous-horizon)
         freeze = np.exp(-((k / a_min) / H_a_min) / p.freeze_scale)

-        init_amp = freeze * p.delta_phi_param * p.phi0_init * np.exp(-(k / p.k_split) ** 2)
+        init_amp = (
+            freeze * p.delta_phi_param * p.phi0_init * np.exp(-((k / p.k_split) ** 2))
+        )

         sol = integrate.solve_ivp(
             lambda aa, yy: _kg_eq(float(aa), yy, float(k), p),
@@ -270,7 +276,9 @@ def test_delta_phi_against_reference(
     if not f.exists():
         return
     k_ref, d_ref = _load_ref_csv(f)
-    d_test = compute_delta_phi(k_ref, np.array([1.0], dtype=float), _default_params())[:, -1]
+    d_test = compute_delta_phi(k_ref, np.array([1.0], dtype=float), _default_params())[
+        :, -1
+    ]
     assert np.allclose(d_ref, d_test, rtol=0.2)


diff --git a/zz-checklists/CHAPTER01_CHECKLIST.txt b/zz-checklists/CHAPTER01_CHECKLIST.txt
index 8079c52..8919557 100755
--- a/zz-checklists/CHAPTER01_CHECKLIST.txt
+++ b/zz-checklists/CHAPTER01_CHECKLIST.txt
@@ -49,7 +49,7 @@ CHECKLIST — CHAPITRE 01 (Introduction & Applications, MCGT)
       • 01_dimensionless_invariants.csv             (T, I1)

 [PRODUCTION DES FIGURES]
-- [ ] Générer :
+- [ ] Générer :
       python zz-scripts/chapter01/plot_fig01_early_plateau.py
       python zz-scripts/chapter01/plot_fig02_logistic_calibration.py
       python zz-scripts/chapter01/plot_fig03_relative_error_timeline.py
diff --git a/zz-checklists/CHAPTER02_CHECKLIST.txt b/zz-checklists/CHAPTER02_CHECKLIST.txt
index 96b3057..7840425 100755
--- a/zz-checklists/CHAPTER02_CHECKLIST.txt
+++ b/zz-checklists/CHAPTER02_CHECKLIST.txt
@@ -110,4 +110,3 @@ CHECKLIST — CHAPITRE 02 (Validation chronologique, MCGT)
 - [ ] Schémas   :  validate_json.py & validate_csv_table.py (voir bloc “Validation”)
 - [ ] LaTeX     :  commandes pdflatex ci-dessus
 - [ ] Diagnostic:  diag_consistency.py (report MD) + mise à jour manifeste des figures
-
diff --git a/zz-checklists/CHAPTER04_CHECKLIST.txt b/zz-checklists/CHAPTER04_CHECKLIST.txt
index d6cb3b5..54470ed 100755
--- a/zz-checklists/CHAPTER04_CHECKLIST.txt
+++ b/zz-checklists/CHAPTER04_CHECKLIST.txt
@@ -104,4 +104,3 @@ CHECKLIST — CHAPITRE 04 (Invariants adimensionnels, MCGT)
 - [ ] Figures   :  exécuter les 4 scripts plot_fig**
 - [ ] Validation:  (schéma CSV si disponible) validate_csv_table.py ; sinon tests zz-tests
 - [ ] LaTeX     :  commandes pdflatex ci-dessus
-
diff --git a/zz-checklists/CHAPTER07_CHECKLIST.txt b/zz-checklists/CHAPTER07_CHECKLIST.txt
index f02a309..a48c47d 100755
--- a/zz-checklists/CHAPTER07_CHECKLIST.txt
+++ b/zz-checklists/CHAPTER07_CHECKLIST.txt
@@ -157,4 +157,3 @@ CHECKLIST — CHAPITRE 07 (Perturbations scalaires / Solveur MCGT)
 - [ ] Données :  python zz-scripts/chapter07/generate_data_chapter07.py --ini zz-configuration/scalar_perturbations.ini --export-derivative --export-matrix
 - [ ] Figures :  exécuter les scripts plot_fig** / tracer_fig**
 - [ ] LaTeX   :  2× pdflatex pour chaque .tex (conceptuel, details)
-
diff --git a/zz-checklists/CHAPTER08_CHECKLIST.txt b/zz-checklists/CHAPTER08_CHECKLIST.txt
index fd91cd5..44024be 100755
--- a/zz-checklists/CHAPTER08_CHECKLIST.txt
+++ b/zz-checklists/CHAPTER08_CHECKLIST.txt
@@ -169,4 +169,3 @@ CHECKLIST — CHAPITRE 08 (Couplage sombre : BAO + Pantheon+)
       python zz-scripts/chapter08/plot_fig01_chi2_total_vs_q0.py  # etc.
 - [ ] LaTeX :
       pdflatex 08-couplage-sombre/08_couplage_sombre_conceptuel.tex ; pdflatex 08-couplage_sombre/08_couplage_sombre_details.tex
-
diff --git a/zz-checklists/CHAPTER09_CHECKLIST.txt b/zz-checklists/CHAPTER09_CHECKLIST.txt
index e213385..047a006 100755
--- a/zz-checklists/CHAPTER09_CHECKLIST.txt
+++ b/zz-checklists/CHAPTER09_CHECKLIST.txt
@@ -186,4 +186,3 @@ PY
       python zz-scripts/chapter09/plot_fig01_phase_overlay.py   # etc. (02,03,04,05)

 [FIN DE CHECKLIST]
-
diff --git a/zz-configuration/README.md b/zz-configuration/README.md
index 3026ee2..84e2d29 100755
--- a/zz-configuration/README.md
+++ b/zz-configuration/README.md
@@ -37,6 +37,3 @@ Ce répertoire contient les fichiers de configuration partagés entre chapitres.
 &nbsp; et un fichier `\*.template` (comme celui fourni) à la place.

 \- Documenter toute variable non triviale directement dans le `.ini` avec un commentaire.
-
-
-
diff --git a/zz-configuration/perturbations_07.ini b/zz-configuration/perturbations_07.ini
index 4dc841f..47a364e 100755
--- a/zz-configuration/perturbations_07.ini
+++ b/zz-configuration/perturbations_07.ini
@@ -82,4 +82,3 @@ k_min_hmpc = 1.0e-4
 k_max_hmpc = 1.0e+1
 dlog_k = 0.01
 n_k = 0
-
diff --git a/zz-data/chapter02/02_optimal_parameters.json b/zz-data/chapter02/02_optimal_parameters.json
index 6d0bbd8..75efe83 100755
--- a/zz-data/chapter02/02_optimal_parameters.json
+++ b/zz-data/chapter02/02_optimal_parameters.json
@@ -22,4 +22,4 @@
   },
   "max_epsilon_primary": 2.1947570587947003e-05,
   "max_epsilon_order2": 0.07194300543935292
-}
\ No newline at end of file
+}
diff --git a/zz-data/chapter02/02_primordial_spectrum_spec.json b/zz-data/chapter02/02_primordial_spectrum_spec.json
index d5be0e4..80b9d4c 100755
--- a/zz-data/chapter02/02_primordial_spectrum_spec.json
+++ b/zz-data/chapter02/02_primordial_spectrum_spec.json
@@ -12,4 +12,4 @@
     "c2": 0.01,
     "c2_2": 0.0
   }
-}
\ No newline at end of file
+}
diff --git a/zz-data/chapter02/02_primordial_spectrum_spec.json.bak b/zz-data/chapter02/02_primordial_spectrum_spec.json.bak
index c9a8b5a..d5c632c 100644
--- a/zz-data/chapter02/02_primordial_spectrum_spec.json.bak
+++ b/zz-data/chapter02/02_primordial_spectrum_spec.json.bak
@@ -12,4 +12,4 @@
     "c2": 0.01,
     "c2_2": 0.0
   }
-}
\ No newline at end of file
+}
diff --git a/zz-data/chapter05/05_bbn_params.json b/zz-data/chapter05/05_bbn_params.json
index 78a7d2a..8b4f5c4 100755
--- a/zz-data/chapter05/05_bbn_params.json
+++ b/zz-data/chapter05/05_bbn_params.json
@@ -5,4 +5,4 @@
     "primary": 0.0,
     "order2": 0.0
   }
-}
\ No newline at end of file
+}
diff --git a/zz-data/chapter06/06_params_cmb.json b/zz-data/chapter06/06_params_cmb.json
index bff84a8..91b4433 100755
--- a/zz-data/chapter06/06_params_cmb.json
+++ b/zz-data/chapter06/06_params_cmb.json
@@ -20,4 +20,4 @@
   "c1": 0.1,
   "c2": 0.01,
   "max_delta_Cl_rel": 0.6296832401981298
-}
\ No newline at end of file
+}
diff --git a/zz-data/chapter08/08_coupling_params.json b/zz-data/chapter08/08_coupling_params.json
index 8a7dbbc..46697ff 100755
--- a/zz-data/chapter08/08_coupling_params.json
+++ b/zz-data/chapter08/08_coupling_params.json
@@ -8,4 +8,4 @@
   "param2_min": -1.0,
   "param2_max": 1.0,
   "n_param2": 101
-}
\ No newline at end of file
+}
diff --git a/zz-data/chapter09/09_best_params.json b/zz-data/chapter09/09_best_params.json
index 850b497..cad9498 100755
--- a/zz-data/chapter09/09_best_params.json
+++ b/zz-data/chapter09/09_best_params.json
@@ -44,4 +44,4 @@
     "scan_csv": "/home/jplal/MCGT/zz-data/chapitre9/09_scan_20250816T182747Z_-0.50_0.60_21x21.csv",
     "checkpoint_csv": "/home/jplal/MCGT/zz-data/chapitre9/09_scan_20250816T182747Z_-0.50_0.60_21x21_ckpt.csv"
   }
-}
\ No newline at end of file
+}
diff --git a/zz-data/chapter09/09_metrics_phase.json b/zz-data/chapter09/09_metrics_phase.json
index da3261d..23538f5 100755
--- a/zz-data/chapter09/09_metrics_phase.json
+++ b/zz-data/chapter09/09_metrics_phase.json
@@ -101,4 +101,4 @@
     "k_cycles": -56
   },
   "poly_rebranch_k_cycles": 0
-}
\ No newline at end of file
+}
diff --git a/zz-data/chapter09/09_phases_imrphenom.meta.json b/zz-data/chapter09/09_phases_imrphenom.meta.json
index 0cea480..6f03bce 100755
--- a/zz-data/chapter09/09_phases_imrphenom.meta.json
+++ b/zz-data/chapter09/09_phases_imrphenom.meta.json
@@ -12,4 +12,4 @@
     "pandas": "2.3.1",
     "lalsuite": "6.2.0"
   }
-}
\ No newline at end of file
+}
diff --git a/zz-data/chapter10/10_mc_best.json b/zz-data/chapter10/10_mc_best.json
index 7f22a73..3f3e3d4 100755
--- a/zz-data/chapter10/10_mc_best.json
+++ b/zz-data/chapter10/10_mc_best.json
@@ -865,4 +865,4 @@
       "tc": 0.0
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/zz-manifests/add_to_manifest.py b/zz-manifests/add_to_manifest.py
index 24eaeaa..7ffa2fe 100755
--- a/zz-manifests/add_to_manifest.py
+++ b/zz-manifests/add_to_manifest.py
@@ -19,21 +19,39 @@

 from __future__ import annotations
 from pathlib import Path
-import argparse, json, hashlib, datetime, shutil, sys, tempfile, subprocess, glob, mimetypes, os
-from typing import Dict, Any, Iterable, List, Optional, Tuple
+import argparse
+import json
+import hashlib
+import datetime
+import shutil
+import sys
+import tempfile
+import subprocess
+import glob
+import mimetypes
+import os
+from typing import Dict, Any, List, Optional, Tuple

 UTC = datetime.timezone.utc
 REPO_ROOT = Path.cwd().resolve()

 # ------------------------- utilitaires horodatage / hash -------------------------

+
 def utc_now_iso() -> str:
-    return datetime.datetime.now(UTC).replace(microsecond=0).isoformat().replace("+00:00", "Z")
+    return (
+        datetime.datetime.now(UTC)
+        .replace(microsecond=0)
+        .isoformat()
+        .replace("+00:00", "Z")
+    )
+

 def iso_mtime(path: Path) -> str:
     t = datetime.datetime.fromtimestamp(path.stat().st_mtime, tz=UTC)
     return t.replace(microsecond=0).isoformat().replace("+00:00", "Z")

+
 def sha256_of_file(path: Path) -> str:
     h = hashlib.sha256()
     with path.open("rb") as f:
@@ -41,11 +59,15 @@ def sha256_of_file(path: Path) -> str:
             h.update(b)
     return h.hexdigest()

+
 def git_hash_of(path: Path) -> Optional[str]:
     try:
         res = subprocess.run(
             ["git", "hash-object", str(path)],
-            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=False
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+            text=True,
+            check=False,
         )
         if res.returncode == 0:
             return res.stdout.strip()
@@ -53,8 +75,10 @@ def git_hash_of(path: Path) -> Optional[str]:
         pass
     return None

+
 # ------------------------- heuristiques rôle / chapitre / type -------------------

+
 def guess_role(p: Path) -> str:
     s = str(p).replace("\\", "/")
     if "/zz-schemas/" in s:
@@ -73,6 +97,7 @@ def guess_role(p: Path) -> str:
         return "source"
     return "artifact"

+
 def guess_chapter_tag(p: Path) -> Optional[str]:
     s = p.as_posix().lower()
     # supporte chapter09, chapter9, chapitre9, etc.
@@ -91,6 +116,7 @@ def guess_chapter_tag(p: Path) -> Optional[str]:
         return f"chapter{int(stem[:2]):02d}"
     return None

+
 def guess_media_type(p: Path) -> str:
     mt, _ = mimetypes.guess_type(p.name)
     return mt or {
@@ -99,12 +125,14 @@ def guess_media_type(p: Path) -> str:
         ".dat": "text/plain",
         ".png": "image/png",
         ".ini": "text/plain",
-        ".md":  "text/markdown",
+        ".md": "text/markdown",
         ".tex": "text/x-tex",
     }.get(p.suffix.lower(), "application/octet-stream")

+
 # ------------------------- I/O manifest -------------------------

+
 def manifest_skeleton() -> Dict[str, Any]:
     return {
         "manifest_version": "1.0",
@@ -113,9 +141,10 @@ def manifest_skeleton() -> Dict[str, Any]:
         "generated_at": utc_now_iso(),
         "total_entries": 0,
         "total_size_bytes": 0,
-        "entries": []  # liste d'objets {path, role, size_bytes, sha256, mtime_iso, ...}
+        "entries": [],  # liste d'objets {path, role, size_bytes, sha256, mtime_iso, ...}
     }

+
 def load_manifest(path_manifest: Path) -> Dict[str, Any]:
     if path_manifest.exists():
         try:
@@ -132,11 +161,16 @@ def load_manifest(path_manifest: Path) -> Dict[str, Any]:
                 obj.pop("files", None)
             return obj
         except Exception as e:
-            print(f"ERROR: cannot parse manifest: {path_manifest} -> {e}", file=sys.stderr)
+            print(
+                f"ERROR: cannot parse manifest: {path_manifest} -> {e}", file=sys.stderr
+            )
             sys.exit(2)
     return manifest_skeleton()

-def write_manifest_atomic(path: Path, obj: Dict[str, Any], do_backup: bool=True) -> None:
+
+def write_manifest_atomic(
+    path: Path, obj: Dict[str, Any], do_backup: bool = True
+) -> None:
     # champs agrégés
     obj["generated_at"] = utc_now_iso()
     entries = obj.get("entries", [])
@@ -149,28 +183,42 @@ def write_manifest_atomic(path: Path, obj: Dict[str, Any], do_backup: bool=True)
         f.write("\n")

     if do_backup and path.exists():
-        bak = path.with_suffix(path.suffix + "." + datetime.datetime.now(UTC).strftime("%Y%m%dT%H%M%SZ") + ".bak")
+        bak = path.with_suffix(
+            path.suffix
+            + "."
+            + datetime.datetime.now(UTC).strftime("%Y%m%dT%H%M%SZ")
+            + ".bak"
+        )
         shutil.copy2(path, bak)
         print("Backup manifest ->", bak)

     Path(tmp_name).replace(path)
     print("Wrote:", path)

+
 # ------------------------- collecte des chemins -------------------------

+
 def expand_paths(arg_path: str) -> List[Path]:
     # "-" -> stdin (une voie par ligne)
     if arg_path == "-":
         items = [ln.strip() for ln in sys.stdin.read().splitlines() if ln.strip()]
         paths: List[Path] = []
         for it in items:
-            paths.extend(Path().glob(it) if any(ch in it for ch in "*?[]") else [Path(it)])
-        return [p.resolve() for sub in paths for p in ([sub] if isinstance(sub, Path) else [])]
+            paths.extend(
+                Path().glob(it) if any(ch in it for ch in "*?[]") else [Path(it)]
+            )
+        return [
+            p.resolve()
+            for sub in paths
+            for p in ([sub] if isinstance(sub, Path) else [])
+        ]
     # motif glob ?
     if any(ch in arg_path for ch in "*?[]"):
         return [Path(p).resolve() for p in glob.glob(arg_path)]
     return [Path(arg_path).expanduser().resolve()]

+
 def read_list_file(path: Path) -> List[Path]:
     items = []
     for ln in path.read_text(encoding="utf-8").splitlines():
@@ -183,6 +231,7 @@ def read_list_file(path: Path) -> List[Path]:
             items.append(Path(ln))
     return [p.resolve() for p in items]

+
 def to_rel_repo(p: Path) -> str:
     try:
         rel = p.relative_to(REPO_ROOT)
@@ -191,9 +240,17 @@ def to_rel_repo(p: Path) -> str:
         rel = Path(os.path.relpath(str(p), str(REPO_ROOT)))
     return rel.as_posix()

+
 # ------------------------- opérations sur le manifest -------------------------

-def upsert_entry(manifest: Dict[str, Any], path: Path, role: Optional[str], tags: List[str], set_git: bool) -> Tuple[bool, Dict[str, Any]]:
+
+def upsert_entry(
+    manifest: Dict[str, Any],
+    path: Path,
+    role: Optional[str],
+    tags: List[str],
+    set_git: bool,
+) -> Tuple[bool, Dict[str, Any]]:
     if not path.exists():
         print("WARN: file not found, skip ->", path, file=sys.stderr)
         return False, {}
@@ -246,9 +303,11 @@ def upsert_entry(manifest: Dict[str, Any], path: Path, role: Optional[str], tags
     print("ADDED    :", rel)
     return True, payload

+
 def remove_entries(manifest: Dict[str, Any], pattern: str) -> int:
     # pattern sur le champ "path" (glob)
     import fnmatch
+
     entries: List[Dict[str, Any]] = manifest.get("entries", [])
     keep = []
     removed = 0
@@ -262,22 +321,71 @@ def remove_entries(manifest: Dict[str, Any], pattern: str) -> int:
     manifest["entries"] = keep
     return removed

+
 # ------------------------- CLI -------------------------

+
 def parse_args(argv: List[str]) -> argparse.Namespace:
-    p = argparse.ArgumentParser(description="Register/update files in zz-manifests/manifest_master.json (repro inventory).")
+    p = argparse.ArgumentParser(
+        description="Register/update files in zz-manifests/manifest_master.json (repro inventory)."
+    )
     gsrc = p.add_mutually_exclusive_group(required=False)
-    gsrc.add_argument("path", nargs="?", default=None, help="file path or glob (use '-' to read from stdin)")
-    gsrc.add_argument("--from-list", dest="from_list", help="text file listing paths/globs (one per line)")
-    p.add_argument("--manifest", default="zz-manifests/manifest_master.json", help="manifest JSON path (default: zz-manifests/manifest_master.json)")
-    p.add_argument("--role", choices=["data", "figure", "schema", "manifest", "config", "script", "artifact", "source"], help="force role for all inputs")
-    p.add_argument("--tags", default="", help="comma-separated tags to attach (e.g., chapter09,phase)")
-    p.add_argument("--no-backup", action="store_true", help="do not create timestamped .bak before writing")
-    p.add_argument("--with-git-hash", action="store_true", help="store git blob hash (if available)")
-    p.add_argument("--remove", metavar="GLOB", help="remove entries whose path matches this glob (e.g., 'zz-data/chapter09/*.tmp')")
-    p.add_argument("--dry-run", action="store_true", help="do not write manifest (report only)")
+    gsrc.add_argument(
+        "path",
+        nargs="?",
+        default=None,
+        help="file path or glob (use '-' to read from stdin)",
+    )
+    gsrc.add_argument(
+        "--from-list",
+        dest="from_list",
+        help="text file listing paths/globs (one per line)",
+    )
+    p.add_argument(
+        "--manifest",
+        default="zz-manifests/manifest_master.json",
+        help="manifest JSON path (default: zz-manifests/manifest_master.json)",
+    )
+    p.add_argument(
+        "--role",
+        choices=[
+            "data",
+            "figure",
+            "schema",
+            "manifest",
+            "config",
+            "script",
+            "artifact",
+            "source",
+        ],
+        help="force role for all inputs",
+    )
+    p.add_argument(
+        "--tags",
+        default="",
+        help="comma-separated tags to attach (e.g., chapter09,phase)",
+    )
+    p.add_argument(
+        "--no-backup",
+        action="store_true",
+        help="do not create timestamped .bak before writing",
+    )
+    p.add_argument(
+        "--with-git-hash",
+        action="store_true",
+        help="store git blob hash (if available)",
+    )
+    p.add_argument(
+        "--remove",
+        metavar="GLOB",
+        help="remove entries whose path matches this glob (e.g., 'zz-data/chapter09/*.tmp')",
+    )
+    p.add_argument(
+        "--dry-run", action="store_true", help="do not write manifest (report only)"
+    )
     return p.parse_args(argv)

+
 def main(argv: List[str]) -> int:
     args = parse_args(argv)
     manifest_path = Path(args.manifest)
@@ -315,11 +423,13 @@ def main(argv: List[str]) -> int:
                 if sub.is_file():
                     rp = sub.resolve()
                     if rp not in seen:
-                        input_files.append(rp); seen.add(rp)
+                        input_files.append(rp)
+                        seen.add(rp)
         elif p.is_file():
             rp = p.resolve()
             if rp not in seen:
-                input_files.append(rp); seen.add(rp)
+                input_files.append(rp)
+                seen.add(rp)
         else:
             # motif qui n'a rien trouvé ou chemin inexistant
             continue
@@ -345,5 +455,6 @@ def main(argv: List[str]) -> int:
     write_manifest_atomic(manifest_path, manifest, do_backup=not args.no_backup)
     return 0

+
 if __name__ == "__main__":
     sys.exit(main(sys.argv[1:]))
diff --git a/zz-manifests/chapters/chapter_manifest_01.json.bak b/zz-manifests/chapters/chapter_manifest_01.json.bak
index f611567..8ef9474 100755
--- a/zz-manifests/chapters/chapter_manifest_01.json.bak
+++ b/zz-manifests/chapters/chapter_manifest_01.json.bak
@@ -63,4 +63,3 @@
 ],
 "notes": "Homogénéiser les noms de colonnes; vérifier la cohérence des dérivées vs Chap.2."
 }
-
diff --git a/zz-manifests/chapters/chapter_manifest_02.json b/zz-manifests/chapters/chapter_manifest_02.json
index db38725..0915e36 100755
--- a/zz-manifests/chapters/chapter_manifest_02.json
+++ b/zz-manifests/chapters/chapter_manifest_02.json
@@ -63,4 +63,4 @@
 "Séries F/G: vérifier ordres et signes."
 ],
 "notes": "Contrôler la jonction low/high autour du split; vérifier les écarts relatifs sur jalons."
-}
\ No newline at end of file
+}
diff --git a/zz-manifests/chapters/chapter_manifest_03.json b/zz-manifests/chapters/chapter_manifest_03.json
index 5e02b66..1cd3372 100755
--- a/zz-manifests/chapters/chapter_manifest_03.json
+++ b/zz-manifests/chapters/chapter_manifest_03.json
@@ -63,4 +63,4 @@
 "Consistance `03_meta_stability_fR.json` ↔ fichiers réellement présents."
 ],
 "notes": "Frontière précédemment absente/vide; documenter l’origine de m_s2/R0 ≈ 3.33e5."
-}
\ No newline at end of file
+}
diff --git a/zz-manifests/chapters/chapter_manifest_04.json b/zz-manifests/chapters/chapter_manifest_04.json
index 4f3548a..e7cba17 100755
--- a/zz-manifests/chapters/chapter_manifest_04.json
+++ b/zz-manifests/chapters/chapter_manifest_04.json
@@ -48,4 +48,4 @@
 "Ancien nom `04_donnees_T_contre_T.dat` remplacé par `04_T_reference_grid.dat`."
 ],
 "notes": "Erreur I3 identifiée; prévoir recalcul et figure mise à jour."
-}
\ No newline at end of file
+}
diff --git a/zz-manifests/chapters/chapter_manifest_05.json b/zz-manifests/chapters/chapter_manifest_05.json
index 60f320f..fcdf9f2 100755
--- a/zz-manifests/chapters/chapter_manifest_05.json
+++ b/zz-manifests/chapters/chapter_manifest_05.json
@@ -53,4 +53,4 @@
 "Forme de χ²(T) justifiée (artefacts exclus)."
 ],
 "notes": "Corriger mojibake; remplir dérivée; documenter seuils spécifiques si dérogation."
-}
\ No newline at end of file
+}
diff --git a/zz-manifests/chapters/chapter_manifest_06.json b/zz-manifests/chapters/chapter_manifest_06.json
index 42e14a0..a642c5a 100755
--- a/zz-manifests/chapters/chapter_manifest_06.json
+++ b/zz-manifests/chapters/chapter_manifest_06.json
@@ -54,4 +54,4 @@
 "Normalisation χ² dans scans (pas d’échelles absurdes)."
 ],
 "notes": "Les premières multipôles montraient delta=0: pipeline à valider avec CAMB ini."
-}
\ No newline at end of file
+}
diff --git a/zz-manifests/chapters/chapter_manifest_07.json b/zz-manifests/chapters/chapter_manifest_07.json
index d39e8c8..9eb8b24 100755
--- a/zz-manifests/chapters/chapter_manifest_07.json
+++ b/zz-manifests/chapters/chapter_manifest_07.json
@@ -60,4 +60,4 @@
 "cs2 ≈ 1 mais variabilité physique vs artefacts numériques."
 ],
 "notes": "Écart a_min relevé précédemment; revoir seuils cs2/δφ et segmentation low/high."
-}
\ No newline at end of file
+}
diff --git a/zz-manifests/chapters/chapter_manifest_08.json b/zz-manifests/chapters/chapter_manifest_08.json
index e3a257e..c1e65bc 100755
--- a/zz-manifests/chapters/chapter_manifest_08.json
+++ b/zz-manifests/chapters/chapter_manifest_08.json
@@ -62,4 +62,4 @@
 "Résidus/pulls: centrage ~~0 et σ~~1."
 ],
 "notes": "Scan 2D χ² constant observé; instrumenter le pipeline pour tracer la dépendance en `param2`."
-}
\ No newline at end of file
+}
diff --git a/zz-manifests/chapters/chapter_manifest_09.json b/zz-manifests/chapters/chapter_manifest_09.json
index 8e33790..ea37356 100755
--- a/zz-manifests/chapters/chapter_manifest_09.json
+++ b/zz-manifests/chapters/chapter_manifest_09.json
@@ -58,4 +58,4 @@
 "Fenêtres et poids (fit/metrics) conformes au global."
 ],
 "notes": "Documenter la stabilité hors fenêtre de la poly-correction (base log10, degré 5)."
-}
\ No newline at end of file
+}
diff --git a/zz-manifests/chapters/chapter_manifest_10.json b/zz-manifests/chapters/chapter_manifest_10.json
index 303d665..a4ea4e2 100755
--- a/zz-manifests/chapters/chapter_manifest_10.json
+++ b/zz-manifests/chapters/chapter_manifest_10.json
@@ -63,4 +63,4 @@
 "Bootstrap: cohérence `p95_boot` vs `p95` calc (tolérance < 2%)."
 ],
 "notes": "Harmoniser `p95` entre JSON/CSV; relier `id` ↔ résultats; définir `k` ou renommer `k_cycles`."
-}
\ No newline at end of file
+}
diff --git a/zz-manifests/diag_consistency.py b/zz-manifests/diag_consistency.py
index 3586c16..0439b09 100755
--- a/zz-manifests/diag_consistency.py
+++ b/zz-manifests/diag_consistency.py
@@ -51,6 +51,7 @@ UTC = timezone.utc

 # ------------------------- Utilitaires temps / hash / git -------------------------

+
 def utc_now_iso() -> str:
     """Horodatage ISO-8601 en UTC, sans microsecondes."""
     return datetime.now(UTC).replace(microsecond=0).isoformat().replace("+00:00", "Z")
@@ -58,7 +59,12 @@ def utc_now_iso() -> str:

 def iso_from_mtime(ts: float) -> str:
     """Convertit un mtime POSIX en ISO-8601 UTC, sans microsecondes."""
-    return datetime.fromtimestamp(ts, UTC).replace(microsecond=0).isoformat().replace("+00:00", "Z")
+    return (
+        datetime.fromtimestamp(ts, UTC)
+        .replace(microsecond=0)
+        .isoformat()
+        .replace("+00:00", "Z")
+    )


 def sha256_of(path: Path) -> str:
@@ -75,7 +81,10 @@ def git_hash_of(path: Path) -> Optional[str]:
     try:
         res = subprocess.run(
             ["git", "hash-object", str(path)],
-            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=False
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+            text=True,
+            check=False,
         )
         if res.returncode == 0:
             return res.stdout.strip()
@@ -91,6 +100,7 @@ def ensure_relative(p: Path) -> bool:

 # ------------------------- Structures de rapport -------------------------

+
 @dataclass
 class Issue:
     severity: str  # "ERROR" | "WARN"
@@ -115,6 +125,7 @@ class EntryCheck:

 # ------------------------- Lecture / gestion des règles -------------------------

+
 def load_rules(path: Optional[Path]) -> Dict[str, Any]:
     if not path:
         return {}
@@ -134,7 +145,7 @@ def normalize_path_aliases(rel_path: str, rules: Dict[str, Any]) -> str:
     norm = rel_path
     for src, dst in items:
         if norm.startswith(src):
-            norm = dst + norm[len(src):]
+            norm = dst + norm[len(src) :]
     return norm


@@ -176,23 +187,30 @@ def try_get(d: Dict[str, Any], keys: List[str], default=None):

 # ------------------------- Vérification d'une entrée du manifeste -------------------------

+
 def _compare_field(stored: Any, computed: Any) -> bool:
     return stored == computed


-def compute_info(repo_root: Path, rel_path: str) -> Tuple[Optional[Dict[str, Any]], Optional[Issue]]:
+def compute_info(
+    repo_root: Path, rel_path: str
+) -> Tuple[Optional[Dict[str, Any]], Optional[Issue]]:
     p = Path(rel_path)
     if p.is_absolute():
-        return None, Issue("ERROR", "ABSOLUTE_PATH", f"Chemin absolu interdit: {p}", -1, rel_path)
+        return None, Issue(
+            "ERROR", "ABSOLUTE_PATH", f"Chemin absolu interdit: {p}", -1, rel_path
+        )
     full = (repo_root / p).resolve()
     if not full.exists():
-        return None, Issue("ERROR", "FILE_MISSING", f"Fichier introuvable: {rel_path}", -1, rel_path)
+        return None, Issue(
+            "ERROR", "FILE_MISSING", f"Fichier introuvable: {rel_path}", -1, rel_path
+        )
     st = full.stat()
     info = {
         "size_bytes": st.st_size,
         "sha256": sha256_of(full),
         "mtime_iso": iso_from_mtime(st.st_mtime),
-        "git_hash": git_hash_of(full)
+        "git_hash": git_hash_of(full),
     }
     return info, None

@@ -202,7 +220,9 @@ def check_entry(e: Dict[str, Any], idx: int, repo_root: Path) -> EntryCheck:
     rel = e.get("path", "")
     rel_ok = ensure_relative(Path(rel))
     if not rel_ok:
-        issues.append(Issue("ERROR", "ABSOLUTE_PATH", f"path doit être relatif: {rel}", idx, rel))
+        issues.append(
+            Issue("ERROR", "ABSOLUTE_PATH", f"path doit être relatif: {rel}", idx, rel)
+        )

     info, err = compute_info(repo_root, rel)
     if err:
@@ -214,40 +234,78 @@ def check_entry(e: Dict[str, Any], idx: int, repo_root: Path) -> EntryCheck:
     if e.get("size_bytes") is None:
         issues.append(Issue("WARN", "SIZE_MISSING", "size_bytes manquant", idx, rel))
     elif not size_ok:
-        issues.append(Issue("ERROR", "SIZE_MISMATCH",
-                            f"size_bytes {e.get('size_bytes')} != {info['size_bytes']}", idx, rel))
+        issues.append(
+            Issue(
+                "ERROR",
+                "SIZE_MISMATCH",
+                f"size_bytes {e.get('size_bytes')} != {info['size_bytes']}",
+                idx,
+                rel,
+            )
+        )

     sha_ok = _compare_field(e.get("sha256"), info["sha256"])
     if e.get("sha256") is None:
         issues.append(Issue("WARN", "SHA_MISSING", "sha256 manquant", idx, rel))
     elif not sha_ok:
-        issues.append(Issue("ERROR", "SHA_MISMATCH",
-                            f"sha256 {e.get('sha256')} != {info['sha256']}", idx, rel))
+        issues.append(
+            Issue(
+                "ERROR",
+                "SHA_MISMATCH",
+                f"sha256 {e.get('sha256')} != {info['sha256']}",
+                idx,
+                rel,
+            )
+        )

     mt_ok = _compare_field(e.get("mtime_iso"), info["mtime_iso"])
     if e.get("mtime_iso") is None:
         issues.append(Issue("WARN", "MTIME_MISSING", "mtime_iso manquant", idx, rel))
     elif not mt_ok:
-        issues.append(Issue("WARN", "MTIME_DIFFERS",
-                            f"mtime_iso diffère (manifest={e.get('mtime_iso')}, fs={info['mtime_iso']})",
-                            idx, rel))
+        issues.append(
+            Issue(
+                "WARN",
+                "MTIME_DIFFERS",
+                f"mtime_iso diffère (manifest={e.get('mtime_iso')}, fs={info['mtime_iso']})",
+                idx,
+                rel,
+            )
+        )

     gh_stored = e.get("git_hash")
     gh_comp = info["git_hash"]
     git_ok = (gh_stored == gh_comp) or (gh_stored is None and gh_comp is None)
     if gh_stored is None:
-        issues.append(Issue("WARN", "GIT_HASH_MISSING", "git_hash manquant (ok si hors git)", idx, rel))
+        issues.append(
+            Issue(
+                "WARN",
+                "GIT_HASH_MISSING",
+                "git_hash manquant (ok si hors git)",
+                idx,
+                rel,
+            )
+        )
     elif gh_comp is None:
-        issues.append(Issue("WARN", "GIT_HASH_UNAVAILABLE", "git hash non disponible", idx, rel))
+        issues.append(
+            Issue("WARN", "GIT_HASH_UNAVAILABLE", "git hash non disponible", idx, rel)
+        )
     elif not git_ok:
-        issues.append(Issue("WARN", "GIT_HASH_DIFFERS",
-                            f"git_hash diffère (manifest={gh_stored}, git={gh_comp})", idx, rel))
+        issues.append(
+            Issue(
+                "WARN",
+                "GIT_HASH_DIFFERS",
+                f"git_hash diffère (manifest={gh_stored}, git={gh_comp})",
+                idx,
+                rel,
+            )
+        )

     return EntryCheck(idx, rel, exists, size_ok, sha_ok, mt_ok, git_ok, rel_ok, issues)


 # ------------------------- I/O manifeste -------------------------

+
 def _migrate_files_to_entries(obj: Dict[str, Any]) -> Dict[str, Any]:
     """Migration legacy: convertit 'files' → 'entries' si nécessaire."""
     if isinstance(obj, dict) and "entries" not in obj and "files" in obj:
@@ -274,7 +332,9 @@ def write_manifest_atomic(path: Path, obj: Dict[str, Any]) -> None:
         with open(tmp, "w", encoding="utf-8") as f:
             json.dump(obj, f, indent=2, ensure_ascii=False)
             f.write("\n")
-        bak = path.with_suffix(path.suffix + f".{datetime.now(UTC).strftime('%Y%m%dT%H%M%SZ')}.bak")
+        bak = path.with_suffix(
+            path.suffix + f".{datetime.now(UTC).strftime('%Y%m%dT%H%M%SZ')}.bak"
+        )
         if path.exists():
             shutil.copy2(path, bak)
         shutil.move(str(tmp), str(path))
@@ -289,7 +349,10 @@ def write_manifest_atomic(path: Path, obj: Dict[str, Any]) -> None:

 # ------------------------- Rapport (agrégé) -------------------------

-def build_report(results: List[EntryCheck], content_issues: List[Issue], rules_meta: Dict[str, Any]) -> Dict[str, Any]:
+
+def build_report(
+    results: List[EntryCheck], content_issues: List[Issue], rules_meta: Dict[str, Any]
+) -> Dict[str, Any]:
     errors, warnings = 0, 0
     issues = []
     for r in results:
@@ -321,31 +384,43 @@ def print_report(report: Dict[str, Any], mode: str, stream=sys.stdout) -> None:
         stream.write("\n")
         return
     if mode == "md":
-        stream.write(f"# Consistency report\n\n")
+        stream.write("# Consistency report\n\n")
         stream.write(f"- Errors: **{report['errors']}**\n")
         stream.write(f"- Warnings: **{report['warnings']}**\n\n")
         if report.get("rules"):
-            stream.write(f"- Rules version: `{report['rules'].get('schema_version','n/a')}`\n\n")
+            stream.write(
+                f"- Rules version: `{report['rules'].get('schema_version','n/a')}`\n\n"
+            )
         if report["issues"]:
             stream.write("| Sev | Code | Entry | Path | Message |\n")
             stream.write("|-----|------|-------|------|---------|\n")
             for it in report["issues"]:
-                stream.write(f"| {it['severity']} | {it['code']} | {it['entry_index']} | `{it['path']}` | {it['message']} |\n")
+                stream.write(
+                    f"| {it['severity']} | {it['code']} | {it['entry_index']} | `{it['path']}` | {it['message']} |\n"
+                )
         else:
             stream.write("No problems detected.\n")
         return
     # texte
     stream.write(f"Errors: {report['errors']}  |  Warnings: {report['warnings']}\n")
     for it in report["issues"]:
-        stream.write(f"- [{it['severity']}] {it['code']} (#{it['entry_index']}:{it['path']}): {it['message']}\n")
+        stream.write(
+            f"- [{it['severity']}] {it['code']} (#{it['entry_index']}:{it['path']}): {it['message']}\n"
+        )
     if not report["issues"]:
         stream.write("OK: no problems detected.\n")


 # ------------------------- Corrections manifeste -------------------------

-def apply_fixes(manifest: Dict[str, Any], results: List[EntryCheck], repo_root: Path,
-                normalize_paths: bool, strip_internal: bool) -> int:
+
+def apply_fixes(
+    manifest: Dict[str, Any],
+    results: List[EntryCheck],
+    repo_root: Path,
+    normalize_paths: bool,
+    strip_internal: bool,
+) -> int:
     updated = 0
     for r in results:
         if not r.exists:
@@ -378,14 +453,19 @@ def apply_fixes(manifest: Dict[str, Any], results: List[EntryCheck], repo_root:
     # Top-level
     manifest["generated_at"] = utc_now_iso()
     manifest["total_entries"] = len(manifest.get("entries", []))
-    manifest["total_size_bytes"] = int(sum((e.get("size_bytes") or 0) for e in manifest.get("entries", [])))
+    manifest["total_size_bytes"] = int(
+        sum((e.get("size_bytes") or 0) for e in manifest.get("entries", []))
+    )
     manifest["entries_updated"] = updated
     return updated


 # ------------------------- Corrections alias de chemin -------------------------

-def apply_aliases_to_manifest_paths(manifest: Dict[str, Any], repo_root: Path, rules: Dict[str, Any]) -> int:
+
+def apply_aliases_to_manifest_paths(
+    manifest: Dict[str, Any], repo_root: Path, rules: Dict[str, Any]
+) -> int:
     """Remplace e['path'] via les alias, si le fichier existe au chemin normalisé."""
     updated = 0
     for idx, e in enumerate(manifest.get("entries", [])):
@@ -401,6 +481,7 @@ def apply_aliases_to_manifest_paths(manifest: Dict[str, Any], repo_root: Path, r

 # ------------------------- Contrôles de contenu (transverses) -------------------------

+
 def _content_issue(sev: str, code: str, msg: str, idx: int, path: str) -> Issue:
     return Issue(sev, code, msg, idx, path)

@@ -413,7 +494,9 @@ def _read_json(path: Path) -> Optional[Dict[str, Any]]:
         return None


-def _check_canonical_constants(json_obj: Dict[str, Any], rules: Dict[str, Any], idx: int, rel: str) -> List[Issue]:
+def _check_canonical_constants(
+    json_obj: Dict[str, Any], rules: Dict[str, Any], idx: int, rel: str
+) -> List[Issue]:
     issues: List[Issue] = []
     canon = rules.get("canonical_constants", {})
     toler = rules.get("tolerances", {})
@@ -422,38 +505,70 @@ def _check_canonical_constants(json_obj: Dict[str, Any], rules: Dict[str, Any],
     if H0 is not None and "H0" in canon:
         tol = toler.get("H0", {"abs": 0.5})
         if not within_tolerance(float(H0), float(canon["H0"]), tol):
-            issues.append(_content_issue("WARN", "CONST_MISMATCH",
-                                         f"H0={H0} deviates from canonical {canon['H0']} (tol={tol})", idx, rel))
+            issues.append(
+                _content_issue(
+                    "WARN",
+                    "CONST_MISMATCH",
+                    f"H0={H0} deviates from canonical {canon['H0']} (tol={tol})",
+                    idx,
+                    rel,
+                )
+            )
     # A_s0 / As0
     As = try_get(json_obj, ["A_s0", "As0", "primordial.A_s0"])
     if As is not None and "A_s0" in canon:
         tol = toler.get("A_s0", {"rel": 0.05})
         if not within_tolerance(float(As), float(canon["A_s0"]), tol):
-            issues.append(_content_issue("WARN", "CONST_MISMATCH",
-                                         f"A_s0={As} deviates from canonical {canon['A_s0']} (tol={tol})", idx, rel))
+            issues.append(
+                _content_issue(
+                    "WARN",
+                    "CONST_MISMATCH",
+                    f"A_s0={As} deviates from canonical {canon['A_s0']} (tol={tol})",
+                    idx,
+                    rel,
+                )
+            )
     # n_s0 / ns0
     ns = try_get(json_obj, ["n_s0", "ns0", "primordial.ns0"])
     if ns is not None and "ns0" in canon:
         tol = toler.get("ns0", {"abs": 0.01})
         if not within_tolerance(float(ns), float(canon["ns0"]), tol):
-            issues.append(_content_issue("WARN", "CONST_MISMATCH",
-                                         f"ns0={ns} deviates from canonical {canon['ns0']} (tol={tol})", idx, rel))
+            issues.append(
+                _content_issue(
+                    "WARN",
+                    "CONST_MISMATCH",
+                    f"ns0={ns} deviates from canonical {canon['ns0']} (tol={tol})",
+                    idx,
+                    rel,
+                )
+            )
     return issues


-def _check_thresholds(json_obj: Dict[str, Any], rules: Dict[str, Any], idx: int, rel: str) -> List[Issue]:
+def _check_thresholds(
+    json_obj: Dict[str, Any], rules: Dict[str, Any], idx: int, rel: str
+) -> List[Issue]:
     issues: List[Issue] = []
     want = rules.get("thresholds", {})
     have = try_get(json_obj, ["thresholds"])
     if isinstance(have, dict) and want:
         for k in ("primary", "order2"):
             if k in want and k in have and float(have[k]) != float(want[k]):
-                issues.append(_content_issue("WARN", "THRESHOLD_DIFFERS",
-                                             f"thresholds.{k}={have[k]} vs canonical {want[k]}", idx, rel))
+                issues.append(
+                    _content_issue(
+                        "WARN",
+                        "THRESHOLD_DIFFERS",
+                        f"thresholds.{k}={have[k]} vs canonical {want[k]}",
+                        idx,
+                        rel,
+                    )
+                )
     return issues


-def _check_derivative(json_obj: Dict[str, Any], rules: Dict[str, Any], idx: int, rel: str) -> List[Issue]:
+def _check_derivative(
+    json_obj: Dict[str, Any], rules: Dict[str, Any], idx: int, rel: str
+) -> List[Issue]:
     issues: List[Issue] = []
     want = rules.get("derivative", {})
     if not want:
@@ -461,15 +576,31 @@ def _check_derivative(json_obj: Dict[str, Any], rules: Dict[str, Any], idx: int,
     w = try_get(json_obj, ["derivative_window"])
     p = try_get(json_obj, ["derivative_polyorder"])
     if w is not None and "window" in want and int(w) != int(want["window"]):
-        issues.append(_content_issue("WARN", "DERIV_WINDOW_DIFFERS",
-                                     f"derivative_window={w} vs canonical {want['window']}", idx, rel))
+        issues.append(
+            _content_issue(
+                "WARN",
+                "DERIV_WINDOW_DIFFERS",
+                f"derivative_window={w} vs canonical {want['window']}",
+                idx,
+                rel,
+            )
+        )
     if p is not None and "polyorder" in want and int(p) != int(want["polyorder"]):
-        issues.append(_content_issue("WARN", "DERIV_POLYORDER_DIFFERS",
-                                     f"derivative_polyorder={p} vs canonical {want['polyorder']}", idx, rel))
+        issues.append(
+            _content_issue(
+                "WARN",
+                "DERIV_POLYORDER_DIFFERS",
+                f"derivative_polyorder={p} vs canonical {want['polyorder']}",
+                idx,
+                rel,
+            )
+        )
     return issues


-def _check_metrics_window(json_obj: Dict[str, Any], rules: Dict[str, Any], idx: int, rel: str) -> List[Issue]:
+def _check_metrics_window(
+    json_obj: Dict[str, Any], rules: Dict[str, Any], idx: int, rel: str
+) -> List[Issue]:
     issues: List[Issue] = []
     want = rules.get("metric_windows", {}).get("phase_20_300")
     have = try_get(json_obj, ["metrics_active.metrics_window_Hz", "metrics_window_Hz"])
@@ -477,13 +608,21 @@ def _check_metrics_window(json_obj: Dict[str, Any], rules: Dict[str, Any], idx:
         wtuple = (float(have[0]), float(have[1]))
         wwant = (float(want[0]), float(want[1]))
         if wtuple != wwant:
-            issues.append(_content_issue("WARN", "METRIC_WINDOW_DIFFERS",
-                                         f"metrics_window_Hz={wtuple} vs canonical {wwant}", idx, rel))
+            issues.append(
+                _content_issue(
+                    "WARN",
+                    "METRIC_WINDOW_DIFFERS",
+                    f"metrics_window_Hz={wtuple} vs canonical {wwant}",
+                    idx,
+                    rel,
+                )
+            )
     return issues


-def _check_csv_milestones(path: Path, rules: Dict[str, Any], idx: int, rel: str,
-                          fix_content: bool) -> List[Issue]:
+def _check_csv_milestones(
+    path: Path, rules: Dict[str, Any], idx: int, rel: str, fix_content: bool
+) -> List[Issue]:
     """
     Vérifie le CSV des jalons de comparaison (chapter09/09_comparison_milestones.csv).
     - classes autorisées/normalisation
@@ -506,13 +645,26 @@ def _check_csv_milestones(path: Path, rules: Dict[str, Any], idx: int, rel: str,
                 ev = row.get("event", "")
                 cl = row.get("classe", "")
                 if ev and not evrx.match(ev):
-                    issues.append(_content_issue("WARN", "EVENT_ID_PATTERN",
-                                                 f"event '{ev}' does not match regex {event_pat}", idx, rel))
+                    issues.append(
+                        _content_issue(
+                            "WARN",
+                            "EVENT_ID_PATTERN",
+                            f"event '{ev}' does not match regex {event_pat}",
+                            idx,
+                            rel,
+                        )
+                    )
                 if cl not in allowed:
                     norm = normalize_map.get(cl, cl)
-                    issues.append(_content_issue("WARN", "CLASS_LABEL_UNKNOWN",
-                                                 f"classe '{cl}' not in {sorted(allowed)}; normalize→'{norm}'",
-                                                 idx, rel))
+                    issues.append(
+                        _content_issue(
+                            "WARN",
+                            "CLASS_LABEL_UNKNOWN",
+                            f"classe '{cl}' not in {sorted(allowed)}; normalize→'{norm}'",
+                            idx,
+                            rel,
+                        )
+                    )
                     if fix_content and norm in allowed:
                         row["classe"] = norm
                         changed = True
@@ -520,30 +672,58 @@ def _check_csv_milestones(path: Path, rules: Dict[str, Any], idx: int, rel: str,
                 if sp is not None and sp != "":
                     try:
                         if float(sp) <= 0:
-                            issues.append(_content_issue("ERROR", "NEGATIVE_SIGMA",
-                                                         f"sigma_phase={sp} ≤ 0", idx, rel))
+                            issues.append(
+                                _content_issue(
+                                    "ERROR",
+                                    "NEGATIVE_SIGMA",
+                                    f"sigma_phase={sp} ≤ 0",
+                                    idx,
+                                    rel,
+                                )
+                            )
                     except ValueError:
-                        issues.append(_content_issue("ERROR", "NONNUMERIC_SIGMA",
-                                                     f"sigma_phase not numeric: {sp}", idx, rel))
+                        issues.append(
+                            _content_issue(
+                                "ERROR",
+                                "NONNUMERIC_SIGMA",
+                                f"sigma_phase not numeric: {sp}",
+                                idx,
+                                rel,
+                            )
+                        )
                 rows_out.append(row)

         if fix_content and changed and rows_out:
-            bak = path.with_suffix(path.suffix + f".{datetime.now(UTC).strftime('%Y%m%dT%H%M%SZ')}.bak")
+            bak = path.with_suffix(
+                path.suffix + f".{datetime.now(UTC).strftime('%Y%m%dT%H%M%SZ')}.bak"
+            )
             shutil.copy2(path, bak)
             with path.open("w", encoding="utf-8", newline="") as fw:
                 writer = csv.DictWriter(fw, fieldnames=rows_out[0].keys())
                 writer.writeheader()
                 writer.writerows(rows_out)
     except FileNotFoundError:
-        issues.append(_content_issue("ERROR", "FILE_MISSING", f"CSV not found: {rel}", idx, rel))
+        issues.append(
+            _content_issue("ERROR", "FILE_MISSING", f"CSV not found: {rel}", idx, rel)
+        )
     except Exception as e:
-        issues.append(_content_issue("ERROR", "CSV_READ_ERROR", f"{type(e).__name__}: {e}", idx, rel))
+        issues.append(
+            _content_issue(
+                "ERROR", "CSV_READ_ERROR", f"{type(e).__name__}: {e}", idx, rel
+            )
+        )

     return issues


-def run_content_checks(manifest: Dict[str, Any], repo_root: Path, rules: Dict[str, Any],
-                       do_checks: bool, fix_content: bool, apply_aliases_hint: bool) -> List[Issue]:
+def run_content_checks(
+    manifest: Dict[str, Any],
+    repo_root: Path,
+    rules: Dict[str, Any],
+    do_checks: bool,
+    fix_content: bool,
+    apply_aliases_hint: bool,
+) -> List[Issue]:
     if not do_checks or not rules:
         return []
     issues: List[Issue] = []
@@ -576,21 +756,33 @@ def run_content_checks(manifest: Dict[str, Any], repo_root: Path, rules: Dict[st
         alias_rel = normalize_path_aliases(rel, rules)
         if alias_rel != rel and (repo_root / alias_rel).exists():
             if not apply_aliases_hint:
-                issues.append(_content_issue("WARN", "PATH_ALIAS_CANDIDATE",
-                                             f"path '{rel}' → canonical '{alias_rel}' exists; consider --apply-aliases",
-                                             idx, rel))
-
-        full = (repo_root / rel)
+                issues.append(
+                    _content_issue(
+                        "WARN",
+                        "PATH_ALIAS_CANDIDATE",
+                        f"path '{rel}' → canonical '{alias_rel}' exists; consider --apply-aliases",
+                        idx,
+                        rel,
+                    )
+                )
+
+        full = repo_root / rel
         low = rel.replace("\\", "/")

         # JSON – constantes/seuils/dérivées/fenêtres métriques
-        if (low.endswith(json_cmb_suffix) or
-            low.endswith(json_primordial_suffix) or
-            low.endswith(metrics_phase_suffix) or
-            low.endswith(json_threshold_like)):
+        if (
+            low.endswith(json_cmb_suffix)
+            or low.endswith(json_primordial_suffix)
+            or low.endswith(metrics_phase_suffix)
+            or low.endswith(json_threshold_like)
+        ):
             jobj = _read_json(full)
             if jobj is None:
-                issues.append(_content_issue("ERROR", "JSON_READ_ERROR", "unable to parse JSON", idx, rel))
+                issues.append(
+                    _content_issue(
+                        "ERROR", "JSON_READ_ERROR", "unable to parse JSON", idx, rel
+                    )
+                )
             else:
                 issues += _check_canonical_constants(jobj, rules, idx, rel)
                 issues += _check_thresholds(jobj, rules, idx, rel)
@@ -609,6 +801,7 @@ def run_content_checks(manifest: Dict[str, Any], repo_root: Path, rules: Dict[st

 # ------------------------- Signature / empreintes -------------------------

+
 def write_sha256sum(path: Path) -> Path:
     h = sha256_of(path)
     out = path.with_suffix(path.suffix + ".sha256sum")
@@ -620,7 +813,10 @@ def gpg_sign(path: Path) -> Optional[Path]:
     try:
         res = subprocess.run(
             ["gpg", "--armor", "--detach-sign", str(path)],
-            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=False
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+            text=True,
+            check=False,
         )
         if res.returncode == 0:
             sig = path.with_suffix(path.suffix + ".asc")
@@ -635,32 +831,85 @@ def gpg_sign(path: Path) -> Optional[Path]:

 # ------------------------- CLI -------------------------

+
 def parse_args(argv: List[str]) -> argparse.Namespace:
-    p = argparse.ArgumentParser(description="Manifest audit + transverse consistency checks for MCGT.")
-    p.add_argument("manifest", help="Chemin du manifeste JSON (p.ex. zz-manifests/manifest_publication.json)")
+    p = argparse.ArgumentParser(
+        description="Manifest audit + transverse consistency checks for MCGT."
+    )
+    p.add_argument(
+        "manifest",
+        help="Chemin du manifeste JSON (p.ex. zz-manifests/manifest_publication.json)",
+    )
     p.add_argument("--repo-root", default=".", help="Racine du dépôt (défaut: .)")
-    p.add_argument("--rules", default="zz-schemas/consistency_rules.json", help="Fichier de règles de cohérence")
-    p.add_argument("--report", choices=["text", "json", "md"], default="text", help="Format du rapport")
-    p.add_argument("--fix", action="store_true", help="Applique les corrections techniques et réécrit le manifeste")
-    p.add_argument("--normalize-paths", action="store_true", help="Normalise les champs internes *_path du manifeste")
-    p.add_argument("--strip-internal", action="store_true", help="Supprime les clés internes commençant par '_'")
-    p.add_argument("--set-repo-root", action="store_true", help="Force repository_root='.' dans le manifeste")
-    p.add_argument("--fail-on", choices=["errors", "warnings", "none"], default="errors",
-                   help="Politique d'échec (code retour)")
-    p.add_argument("--sha256-out", action="store_true", help="Écrit un fichier .sha256sum du manifeste")
-    p.add_argument("--gpg-sign", action="store_true", help="Génère une signature GPG détachée (.sig)")
+    p.add_argument(
+        "--rules",
+        default="zz-schemas/consistency_rules.json",
+        help="Fichier de règles de cohérence",
+    )
+    p.add_argument(
+        "--report",
+        choices=["text", "json", "md"],
+        default="text",
+        help="Format du rapport",
+    )
+    p.add_argument(
+        "--fix",
+        action="store_true",
+        help="Applique les corrections techniques et réécrit le manifeste",
+    )
+    p.add_argument(
+        "--normalize-paths",
+        action="store_true",
+        help="Normalise les champs internes *_path du manifeste",
+    )
+    p.add_argument(
+        "--strip-internal",
+        action="store_true",
+        help="Supprime les clés internes commençant par '_'",
+    )
+    p.add_argument(
+        "--set-repo-root",
+        action="store_true",
+        help="Force repository_root='.' dans le manifeste",
+    )
+    p.add_argument(
+        "--fail-on",
+        choices=["errors", "warnings", "none"],
+        default="errors",
+        help="Politique d'échec (code retour)",
+    )
+    p.add_argument(
+        "--sha256-out",
+        action="store_true",
+        help="Écrit un fichier .sha256sum du manifeste",
+    )
+    p.add_argument(
+        "--gpg-sign",
+        action="store_true",
+        help="Génère une signature GPG détachée (.sig)",
+    )
     # Nouveaux contrôles / corrections
-    p.add_argument("--content-check", action="store_true",
-                   help="Active les contrôles de contenu via les règles de cohérence")
-    p.add_argument("--apply-aliases", action="store_true",
-                   help="Réécrit les chemins du manifeste selon les alias si la cible existe")
-    p.add_argument("--fix-content", action="store_true",
-                   help="Normalise en place les classes dans CSV jalons (backup .bak horodaté)")
+    p.add_argument(
+        "--content-check",
+        action="store_true",
+        help="Active les contrôles de contenu via les règles de cohérence",
+    )
+    p.add_argument(
+        "--apply-aliases",
+        action="store_true",
+        help="Réécrit les chemins du manifeste selon les alias si la cible existe",
+    )
+    p.add_argument(
+        "--fix-content",
+        action="store_true",
+        help="Normalise en place les classes dans CSV jalons (backup .bak horodaté)",
+    )
     return p.parse_args(argv)


 # ------------------------- main -------------------------

+
 def main(argv: List[str]) -> int:
     args = parse_args(argv)
     manifest_path = Path(args.manifest).resolve()
@@ -688,7 +937,9 @@ def main(argv: List[str]) -> int:
         updated = apply_aliases_to_manifest_paths(manifest, repo_root, rules)
         if updated:
             manifest.setdefault("notes", [])
-            manifest["notes"].append(f"apply_aliases: {updated} paths normalized at {utc_now_iso()}")
+            manifest["notes"].append(
+                f"apply_aliases: {updated} paths normalized at {utc_now_iso()}"
+            )

     # Per-entry technical checks
     checks: List[EntryCheck] = []
@@ -697,10 +948,12 @@ def main(argv: List[str]) -> int:

     # Transverse content checks
     content_issues: List[Issue] = run_content_checks(
-        manifest, repo_root, rules,
+        manifest,
+        repo_root,
+        rules,
         do_checks=(args.content_check or bool(rules)),
         fix_content=args.fix_content,
-        apply_aliases_hint=args.apply_aliases
+        apply_aliases_hint=args.apply_aliases,
     )

     # Build & print report
@@ -709,14 +962,22 @@ def main(argv: List[str]) -> int:

     # Apply technical fixes to manifest if requested
     if args.fix:
-        updated = apply_fixes(manifest, checks, repo_root,
-                              normalize_paths=args.normalize_paths,
-                              strip_internal=args.strip_internal)
+        updated = apply_fixes(
+            manifest,
+            checks,
+            repo_root,
+            normalize_paths=args.normalize_paths,
+            strip_internal=args.strip_internal,
+        )
         manifest.setdefault("manifest_tool_version", "diag_consistency.py")
-        manifest.setdefault("generated_by", os.getenv("USER") or os.getenv("USERNAME") or "unknown")
+        manifest.setdefault(
+            "generated_by", os.getenv("USER") or os.getenv("USERNAME") or "unknown"
+        )
         write_manifest_atomic(manifest_path, manifest)
-        print(f"\nWrote: {manifest_path}  (entries_updated={manifest.get('entries_updated')}, "
-              f"total_size_bytes={manifest.get('total_size_bytes')})")
+        print(
+            f"\nWrote: {manifest_path}  (entries_updated={manifest.get('entries_updated')}, "
+            f"total_size_bytes={manifest.get('total_size_bytes')})"
+        )

     # Optional outputs
     if args.sha256_out:
@@ -733,7 +994,9 @@ def main(argv: List[str]) -> int:
     code = 0
     if args.fail_on == "errors" and report["errors"] > 0:
         code = 3
-    elif args.fail_on == "warnings" and (report["errors"] > 0 or report["warnings"] > 0):
+    elif args.fail_on == "warnings" and (
+        report["errors"] > 0 or report["warnings"] > 0
+    ):
         code = 2
     else:
         code = 0
diff --git a/zz-manifests/manifest_master.json.20250917T161003Z.bak b/zz-manifests/manifest_master.json.20250917T161003Z.bak
index 21b53af..9337b52 100755
--- a/zz-manifests/manifest_master.json.20250917T161003Z.bak
+++ b/zz-manifests/manifest_master.json.20250917T161003Z.bak
@@ -11,4 +11,4 @@
     "website": "https://example.org/mcgt",
     "license": "MIT"
   }
-}
\ No newline at end of file
+}
diff --git a/zz-manifests/manifest_master.json.pre-alias.bak b/zz-manifests/manifest_master.json.pre-alias.bak
index 21b53af..9337b52 100755
--- a/zz-manifests/manifest_master.json.pre-alias.bak
+++ b/zz-manifests/manifest_master.json.pre-alias.bak
@@ -11,4 +11,4 @@
     "website": "https://example.org/mcgt",
     "license": "MIT"
   }
-}
\ No newline at end of file
+}
diff --git a/zz-manifests/manifest_publication.json.20250917T161013Z.bak b/zz-manifests/manifest_publication.json.20250917T161013Z.bak
index 14742a1..bcb5985 100755
--- a/zz-manifests/manifest_publication.json.20250917T161013Z.bak
+++ b/zz-manifests/manifest_publication.json.20250917T161013Z.bak
@@ -11,4 +11,4 @@
     "website": "https://example.org/mcgt",
     "license": "MIT"
   }
-}
\ No newline at end of file
+}
diff --git a/zz-manifests/manifest_publication.json.pre-alias.bak b/zz-manifests/manifest_publication.json.pre-alias.bak
index 14742a1..bcb5985 100755
--- a/zz-manifests/manifest_publication.json.pre-alias.bak
+++ b/zz-manifests/manifest_publication.json.pre-alias.bak
@@ -11,4 +11,4 @@
     "website": "https://example.org/mcgt",
     "license": "MIT"
   }
-}
\ No newline at end of file
+}
diff --git a/zz-manifests/meta_template.json.bak b/zz-manifests/meta_template.json.bak
index 9ee02fc..3822e5b 100755
--- a/zz-manifests/meta_template.json.bak
+++ b/zz-manifests/meta_template.json.bak
@@ -193,4 +193,3 @@
 }
 }
 }
-
diff --git a/zz-schemas/02_optimal_parameters.schema.json b/zz-schemas/02_optimal_parameters.schema.json
index 15b5000..5a60ad4 100755
--- a/zz-schemas/02_optimal_parameters.schema.json
+++ b/zz-schemas/02_optimal_parameters.schema.json
@@ -102,4 +102,4 @@
 }
 }
 ]
-}
\ No newline at end of file
+}
diff --git a/zz-schemas/02_optimal_parameters.schema.json.bak b/zz-schemas/02_optimal_parameters.schema.json.bak
index 4365cb9..83912de 100755
--- a/zz-schemas/02_optimal_parameters.schema.json.bak
+++ b/zz-schemas/02_optimal_parameters.schema.json.bak
@@ -102,4 +102,4 @@
 }
 }
 ]
-}
\ No newline at end of file
+}
diff --git a/zz-schemas/02_spec_spectrum.schema.json b/zz-schemas/02_spec_spectrum.schema.json
index d238c82..5790e6f 100755
--- a/zz-schemas/02_spec_spectrum.schema.json
+++ b/zz-schemas/02_spec_spectrum.schema.json
@@ -85,4 +85,4 @@
 "notes": "c1,c2 représentent des corrections linéaires; *_2 réservés pour ordre supérieur."
 }
 ]
-}
\ No newline at end of file
+}
diff --git a/zz-schemas/03_meta_stability_fR.schema.json b/zz-schemas/03_meta_stability_fR.schema.json
index f7e537a..a028ee5 100755
--- a/zz-schemas/03_meta_stability_fR.schema.json
+++ b/zz-schemas/03_meta_stability_fR.schema.json
@@ -105,4 +105,4 @@
 "notes": "m_s2_over_R0 clipped pour stabilité numérique à 333333 sur cette exécution."
 }
 ]
-}
\ No newline at end of file
+}
diff --git a/zz-schemas/05_nucleosynthesis_parameters.schema.json b/zz-schemas/05_nucleosynthesis_parameters.schema.json
index 4c33d4f..d452f74 100755
--- a/zz-schemas/05_nucleosynthesis_parameters.schema.json
+++ b/zz-schemas/05_nucleosynthesis_parameters.schema.json
@@ -92,4 +92,4 @@
 "notes": "Max epsilons choisis selon les règles de tolérance internes du chapitre 5."
 }
 ]
-}
\ No newline at end of file
+}
diff --git a/zz-schemas/06_cmb_params.schema.json b/zz-schemas/06_cmb_params.schema.json
index 3fdcf7d..8234ea0 100755
--- a/zz-schemas/06_cmb_params.schema.json
+++ b/zz-schemas/06_cmb_params.schema.json
@@ -136,4 +136,4 @@
 "notes": "Vérifier la cohérence ell_max/n_points avec la sortie CAMB."
 }
 ]
-}
\ No newline at end of file
+}
diff --git a/zz-schemas/07_meta_perturbations.schema.json b/zz-schemas/07_meta_perturbations.schema.json
index ba57f49..babe594 100755
--- a/zz-schemas/07_meta_perturbations.schema.json
+++ b/zz-schemas/07_meta_perturbations.schema.json
@@ -252,4 +252,4 @@
       "notes": "cs2 \\~ 1; vérifier n_nonfinite==0."
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/zz-schemas/07_params_perturbations.schema.json b/zz-schemas/07_params_perturbations.schema.json
index 871058d..b8e9b80 100755
--- a/zz-schemas/07_params_perturbations.schema.json
+++ b/zz-schemas/07_params_perturbations.schema.json
@@ -136,4 +136,4 @@
 "notes": "Ajuster la fenêtre SavGol si oscillations numériques."
 }
 ]
-}
\ No newline at end of file
+}
diff --git a/zz-schemas/08_coupling_params.schema.json b/zz-schemas/08_coupling_params.schema.json
index eb77ad5..afab786 100644
--- a/zz-schemas/08_coupling_params.schema.json
+++ b/zz-schemas/08_coupling_params.schema.json
@@ -51,4 +51,4 @@
     }
   },
   "description": "Note: la contrainte param2_max > param2_min n'est pas exprim\u00e9e ici (cross-field); elle reste \u00e0 la charge des scripts."
-}
\ No newline at end of file
+}
diff --git a/zz-schemas/09_best_params.schema.json b/zz-schemas/09_best_params.schema.json
index 5915ba6..0c4d57a 100755
--- a/zz-schemas/09_best_params.schema.json
+++ b/zz-schemas/09_best_params.schema.json
@@ -332,4 +332,4 @@
       ]
     }
   ]
-}
\ No newline at end of file
+}
diff --git a/zz-schemas/09_phases_imrphenom.meta.schema.json b/zz-schemas/09_phases_imrphenom.meta.schema.json
index 366599f..f16d9a1 100755
--- a/zz-schemas/09_phases_imrphenom.meta.schema.json
+++ b/zz-schemas/09_phases_imrphenom.meta.schema.json
@@ -129,4 +129,4 @@
 "notes": "Phases GR de référence pour comparaison avec MCGT."
 }
 ]
-}
\ No newline at end of file
+}
diff --git a/zz-schemas/README.md b/zz-schemas/README.md
index 50446d1..03d73be 100755
--- a/zz-schemas/README.md
+++ b/zz-schemas/README.md
@@ -27,6 +27,3 @@ les fichiers de résultats, manifests et métadonnées.
 ```bash

 python zz-schemas/validate\_json.py --all
-
-
-
diff --git a/zz-schemas/README_SCHEMAS.md b/zz-schemas/README_SCHEMAS.md
index c538e2c..3ac53cf 100755
--- a/zz-schemas/README_SCHEMAS.md
+++ b/zz-schemas/README_SCHEMAS.md
@@ -213,6 +213,3 @@ J. CONTACT ET JOURNAL DES MODIFICATIONS
 • Politique de mise à jour : incrémenter « schema\_version » dans validation\_globals.json lors de changements de règles canoniques ; en cas de modification de structure d’une instance JSON, incrémenter le « $id » du schéma correspondant et consigner le changement dans zz-manifests/manifest\_report.md.

 End of README\_SCHEMAS.md
-
-
-
diff --git a/zz-schemas/metrics_phase_schema.json b/zz-schemas/metrics_phase_schema.json
index c7ad488..61d3c73 100755
--- a/zz-schemas/metrics_phase_schema.json
+++ b/zz-schemas/metrics_phase_schema.json
@@ -458,4 +458,4 @@
       ]
     }
   }
-}
\ No newline at end of file
+}
diff --git a/zz-schemas/validate_csv_schema.py b/zz-schemas/validate_csv_schema.py
index b57d136..87da893 100755
--- a/zz-schemas/validate_csv_schema.py
+++ b/zz-schemas/validate_csv_schema.py
@@ -12,6 +12,7 @@ import json
 import sys
 import pandas as pd

+
 def validate_table(csv_path: str, schema_path: str) -> int:
     df = pd.read_csv(csv_path, encoding="utf-8")
     with open(schema_path, "r", encoding="utf-8") as f:
@@ -49,8 +50,11 @@ def validate_table(csv_path: str, schema_path: str) -> int:
         print("  EXTRA (not in schema):", extra)
     return 1 if missing else 0

+
 if __name__ == "__main__":
     if len(sys.argv) != 3:
-        print("Usage: python zz-schemas/validate_csv_schema.py <schema.json> <file.csv>")
+        print(
+            "Usage: python zz-schemas/validate_csv_schema.py <schema.json> <file.csv>"
+        )
         sys.exit(2)
     sys.exit(validate_table(sys.argv[2], sys.argv[1]))
diff --git a/zz-schemas/validate_csv_table.py b/zz-schemas/validate_csv_table.py
index 77ea789..73d1705 100755
--- a/zz-schemas/validate_csv_table.py
+++ b/zz-schemas/validate_csv_table.py
@@ -29,11 +29,13 @@ from typing import Any, Dict, List, Optional, Tuple

 _NULL_TOKENS = {"", "na", "nan", "none", "null", "n/a", "."}

+
 def is_null_token(s: Optional[str]) -> bool:
     if s is None:
         return True
     return s.strip().lower() in _NULL_TOKENS

+
 def parse_integer(s: str) -> Optional[int]:
     try:
         return int(s)
@@ -46,6 +48,7 @@ def parse_integer(s: str) -> Optional[int]:
             pass
     return None

+
 def parse_number(s: str) -> Optional[float]:
     try:
         f = float(s)
@@ -55,6 +58,7 @@ def parse_number(s: str) -> Optional[float]:
     except ValueError:
         return None

+
 def cast_value(s: Optional[str], typ: str) -> Tuple[bool, Any, str]:
     """
     Retourne (ok, value, err_msg)
@@ -83,10 +87,12 @@ def cast_value(s: Optional[str], typ: str) -> Tuple[bool, Any, str]:
     # fallback: treat as string
     return True, s, ""

+
 # ----------------------------
 # Validation principale
 # ----------------------------

+
 class CSVTableValidator:
     def __init__(self, schema: Dict[str, Any], data_path: str, max_errors: int = 50):
         self.schema = schema
@@ -118,7 +124,9 @@ class CSVTableValidator:
             self._fatal("Schema error: 'columns' doit être une liste non vide.")
         names = [c.get("name") for c in self.columns]
         if any(not isinstance(n, str) or not n for n in names):
-            self._fatal("Schema error: chaque colonne doit avoir un champ 'name' non vide.")
+            self._fatal(
+                "Schema error: chaque colonne doit avoir un champ 'name' non vide."
+            )
         if len(set(names)) != len(names):
             self._fatal("Schema error: noms de colonnes dupliqués.")
         for c in self.columns:
@@ -143,7 +151,9 @@ class CSVTableValidator:
                     # ignorer complètement les lignes vides
                     if all((v is None or str(v).strip() == "") for v in row.values()):
                         continue
-                    rows.append({k: (v if v is not None else "") for k, v in row.items()})
+                    rows.append(
+                        {k: (v if v is not None else "") for k, v in row.items()}
+                    )
             else:
                 reader = csv.reader(f, delimiter=self.delimiter)
                 headers = [c["name"] for c in self.columns]
@@ -189,13 +199,17 @@ class CSVTableValidator:
             # Présence de colonne
             if raw is None:
                 if required:
-                    self.add_error(f"[row {rownum}] column '{name}': missing required column")
+                    self.add_error(
+                        f"[row {rownum}] column '{name}': missing required column"
+                    )
                 continue

             # Null handling
             if is_null_token(raw):
                 if not nullable:
-                    self.add_error(f"[row {rownum}] column '{name}': null/empty not allowed")
+                    self.add_error(
+                        f"[row {rownum}] column '{name}': null/empty not allowed"
+                    )
                 typed[name] = None
                 continue

@@ -208,19 +222,27 @@ class CSVTableValidator:
             # Enum
             if enum is not None:
                 if val not in enum:
-                    self.add_error(f"[row {rownum}] column '{name}': value {val!r} not in enum {enum}")
+                    self.add_error(
+                        f"[row {rownum}] column '{name}': value {val!r} not in enum {enum}"
+                    )

             # Pattern (pour strings)
             if pattern and isinstance(val, str):
                 if not re.fullmatch(pattern, val):
-                    self.add_error(f"[row {rownum}] column '{name}': value {val!r} does not match pattern {pattern!r}")
+                    self.add_error(
+                        f"[row {rownum}] column '{name}': value {val!r} does not match pattern {pattern!r}"
+                    )

             # Min/Max (pour nombres/entiers)
             if isinstance(val, (int, float)):
                 if minv is not None and val < minv:
-                    self.add_error(f"[row {rownum}] column '{name}': value {val} < min {minv}")
+                    self.add_error(
+                        f"[row {rownum}] column '{name}': value {val} < min {minv}"
+                    )
                 if maxv is not None and val > maxv:
-                    self.add_error(f"[row {rownum}] column '{name}': value {val} > max {maxv}")
+                    self.add_error(
+                        f"[row {rownum}] column '{name}': value {val} > max {maxv}"
+                    )

             typed[name] = val
         return typed
@@ -237,11 +259,15 @@ class CSVTableValidator:
             key_vals.append(v)
         tup = tuple(key_vals)
         if tup in self.pk_seen:
-            self.add_error(f"[row {rownum}] duplicate primary_key {self.primary_key}={tup}")
+            self.add_error(
+                f"[row {rownum}] duplicate primary_key {self.primary_key}={tup}"
+            )
         else:
             self.pk_seen.add(tup)

-    def _get_value_for_constraint(self, typed: Dict[str, Any], spec: Dict[str, Any]) -> Tuple[bool, Any]:
+    def _get_value_for_constraint(
+        self, typed: Dict[str, Any], spec: Dict[str, Any]
+    ) -> Tuple[bool, Any]:
         """
         Retourne (present, value) où value peut venir de:
           - spec["left"]/spec["right"] (colonne)
@@ -263,30 +289,52 @@ class CSVTableValidator:
             ctype = c.get("type")
             if ctype == "compare":
                 left_present, left_val = self._get_value_for_constraint(
-                    typed, {"column": c["left"]} if "left" in c else {"left_value": c.get("left_value")}
+                    typed,
+                    {"column": c["left"]}
+                    if "left" in c
+                    else {"left_value": c.get("left_value")},
                 )
                 if "right" in c:
-                    right_present, right_val = self._get_value_for_constraint(typed, {"column": c["right"]})
+                    right_present, right_val = self._get_value_for_constraint(
+                        typed, {"column": c["right"]}
+                    )
                 else:
-                    right_present, right_val = self._get_value_for_constraint(typed, {"right_value": c.get("right_value")})
+                    right_present, right_val = self._get_value_for_constraint(
+                        typed, {"right_value": c.get("right_value")}
+                    )

-                if not left_present or left_val is None or not right_present or right_val is None:
+                if (
+                    not left_present
+                    or left_val is None
+                    or not right_present
+                    or right_val is None
+                ):
                     continue

                 op = c.get("op")
                 ok = True
                 try:
-                    if   op == "<":  ok = left_val <  right_val
-                    elif op == "<=": ok = left_val <= right_val
-                    elif op == ">":  ok = left_val >  right_val
-                    elif op == ">=": ok = left_val >= right_val
-                    elif op == "==": ok = left_val == right_val
-                    elif op == "!=": ok = left_val != right_val
+                    if op == "<":
+                        ok = left_val < right_val
+                    elif op == "<=":
+                        ok = left_val <= right_val
+                    elif op == ">":
+                        ok = left_val > right_val
+                    elif op == ">=":
+                        ok = left_val >= right_val
+                    elif op == "==":
+                        ok = left_val == right_val
+                    elif op == "!=":
+                        ok = left_val != right_val
                     else:
-                        self.add_error(f"[row {rownum}] constraint 'compare': unknown operator {op!r}")
+                        self.add_error(
+                            f"[row {rownum}] constraint 'compare': unknown operator {op!r}"
+                        )
                         continue
                 except Exception as e:
-                    self.add_error(f"[row {rownum}] constraint 'compare' failed to evaluate: {e}")
+                    self.add_error(
+                        f"[row {rownum}] constraint 'compare' failed to evaluate: {e}"
+                    )
                     continue

                 if not ok:
@@ -307,29 +355,37 @@ class CSVTableValidator:
                 if_match = False
                 if if_present:
                     if if_equals is None:
-                        if_match = (if_val is not None)
+                        if_match = if_val is not None
                     else:
-                        if_match = (if_val == if_equals)
+                        if_match = if_val == if_equals

                 if not if_match:
                     continue

                 then_col = cond_then.get("column")
                 if then_col is None:
-                    self.add_error(f"[row {rownum}] constraint 'implies' missing 'then.column'")
+                    self.add_error(
+                        f"[row {rownum}] constraint 'implies' missing 'then.column'"
+                    )
                     continue

                 then_val = typed.get(then_col)

                 if cond_then.get("is_null", False):
                     if then_val is not None:
-                        self.add_error(f"[row {rownum}] implies failed: expected '{then_col}' to be null")
+                        self.add_error(
+                            f"[row {rownum}] implies failed: expected '{then_col}' to be null"
+                        )
                 if cond_then.get("not_null", False):
                     if then_val is None:
-                        self.add_error(f"[row {rownum}] implies failed: expected '{then_col}' to be not null")
+                        self.add_error(
+                            f"[row {rownum}] implies failed: expected '{then_col}' to be not null"
+                        )
                 if "equals" in cond_then:
                     if then_val != cond_then["equals"]:
-                        self.add_error(f"[row {rownum}] implies failed: expected '{then_col}' == {cond_then['equals']!r}, got {then_val!r}")
+                        self.add_error(
+                            f"[row {rownum}] implies failed: expected '{then_col}' == {cond_then['equals']!r}, got {then_val!r}"
+                        )

             else:
                 self.add_error(f"[row {rownum}] unknown constraint type: {ctype!r}")
@@ -346,7 +402,10 @@ class CSVTableValidator:
                 break

         if self.errors:
-            print(f"Found {len(self.errors)} error(s) (showing up to {self.max_errors}):", file=sys.stderr)
+            print(
+                f"Found {len(self.errors)} error(s) (showing up to {self.max_errors}):",
+                file=sys.stderr,
+            )
             for e in self.errors:
                 print(f"  - {e}", file=sys.stderr)
             return 1
@@ -354,11 +413,14 @@ class CSVTableValidator:
         print(f"OK: {self.data_path} matches {self.schema.get('$id', '<schema>')}")
         return 0

+
 def main(argv: List[str]) -> int:
     ap = argparse.ArgumentParser(description="Validate CSV against a csv-table schema.")
     ap.add_argument("schema", help="Path to schema JSON")
     ap.add_argument("csv", help="Path to CSV to validate")
-    ap.add_argument("--max-errors", type=int, default=50, help="Max errors to display (default: 50)")
+    ap.add_argument(
+        "--max-errors", type=int, default=50, help="Max errors to display (default: 50)"
+    )
     args = ap.parse_args(argv)

     try:
@@ -371,12 +433,15 @@ def main(argv: List[str]) -> int:
         print(f"Invalid JSON schema ({args.schema}): {e}", file=sys.stderr)
         return 2

-    validator = CSVTableValidator(schema=schema, data_path=args.csv, max_errors=args.max_errors)
+    validator = CSVTableValidator(
+        schema=schema, data_path=args.csv, max_errors=args.max_errors
+    )
     try:
         return validator.validate()
     except FileNotFoundError:
         print(f"CSV file not found: {args.csv}", file=sys.stderr)
         return 2

+
 if __name__ == "__main__":
     sys.exit(main(sys.argv[1:]))
diff --git a/zz-schemas/validate_json.py b/zz-schemas/validate_json.py
index 88ce8fd..54c462a 100755
--- a/zz-schemas/validate_json.py
+++ b/zz-schemas/validate_json.py
@@ -4,6 +4,7 @@ import json
 import sys
 from jsonschema import Draft202012Validator

+
 def validate_instance(schema_path: str, instance_path: str) -> int:
     with open(schema_path, "r", encoding="utf-8") as f:
         schema = json.load(f)
@@ -11,7 +12,9 @@ def validate_instance(schema_path: str, instance_path: str) -> int:
         inst = json.load(f)

     validator = Draft202012Validator(schema)
-    errors = sorted(validator.iter_errors(inst), key=lambda e: (list(e.path), e.message))
+    errors = sorted(
+        validator.iter_errors(inst), key=lambda e: (list(e.path), e.message)
+    )

     if not errors:
         print(f"OK: {instance_path} matches {schema_path}")
@@ -26,6 +29,7 @@ def validate_instance(schema_path: str, instance_path: str) -> int:
         print(f" - at {path}{ctx}: {e.message}")
     return 1

+
 if __name__ == "__main__":
     if len(sys.argv) != 3:
         print("Usage: python zz-schemas/validate_json.py <schema.json> <instance.json>")
diff --git a/zz-scripts/chapter01/generate_data_chapter01.py b/zz-scripts/chapter01/generate_data_chapter01.py
index 487d5ef..7eff830 100755
--- a/zz-scripts/chapter01/generate_data_chapter01.py
+++ b/zz-scripts/chapter01/generate_data_chapter01.py
@@ -8,6 +8,7 @@ Pipeline Chapitre 1 - génération des données
 - Lissage Savitzky–Golay pour P_opt et dérivée initiale
 - Export complet des CSV/DAT
 """
+
 import argparse
 import numpy as np
 import pandas as pd
@@ -16,27 +17,30 @@ from scipy.interpolate import PchipInterpolator, interp1d
 from scipy.signal import savgol_filter
 from math import log10

+
 def read_jalons(path):
     df = pd.read_csv(path)
-    if 'T_i' in df.columns and 'T' not in df.columns:
-        df = df.rename(columns={'T_i':'T'})
-    if 'Pref' in df.columns and 'P_ref' not in df.columns:
-        df = df.rename(columns={'Pref':'P_ref'})
-    if not set(['T','P_ref']).issubset(df.columns):
-        df = pd.read_csv(path, header=None, names=['T','P_ref'])
-    df['T'] = pd.to_numeric(df['T'], errors='coerce')
-    df['P_ref'] = pd.to_numeric(df['P_ref'], errors='coerce')
-    df = df.dropna().sort_values('T')
-    return df['T'].values, df['P_ref'].values
+    if "T_i" in df.columns and "T" not in df.columns:
+        df = df.rename(columns={"T_i": "T"})
+    if "Pref" in df.columns and "P_ref" not in df.columns:
+        df = df.rename(columns={"Pref": "P_ref"})
+    if not set(["T", "P_ref"]).issubset(df.columns):
+        df = pd.read_csv(path, header=None, names=["T", "P_ref"])
+    df["T"] = pd.to_numeric(df["T"], errors="coerce")
+    df["P_ref"] = pd.to_numeric(df["P_ref"], errors="coerce")
+    df = df.dropna().sort_values("T")
+    return df["T"].values, df["P_ref"].values
+

 def build_grid(tmin, tmax, step, spacing):
-    if spacing=='log':
-        n = int((log10(tmax)-log10(tmin))/step)+1
-        return 10**np.linspace(log10(tmin), log10(tmax), n)
+    if spacing == "log":
+        n = int((log10(tmax) - log10(tmin)) / step) + 1
+        return 10 ** np.linspace(log10(tmin), log10(tmax), n)
     else:
-        n = int((tmax-tmin)/step)+1
+        n = int((tmax - tmin) / step) + 1
         return np.linspace(tmin, tmax, n)

+
 def compute_p(T_j, P_j, T_grid):
     logT = np.log10(T_j)
     logP = np.log10(P_j)
@@ -47,17 +51,23 @@ def compute_p(T_j, P_j, T_grid):
     dPg = np.gradient(Pg, T_grid)
     return Pg, dPg

+
 def main():
-    parser = argparse.ArgumentParser(description='Chap1 data gen')
-    parser.add_argument('--csv', type=pathlib.Path,
-                        default=pathlib.Path(__file__).resolve().parents[2]/
-                                'zz-data'/'chapter01'/'01_timeline_milestones.csv')
-    parser.add_argument('--tmin', type=float, default=1e-6)
-    parser.add_argument('--tmax', type=float, default=14.0)
-    parser.add_argument('--step', type=float, default=0.01)
-    parser.add_argument('--grid', choices=['log','lin'], default='log')
-    parser.add_argument('--window', type=int, default=21)
-    parser.add_argument('--poly', type=int, default=3)
+    parser = argparse.ArgumentParser(description="Chap1 data gen")
+    parser.add_argument(
+        "--csv",
+        type=pathlib.Path,
+        default=pathlib.Path(__file__).resolve().parents[2]
+        / "zz-data"
+        / "chapter01"
+        / "01_timeline_milestones.csv",
+    )
+    parser.add_argument("--tmin", type=float, default=1e-6)
+    parser.add_argument("--tmax", type=float, default=14.0)
+    parser.add_argument("--step", type=float, default=0.01)
+    parser.add_argument("--grid", choices=["log", "lin"], default="log")
+    parser.add_argument("--window", type=int, default=21)
+    parser.add_argument("--poly", type=int, default=3)
     args = parser.parse_args()

     base = args.csv.parent
@@ -65,11 +75,13 @@ def main():
     T_j, P_ref = read_jalons(args.csv)

     # Lecture P_init et dérivée initiale lissée
-    init_dat = np.loadtxt(base/'01_initial_grid_data.dat')
-    T_init, P_init = init_dat[:,0], init_dat[:,1]
+    init_dat = np.loadtxt(base / "01_initial_grid_data.dat")
+    T_init, P_init = init_dat[:, 0], init_dat[:, 1]
     dP_raw = np.gradient(P_init, T_init)
     dP_init = savgol_filter(dP_raw, window_length=args.window, polyorder=args.poly)
-    pd.DataFrame({'T': T_init, 'dP_dT': dP_init})       .to_csv(base/'01_P_derivative_initial.csv', index=False)
+    pd.DataFrame({"T": T_init, "dP_dT": dP_init}).to_csv(
+        base / "01_P_derivative_initial.csv", index=False
+    )

     # Grille et interpolation optimisée
     T_grid = build_grid(args.tmin, args.tmax, args.step, args.grid)
@@ -77,18 +89,29 @@ def main():
     dP_opt = savgol_filter(dP_opt_raw, window_length=args.window, polyorder=args.poly)

     # Exports
-    pd.DataFrame({'T': T_grid, 'P_calc': P_opt})       .to_csv(base/'01_optimized_data.csv', index=False)
-    pd.DataFrame({'T': T_grid, 'dP_dT': dP_opt})       .to_csv(base/'01_P_derivative_optimized.csv', index=False)
-    pd.DataFrame({'T': T_grid, 'P_calc': P_opt, 'dP_dT': dP_opt})       .to_csv(base/'01_optimized_data_and_derivatives.csv', index=False)
+    pd.DataFrame({"T": T_grid, "P_calc": P_opt}).to_csv(
+        base / "01_optimized_data.csv", index=False
+    )
+    pd.DataFrame({"T": T_grid, "dP_dT": dP_opt}).to_csv(
+        base / "01_P_derivative_optimized.csv", index=False
+    )
+    pd.DataFrame({"T": T_grid, "P_calc": P_opt, "dP_dT": dP_opt}).to_csv(
+        base / "01_optimized_data_and_derivatives.csv", index=False
+    )

     # Écarts relatifs
-    eps = (interp1d(T_grid, P_opt, fill_value='extrapolate')(T_j) - P_ref) / P_ref
-    pd.DataFrame({'T': T_j, 'epsilon': eps})       .to_csv(base/'01_relative_error_timeline.csv', index=False)
+    eps = (interp1d(T_grid, P_opt, fill_value="extrapolate")(T_j) - P_ref) / P_ref
+    pd.DataFrame({"T": T_j, "epsilon": eps}).to_csv(
+        base / "01_relative_error_timeline.csv", index=False
+    )

     # Invariants
-    pd.DataFrame({'T': T_grid, 'I1': P_opt / T_grid})       .to_csv(base/'01_dimensionless_invariants.csv', index=False)
+    pd.DataFrame({"T": T_grid, "I1": P_opt / T_grid}).to_csv(
+        base / "01_dimensionless_invariants.csv", index=False
+    )

     print("Chap1 data regenerated.")

-if __name__=='__main__':
+
+if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter01/plot_fig01_early_plateau.py b/zz-scripts/chapter01/plot_fig01_early_plateau.py
index 565a32e..f4cbac9 100755
--- a/zz-scripts/chapter01/plot_fig01_early_plateau.py
+++ b/zz-scripts/chapter01/plot_fig01_early_plateau.py
@@ -3,35 +3,46 @@ import matplotlib.pyplot as plt
 import pathlib

 # Lire la grille complète
-data_path = pathlib.Path(__file__).resolve().parents[2] / 'zz-data' / 'chapter01' / '01_optimized_data.csv'
+data_path = (
+    pathlib.Path(__file__).resolve().parents[2]
+    / "zz-data"
+    / "chapter01"
+    / "01_optimized_data.csv"
+)
 df = pd.read_csv(data_path)

 # Ne conserver que le plateau précoce T <= Tp
 Tp = 0.087
-df_plateau = df[df['T'] <= Tp]
+df_plateau = df[df["T"] <= Tp]

-T = df_plateau['T']
-P = df_plateau['P_calc']
+T = df_plateau["T"]
+P = df_plateau["P_calc"]

 # Tracé continu de P(T) sur le plateau
 plt.figure(figsize=(8, 4.5))
-plt.plot(T, P, color='orange', linewidth=1.5, label='P(T) optimisé')
+plt.plot(T, P, color="orange", linewidth=1.5, label="P(T) optimisé")

 # Ligne verticale renforcée à Tp
-plt.axvline(Tp, linestyle='--', color='black', linewidth=1.2,
-            label=r'$T_p=0.087\,\mathrm{Gyr}$')
+plt.axvline(
+    Tp, linestyle="--", color="black", linewidth=1.2, label=r"$T_p=0.087\,\mathrm{Gyr}$"
+)

 # Mise en forme
-plt.xscale('log')
-plt.xlabel('T (Gyr)')
-plt.ylabel('P(T)')
-plt.title('Plateau précoce de P(T)')
+plt.xscale("log")
+plt.xlabel("T (Gyr)")
+plt.ylabel("P(T)")
+plt.title("Plateau précoce de P(T)")
 plt.ylim(0.98, 1.002)
-plt.xlim(df_plateau['T'].min(), Tp*1.05)
-plt.grid(True, which='both', linestyle=':', linewidth=0.5)
-plt.legend(loc='lower right')
+plt.xlim(df_plateau["T"].min(), Tp * 1.05)
+plt.grid(True, which="both", linestyle=":", linewidth=0.5)
+plt.legend(loc="lower right")
 plt.tight_layout()

 # Sauvegarde
-output_path = pathlib.Path(__file__).resolve().parents[2] / 'zz-figures' / 'chapter01' / 'fig_01_early_plateau.png'
+output_path = (
+    pathlib.Path(__file__).resolve().parents[2]
+    / "zz-figures"
+    / "chapter01"
+    / "fig_01_early_plateau.png"
+)
 plt.savefig(output_path, dpi=300)
diff --git a/zz-scripts/chapter01/plot_fig02_logistic_calibration.py b/zz-scripts/chapter01/plot_fig02_logistic_calibration.py
index 1995449..f3ce484 100755
--- a/zz-scripts/chapter01/plot_fig02_logistic_calibration.py
+++ b/zz-scripts/chapter01/plot_fig02_logistic_calibration.py
@@ -1,5 +1,6 @@
 #!/usr/bin/env python3
 """Fig. 02 – Diagramme de calibration P_ref vs P_calc"""
+
 import pandas as pd
 import matplotlib.pyplot as plt
 from scipy.interpolate import interp1d
@@ -7,26 +8,26 @@ from pathlib import Path

 # Configuration des chemins
 base = Path(__file__).resolve().parents[2]
-data_ref = base / 'zz-data' / 'chapter01' / '01_timeline_milestones.csv'
-data_opt = base / 'zz-data' / 'chapter01' / '01_optimized_data.csv'
-output_file = base / 'zz-figures' / 'chapter01' / 'fig_02_logistic_calibration.png'
+data_ref = base / "zz-data" / "chapter01" / "01_timeline_milestones.csv"
+data_opt = base / "zz-data" / "chapter01" / "01_optimized_data.csv"
+output_file = base / "zz-figures" / "chapter01" / "fig_02_logistic_calibration.png"

 # Lecture des données
 df_ref = pd.read_csv(data_ref)
 df_opt = pd.read_csv(data_opt)
-interp = interp1d(df_opt['T'], df_opt['P_calc'], fill_value='extrapolate')
-P_calc_ref = interp(df_ref['T'])
+interp = interp1d(df_opt["T"], df_opt["P_calc"], fill_value="extrapolate")
+P_calc_ref = interp(df_ref["T"])

 # Tracé de la figure
 plt.figure(dpi=300)
-plt.loglog(df_ref['P_ref'], P_calc_ref, 'o', label='Données calibration')
-minv = min(df_ref['P_ref'].min(), P_calc_ref.min())
-maxv = max(df_ref['P_ref'].max(), P_calc_ref.max())
-plt.plot([minv, maxv], [minv, maxv], '--', label='Identité (y = x)')
-plt.xlabel(r'$P_{\mathrm{ref}}$')
-plt.ylabel(r'$P_{\mathrm{calc}}$')
-plt.title('Fig. 02 – Calibration log–log')
-plt.grid(True, which='both', ls=':', lw=0.5)
+plt.loglog(df_ref["P_ref"], P_calc_ref, "o", label="Données calibration")
+minv = min(df_ref["P_ref"].min(), P_calc_ref.min())
+maxv = max(df_ref["P_ref"].max(), P_calc_ref.max())
+plt.plot([minv, maxv], [minv, maxv], "--", label="Identité (y = x)")
+plt.xlabel(r"$P_{\mathrm{ref}}$")
+plt.ylabel(r"$P_{\mathrm{calc}}$")
+plt.title("Fig. 02 – Calibration log–log")
+plt.grid(True, which="both", ls=":", lw=0.5)
 plt.legend()
 plt.tight_layout()
 plt.savefig(output_file)
diff --git a/zz-scripts/chapter01/plot_fig03_relative_error_timeline.py b/zz-scripts/chapter01/plot_fig03_relative_error_timeline.py
index b7ecb38..0954fc5 100755
--- a/zz-scripts/chapter01/plot_fig03_relative_error_timeline.py
+++ b/zz-scripts/chapter01/plot_fig03_relative_error_timeline.py
@@ -1,28 +1,29 @@
 #!/usr/bin/env python3
 """Fig. 03 – Écarts relatifs ε_i"""
+
 import pandas as pd
 import matplotlib.pyplot as plt
 from pathlib import Path

 base = Path(__file__).resolve().parents[2]
-data_file = base / 'zz-data' / 'chapter01' / '01_relative_error_timeline.csv'
-output_file = base / 'zz-figures' / 'chapter01' / 'fig_03_relative_error_timeline.png'
+data_file = base / "zz-data" / "chapter01" / "01_relative_error_timeline.csv"
+output_file = base / "zz-figures" / "chapter01" / "fig_03_relative_error_timeline.png"

 df = pd.read_csv(data_file)
-T = df['T']
-eps = df['epsilon']
+T = df["T"]
+eps = df["epsilon"]

 plt.figure(dpi=300)
-plt.plot(T, eps, 'o', color='orange', label='ε_i')
-plt.xscale('log')
-plt.yscale('symlog', linthresh=1e-4)
+plt.plot(T, eps, "o", color="orange", label="ε_i")
+plt.xscale("log")
+plt.yscale("symlog", linthresh=1e-4)
 # Seuil ±1 %
-plt.axhline(0.01, linestyle='--', color='grey', linewidth=1, label='Seuil ±1 %')
-plt.axhline(-0.01, linestyle='--', color='grey', linewidth=1)
-plt.xlabel('T (Gyr)')
-plt.ylabel('ε (écart relatif)')
-plt.title('Fig. 03 – Écarts relatifs (échelle symlog)')
-plt.grid(True, which='both', linestyle=':', linewidth=0.5)
+plt.axhline(0.01, linestyle="--", color="grey", linewidth=1, label="Seuil ±1 %")
+plt.axhline(-0.01, linestyle="--", color="grey", linewidth=1)
+plt.xlabel("T (Gyr)")
+plt.ylabel("ε (écart relatif)")
+plt.title("Fig. 03 – Écarts relatifs (échelle symlog)")
+plt.grid(True, which="both", linestyle=":", linewidth=0.5)
 plt.legend()
 plt.tight_layout()
 plt.savefig(output_file)
diff --git a/zz-scripts/chapter01/plot_fig04_P_vs_T_evolution.py b/zz-scripts/chapter01/plot_fig04_P_vs_T_evolution.py
index ddcd629..d5b34de 100755
--- a/zz-scripts/chapter01/plot_fig04_P_vs_T_evolution.py
+++ b/zz-scripts/chapter01/plot_fig04_P_vs_T_evolution.py
@@ -1,33 +1,34 @@
 #!/usr/bin/env python3
 """Fig. 04 – Évolution de P(T) : initial vs optimisé"""
+
 import pandas as pd
 import matplotlib.pyplot as plt
 from pathlib import Path

 # Configuration des chemins
 base = Path(__file__).resolve().parents[2]
-init_csv = base / 'zz-data' / 'chapter01' / '01_initial_grid_data.dat'
-opt_csv  = base / 'zz-data' / 'chapter01' / '01_optimized_data_and_derivatives.csv'
-output_file = base / 'zz-figures' / 'chapter01' / 'fig_04_P_vs_T_evolution.png'
+init_csv = base / "zz-data" / "chapter01" / "01_initial_grid_data.dat"
+opt_csv = base / "zz-data" / "chapter01" / "01_optimized_data_and_derivatives.csv"
+output_file = base / "zz-figures" / "chapter01" / "fig_04_P_vs_T_evolution.png"

 # Lecture des données
 df_init = pd.read_csv(init_csv)
-df_opt  = pd.read_csv(opt_csv)
-T_init  = df_init['T']
-P_init  = df_init['P']
-T_opt   = df_opt['T']
-P_opt   = df_opt['P']
+df_opt = pd.read_csv(opt_csv)
+T_init = df_init["T"]
+P_init = df_init["P"]
+T_opt = df_opt["T"]
+P_opt = df_opt["P"]

 # Tracé de la figure
 plt.figure(dpi=300)
-plt.plot(T_init, P_init, '--', color='grey', label=r'$P_{\rm init}(T)$')
-plt.plot(T_opt,  P_opt,  '-',  color='orange', label=r'$P_{\rm opt}(T)$')
-plt.xscale('log')
-plt.yscale('linear')
-plt.xlabel('T (Gyr)')
-plt.ylabel('P(T)')
-plt.title('Fig. 04 – Évolution de P(T) : initial vs optimisé')
-plt.grid(True, which='both', linestyle=':', linewidth=0.5)
+plt.plot(T_init, P_init, "--", color="grey", label=r"$P_{\rm init}(T)$")
+plt.plot(T_opt, P_opt, "-", color="orange", label=r"$P_{\rm opt}(T)$")
+plt.xscale("log")
+plt.yscale("linear")
+plt.xlabel("T (Gyr)")
+plt.ylabel("P(T)")
+plt.title("Fig. 04 – Évolution de P(T) : initial vs optimisé")
+plt.grid(True, which="both", linestyle=":", linewidth=0.5)
 plt.legend()
 plt.tight_layout()
 plt.savefig(output_file)
diff --git a/zz-scripts/chapter01/plot_fig05_I1_vs_T.py b/zz-scripts/chapter01/plot_fig05_I1_vs_T.py
index c298867..4631894 100755
--- a/zz-scripts/chapter01/plot_fig05_I1_vs_T.py
+++ b/zz-scripts/chapter01/plot_fig05_I1_vs_T.py
@@ -1,25 +1,26 @@
 #!/usr/bin/env python3
 """Fig. 05 – Invariant adimensionnel I1(T)"""
+
 import pandas as pd
 import matplotlib.pyplot as plt
 from pathlib import Path

 base = Path(__file__).resolve().parents[2]
-data_file = base / 'zz-data' / 'chapter01' / '01_dimensionless_invariants.csv'
-output_file = base / 'zz-figures' / 'chapter01' / 'fig_05_I1_vs_T.png'
+data_file = base / "zz-data" / "chapter01" / "01_dimensionless_invariants.csv"
+output_file = base / "zz-figures" / "chapter01" / "fig_05_I1_vs_T.png"

 df = pd.read_csv(data_file)
-T = df['T']
-I1 = df['I1']
+T = df["T"]
+I1 = df["I1"]

 plt.figure(dpi=300)
-plt.plot(T, I1, color='orange', label=r'$I_1 = P(T)/T$')
-plt.xscale('log')
-plt.yscale('log')
-plt.xlabel('T (Gyr)')
-plt.ylabel(r'$I_1$')
-plt.title('Fig. 05 – Invariant adimensionnel $I_1$ en fonction de $T$')
-plt.grid(True, which='both', ls=':', lw=0.5)
+plt.plot(T, I1, color="orange", label=r"$I_1 = P(T)/T$")
+plt.xscale("log")
+plt.yscale("log")
+plt.xlabel("T (Gyr)")
+plt.ylabel(r"$I_1$")
+plt.title("Fig. 05 – Invariant adimensionnel $I_1$ en fonction de $T$")
+plt.grid(True, which="both", ls=":", lw=0.5)
 plt.legend()
 plt.tight_layout()
 plt.savefig(output_file)
diff --git a/zz-scripts/chapter01/plot_fig06_P_derivative_comparison.py b/zz-scripts/chapter01/plot_fig06_P_derivative_comparison.py
index 51fb870..bada859 100755
--- a/zz-scripts/chapter01/plot_fig06_P_derivative_comparison.py
+++ b/zz-scripts/chapter01/plot_fig06_P_derivative_comparison.py
@@ -4,23 +4,28 @@ import pandas as pd
 import matplotlib.pyplot as plt
 from pathlib import Path

-base = Path(__file__).resolve().parents[2] / 'zz-data' / 'chapter01'
-df_init = pd.read_csv(base / '01_P_derivative_initial.csv')
-df_opt  = pd.read_csv(base / '01_P_derivative_optimized.csv')
+base = Path(__file__).resolve().parents[2] / "zz-data" / "chapter01"
+df_init = pd.read_csv(base / "01_P_derivative_initial.csv")
+df_opt = pd.read_csv(base / "01_P_derivative_optimized.csv")

-T_i, dP_i = df_init['T'], df_init['dP_dT']
-T_o, dP_o = df_opt['T'], df_opt['dP_dT']
+T_i, dP_i = df_init["T"], df_init["dP_dT"]
+T_o, dP_o = df_opt["T"], df_opt["dP_dT"]

-plt.figure(figsize=(8,4.5), dpi=300)
-plt.plot(T_i, dP_i, '--', color='gray', label=r'$\dot P_{\rm init}$ (lissé)')
-plt.plot(T_o, dP_o, '-',  color='orange', label=r'$\dot P_{\rm opt}$ (lissé)')
-plt.xscale('log')
-plt.xlabel('T (Gyr)')
-plt.ylabel(r'$\dot P\,(\mathrm{Gyr}^{-1})$')
-plt.title(r'Fig. 06 – $\dot{P}(T)$ initial vs optimisé')
-plt.grid(True, which='both', linestyle=':', linewidth=0.5)
-plt.legend(loc='center right')
+plt.figure(figsize=(8, 4.5), dpi=300)
+plt.plot(T_i, dP_i, "--", color="gray", label=r"$\dot P_{\rm init}$ (lissé)")
+plt.plot(T_o, dP_o, "-", color="orange", label=r"$\dot P_{\rm opt}$ (lissé)")
+plt.xscale("log")
+plt.xlabel("T (Gyr)")
+plt.ylabel(r"$\dot P\,(\mathrm{Gyr}^{-1})$")
+plt.title(r"Fig. 06 – $\dot{P}(T)$ initial vs optimisé")
+plt.grid(True, which="both", linestyle=":", linewidth=0.5)
+plt.legend(loc="center right")
 plt.tight_layout()

-out = Path(__file__).resolve().parents[2] / 'zz-figures' / 'chapter01' / 'fig_06_comparison.png'
+out = (
+    Path(__file__).resolve().parents[2]
+    / "zz-figures"
+    / "chapter01"
+    / "fig_06_comparison.png"
+)
 plt.savefig(out)
diff --git a/zz-scripts/chapter01/requirements.txt b/zz-scripts/chapter01/requirements.txt
index 49dc5d2..a947d2a 100755
--- a/zz-scripts/chapter01/requirements.txt
+++ b/zz-scripts/chapter01/requirements.txt
@@ -1,80 +1,80 @@
 # Exigences pour le Chapitre 1 (Introduction conceptuelle)

 1. **Environnement système**
-   * Système : Ubuntu 20.04 (WSL ou natif) ou équivalent Linux
-   * Espace disque : ≥ 500 Mo libre
+   * Système : Ubuntu 20.04 (WSL ou natif) ou équivalent Linux
+   * Espace disque : ≥ 500 Mo libre

 2. **Environnement Python**
-   * Python ≥ 3.9
+   * Python ≥ 3.9
    * Créer et activer un environnement virtuel spécifique :
-
-       cd ~/MCGT
-       python3 -m venv venv1
+
+       cd ~/MCGT
+       python3 -m venv venv1
        source venv1/bin/activate
-
+
    * Installer les dépendances :
-
-       pip install --upgrade pip
+
+       pip install --upgrade pip
        pip install numpy pandas scipy matplotlib
-
+

 3. **Données brutes & scripts d’extraction**
-   * **Jalons chronologie**
-     – Source : fichier `zz-data/chapter01/01_timeline_milestones.csv`
-       (colonnes T, P_ref issus de la littérature)
-   * **Données initiales**
-     – Fichier : `zz-data/chapter01/01_initial_grid_data.dat`
-       (colonnes T, P_init ; espace délimité)
-   * Extraction et interpolation via :
-     `zz-scripts/chapter01/generate_data_chapter01.py`
+   * **Jalons chronologie**
+     – Source : fichier `zz-data/chapter01/01_timeline_milestones.csv`
+       (colonnes T, P_ref issus de la littérature)
+   * **Données initiales**
+     – Fichier : `zz-data/chapter01/01_initial_grid_data.dat`
+       (colonnes T, P_init ; espace délimité)
+   * Extraction et interpolation via :
+     `zz-scripts/chapter01/generate_data_chapter01.py`
      → production des CSV/DAT optimisés

 4. **Structure du dépôt**
-
-    MCGT/
-    ├─ 01-introduction-applications/    # sources LaTeX et GUIDE
-    ├─ zz-data/chapter01/               # .csv, .dat d’entrée et produits
-    ├─ zz-scripts/chapter01/            # requirements.txt, generate_*.py, plot_*.py
-    ├─ zz-figures/chapter01/            # .png produits
+
+    MCGT/
+    ├─ 01-introduction-applications/    # sources LaTeX et GUIDE
+    ├─ zz-data/chapter01/               # .csv, .dat d’entrée et produits
+    ├─ zz-scripts/chapter01/            # requirements.txt, generate_*.py, plot_*.py
+    ├─ zz-figures/chapter01/            # .png produits
     └─ venv1/                           # environnement virtuel Python
-
+

 5. **Exécution du pipeline**
 1. Génération des données :
-
-       source venv1/bin/activate
+
+       source venv1/bin/activate
        python zz-scripts/chapter01/generate_data_chapter01.py --csv zz-data/chapter01/01_timeline_milestones.csv --tmin 1e-6 --tmax 14 --step 0.01 --grid log [--window 21 --poly 3]
-
+
 2. Tracé des figures :
-
-       python zz-scripts/chapter01/plot_fig01_early_plateau.py
-       python zz-scripts/chapter01/plot_fig02_logistic_calibration.py
-       python zz-scripts/chapter01/plot_fig03_relative_error_timeline.py
-       python zz-scripts/chapter01/plot_fig04_P_vs_T_evolution.py
-       python zz-scripts/chapter01/plot_fig05_I1_vs_T.py
+
+       python zz-scripts/chapter01/plot_fig01_early_plateau.py
+       python zz-scripts/chapter01/plot_fig02_logistic_calibration.py
+       python zz-scripts/chapter01/plot_fig03_relative_error_timeline.py
+       python zz-scripts/chapter01/plot_fig04_P_vs_T_evolution.py
+       python zz-scripts/chapter01/plot_fig05_I1_vs_T.py
        python zz-scripts/chapter01/plot_fig06_P_derivative_comparison.py
-
+

 6. **Compilation LaTeX**
-
-    cd 01-introduction-applications/
-    pdflatex -interaction=nonstopmode 01_introduction_conceptual.tex
+
+    cd 01-introduction-applications/
+    pdflatex -interaction=nonstopmode 01_introduction_conceptual.tex
     pdflatex -interaction=nonstopmode 01_applications_calibration_conceptual.tex
-
+

 7. **Tests & validations**
    * Vérifier l’existence des fichiers dans `zz-data/chapter01/` :
-
-       01_timeline_milestones.csv
-       01_initial_grid_data.dat
-       01_optimized_data.csv
-       01_optimized_grid_data.dat
-       01_P_derivative_initial.csv
-       01_P_derivative_optimized.csv
-       01_optimized_data_and_derivatives.csv
-       01_relative_error_timeline.csv
+
+       01_timeline_milestones.csv
+       01_initial_grid_data.dat
+       01_optimized_data.csv
+       01_optimized_grid_data.dat
+       01_P_derivative_initial.csv
+       01_P_derivative_optimized.csv
+       01_optimized_data_and_derivatives.csv
+       01_relative_error_timeline.csv
        01_dimensionless_invariants.csv
-
-   * S’assurer qu’il n’y a pas de `NaN` ou `Inf` dans les CSV
-   * Vérifier que la taille de la grille (n_points) = 1401
+
+   * S’assurer qu’il n’y a pas de `NaN` ou `Inf` dans les CSV
+   * Vérifier que la taille de la grille (n_points) = 1401
    * Contrôler visuellement chaque PNG dans `zz-figures/chapter01/`
diff --git a/zz-scripts/chapter02/extract_sympy_FG.ipynb b/zz-scripts/chapter02/extract_sympy_FG.ipynb
index 51d18da..1acb6a5 100755
--- a/zz-scripts/chapter02/extract_sympy_FG.ipynb
+++ b/zz-scripts/chapter02/extract_sympy_FG.ipynb
@@ -1,111 +1,116 @@
-{
-  "cells": [
-    {
-      "cell_type": "markdown",
-      "metadata": {},
-      "source": [
-        "# Extraction symbolique de F(α) et G(α)\n",
-        "\n",
-        "Ce notebook lit les résultats de calibration `02_As_ns_vs_alpha.csv`, ajuste un polynôme en α pour F(α) et G(α), reconstruit les expressions symboliques, et exporte la table des coefficients."
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": null,
-      "metadata": {},
-      "outputs": [],
-      "source": [
-        "from pathlib import Path\n",
-        "import numpy as np\n",
-        "import pandas as pd\n",
-        "import sympy as sp\n",
-        "\n",
-        "# 1️⃣ Définition des chemins\n",
-        "ROOT     = Path.cwd().parents[1]  # si lancé depuis zz-scripts/chapter02\n",
-        "DATA_IN  = ROOT / \"zz-data\" / \"chapter02\" / \"02_As_ns_vs_alpha.csv\"\n",
-        "DATA_OUT = ROOT / \"zz-data\" / \"chapter02\"\n",
-        "DATA_OUT.mkdir(parents=True, exist_ok=True)"
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": null,
-      "metadata": {},
-      "outputs": [],
-      "source": [
-        "# 2️⃣ Lecture des données et calcul des séries\n",
-        "df    = pd.read_csv(DATA_IN)\n",
-        "alpha = df[\"alpha\"].values\n",
-        "As    = df[\"A_s\"].values\n",
-        "ns    = df[\"n_s\"].values\n",
-        "\n",
-        "# Constantes Planck 2018\n",
-        "A_s0 = 2.10e-9\n",
-        "ns0  = 0.9649\n",
-        "\n",
-        "# 3️⃣ Séries F(α)-1 et G(α)\n",
-        "Fm1 = As / A_s0 - 1\n",
-        "Gm  = ns - ns0\n",
-        "\n",
-        "# 4️⃣ Ajustement polynômial (ordre 2)\n",
-        "ordre  = 2\n",
-        "coef_F = np.polyfit(alpha, Fm1, ordre)  # [c2, c1, c0]\n",
-        "coef_G = np.polyfit(alpha, Gm,  ordre)  # [d2, d1, d0]\n",
-        "\n",
-        "# 5️⃣ Reconstruction symbolique\n",
-        "a      = sp.symbols('alpha')\n",
-        "F_expr = 1 + sum(coef_F[i] * a**(ordre - i) for i in range(ordre+1))\n",
-        "G_expr =      sum(coef_G[i] * a**(ordre - i) for i in range(ordre+1))\n",
-        "\n",
-        "print(\"Expression symbolique pour F(α) :\")\n",
-        "sp.pprint(sp.simplify(F_expr))\n",
-        "print(\"\\nExpression symbolique pour G(α) :\")\n",
-        "sp.pprint(sp.simplify(G_expr))"
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": null,
-      "metadata": {},
-      "outputs": [],
-      "source": [
-        "# 6️⃣ Export des coefficients vers CSV\n",
-        "rows = []\n",
-        "for i, c in enumerate(coef_F[::-1]):  # c0 → c2\n",
-        "    rows.append({\"fonc\": \"F\", \"ordre\": i, \"coeff\": float(c)})\n",
-        "for i, d in enumerate(coef_G[::-1]):  # d0 → d2\n",
-        "    rows.append({\"fonc\": \"G\", \"ordre\": i, \"coeff\": float(d)})\n",
-        "\n",
-        "df_out = pd.DataFrame(rows)\n",
-        "OUT_CSV = DATA_OUT / \"02_FG_series.csv\"\n",
-        "df_out.to_csv(OUT_CSV, index=False, float_format=\"%.6e\")\n",
-        "print(f\"→ Séries F/G exportées dans {OUT_CSV}\")"
-      ]
-    },
-    {
-      "cell_type": "code",
-      "execution_count": null,
-      "metadata": {},
-      "outputs": [],
-      "source": [
-        "# 7️⃣ Tests/Assertions\n",
-        "assert set(df_out[df_out.fonc=='F'].ordre) == {0,1,2}, \"Termes F manquants\"\n",
-        "assert set(df_out[df_out.fonc=='G'].ordre) == {0,1,2}, \"Termes G manquants\"\n",
-        "print(\"✅ Tous les termes F et G jusqu'à l'ordre 2 sont présents.\")"
-      ]
-    }
-  ],
-  "metadata": {
-    "kernelspec": {
-      "display_name": "Python 3",
-      "language": "python",
-      "name": "python3"
-    },
-    "language_info": {
-      "name": "python",
-      "version": ""
-    }
-  },
-  "nbformat": 4,
-  "nbformat_minor": 5
-}
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "id": "7fb27b941602401d91542211134fc71a",
+   "metadata": {},
+   "source": [
+    "# Extraction symbolique de F(α) et G(α)\n",
+    "\n",
+    "Ce notebook lit les résultats de calibration `02_As_ns_vs_alpha.csv`, ajuste un polynôme en α pour F(α) et G(α), reconstruit les expressions symboliques, et exporte la table des coefficients."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "acae54e37e7d407bbb7b55eff062a284",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from pathlib import Path\n",
+    "import numpy as np\n",
+    "import pandas as pd\n",
+    "import sympy as sp\n",
+    "\n",
+    "# 1️⃣ Définition des chemins\n",
+    "ROOT = Path.cwd().parents[1]  # si lancé depuis zz-scripts/chapter02\n",
+    "DATA_IN = ROOT / \"zz-data\" / \"chapter02\" / \"02_As_ns_vs_alpha.csv\"\n",
+    "DATA_OUT = ROOT / \"zz-data\" / \"chapter02\"\n",
+    "DATA_OUT.mkdir(parents=True, exist_ok=True)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# 2️⃣ Lecture des données et calcul des séries\n",
+    "df = pd.read_csv(DATA_IN)\n",
+    "alpha = df[\"alpha\"].values\n",
+    "As = df[\"A_s\"].values\n",
+    "ns = df[\"n_s\"].values\n",
+    "\n",
+    "# Constantes Planck 2018\n",
+    "A_s0 = 2.10e-9\n",
+    "ns0 = 0.9649\n",
+    "\n",
+    "# 3️⃣ Séries F(α)-1 et G(α)\n",
+    "Fm1 = As / A_s0 - 1\n",
+    "Gm = ns - ns0\n",
+    "\n",
+    "# 4️⃣ Ajustement polynômial (ordre 2)\n",
+    "ordre = 2\n",
+    "coef_F = np.polyfit(alpha, Fm1, ordre)  # [c2, c1, c0]\n",
+    "coef_G = np.polyfit(alpha, Gm, ordre)  # [d2, d1, d0]\n",
+    "\n",
+    "# 5️⃣ Reconstruction symbolique\n",
+    "a = sp.symbols(\"alpha\")\n",
+    "F_expr = 1 + sum(coef_F[i] * a ** (ordre - i) for i in range(ordre + 1))\n",
+    "G_expr = sum(coef_G[i] * a ** (ordre - i) for i in range(ordre + 1))\n",
+    "\n",
+    "print(\"Expression symbolique pour F(α) :\")\n",
+    "sp.pprint(sp.simplify(F_expr))\n",
+    "print(\"\\nExpression symbolique pour G(α) :\")\n",
+    "sp.pprint(sp.simplify(G_expr))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "8dd0d8092fe74a7c96281538738b07e2",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# 6️⃣ Export des coefficients vers CSV\n",
+    "rows = []\n",
+    "for i, c in enumerate(coef_F[::-1]):  # c0 → c2\n",
+    "    rows.append({\"fonc\": \"F\", \"ordre\": i, \"coeff\": float(c)})\n",
+    "for i, d in enumerate(coef_G[::-1]):  # d0 → d2\n",
+    "    rows.append({\"fonc\": \"G\", \"ordre\": i, \"coeff\": float(d)})\n",
+    "\n",
+    "df_out = pd.DataFrame(rows)\n",
+    "OUT_CSV = DATA_OUT / \"02_FG_series.csv\"\n",
+    "df_out.to_csv(OUT_CSV, index=False, float_format=\"%.6e\")\n",
+    "print(f\"→ Séries F/G exportées dans {OUT_CSV}\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "72eea5119410473aa328ad9291626812",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# 7️⃣ Tests/Assertions\n",
+    "assert set(df_out[df_out.fonc == \"F\"].ordre) == {0, 1, 2}, \"Termes F manquants\"\n",
+    "assert set(df_out[df_out.fonc == \"G\"].ordre) == {0, 1, 2}, \"Termes G manquants\"\n",
+    "print(\"✅ Tous les termes F et G jusqu'à l'ordre 2 sont présents.\")"
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "name": "python",
+   "version": ""
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 5
+}
diff --git a/zz-scripts/chapter02/generate_data_chapter02.py b/zz-scripts/chapter02/generate_data_chapter02.py
index 01bc232..001a6ca 100755
--- a/zz-scripts/chapter02/generate_data_chapter02.py
+++ b/zz-scripts/chapter02/generate_data_chapter02.py
@@ -33,93 +33,117 @@ logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
 # --- Section 2 : Fonctions utilitaires ---
 def dotP(T, a0, ainf, Tc, Delta, Tp):
     a_log = a0 + (ainf - a0) / (1 + np.exp(-(T - Tc) / Delta))
-    a = a_log * (1 - np.exp(-(T / Tp) ** 2))
-    da_log = ((ainf - a0) / Delta) * np.exp(-(T - Tc) / Delta) \
-             / (1 + np.exp(-(T - Tc) / Delta)) ** 2
-    da = da_log * (1 - np.exp(-(T / Tp) ** 2)) \
-         + a_log * (2 * T / Tp ** 2) * np.exp(-(T / Tp) ** 2)
-    return a * T**(a - 1) + T**a * np.log(T) * da
+    a = a_log * (1 - np.exp(-((T / Tp) ** 2)))
+    da_log = (
+        ((ainf - a0) / Delta)
+        * np.exp(-(T - Tc) / Delta)
+        / (1 + np.exp(-(T - Tc) / Delta)) ** 2
+    )
+    da = da_log * (1 - np.exp(-((T / Tp) ** 2))) + a_log * (2 * T / Tp**2) * np.exp(
+        -((T / Tp) ** 2)
+    )
+    return a * T ** (a - 1) + T**a * np.log(T) * da
+

 def integrate(grid, pars, P0):
     dP = dotP(grid, *pars)
     window = 21 if (len(dP) > 21 and 21 % 2 == 1) else (len(dP) - 1)
-    dP_s = savgol_filter(dP, window, 3, mode='interp')
+    dP_s = savgol_filter(dP, window, 3, mode="interp")
     P = np.empty_like(grid)
     P[0] = P0
     for i in range(1, len(grid)):
-        P[i] = P[i-1] + 0.5*(dP_s[i] + dP_s[i-1])*(grid[i] - grid[i-1])
+        P[i] = P[i - 1] + 0.5 * (dP_s[i] + dP_s[i - 1]) * (grid[i] - grid[i - 1])
     return P

+
 def fit_segment(T, P_ref, mask, grid, P0, weights, prim_mask, thresh_primary):
     def objective(theta):
         P = integrate(grid, theta, P0)
         interp = PchipInterpolator(np.log10(grid), np.log10(P), extrapolate=True)
-        P_opt = 10**interp(np.log10(T[mask]))
+        P_opt = 10 ** interp(np.log10(T[mask]))
         eps = (P_opt - P_ref[mask]) / P_ref[mask]
         penalty = 0.0
         if prim_mask[mask].any():
             excess = np.max(np.abs(eps[prim_mask[mask]])) - thresh_primary
-            penalty = 1e8 * max(0, excess)**2
-        return np.sum((weights[mask] * eps)**2) + penalty
-
-    bounds = [(0.1,1.0),(0.5,2.0),(0.01,0.5),(0.005,0.1),(0.02,0.5)]
-    res = minimize(objective, x0=[0.3,1.0,0.2,0.02,0.15],
-                   bounds=bounds, method='L-BFGS-B', options={'maxiter':400})
+            penalty = 1e8 * max(0, excess) ** 2
+        return np.sum((weights[mask] * eps) ** 2) + penalty
+
+    bounds = [(0.1, 1.0), (0.5, 2.0), (0.01, 0.5), (0.005, 0.1), (0.02, 0.5)]
+    res = minimize(
+        objective,
+        x0=[0.3, 1.0, 0.2, 0.02, 0.15],
+        bounds=bounds,
+        method="L-BFGS-B",
+        options={"maxiter": 400},
+    )
     return res.x

+
 def parse_args():
     parser = argparse.ArgumentParser(
         description="Pipeline Chapitre 2 (+ option spectre primordial)"
     )
     parser.add_argument(
-        "--spectre", action="store_true",
-        help="Après calibrage, génère 02_primordial_spectrum_spec.json & fig_00_spectre.png"
+        "--spectre",
+        action="store_true",
+        help="Après calibrage, génère 02_primordial_spectrum_spec.json & fig_00_spectre.png",
     )
     return parser.parse_args()

+
 # --- Section 3 : Pipeline principal ---
 def main(spectre=False):
-    ROOT    = Path(__file__).resolve().parents[2]
+    ROOT = Path(__file__).resolve().parents[2]
     DATA_DIR = ROOT / "zz-data" / "chapter02"
-    IMG_DIR  = ROOT / "zz-figures" / "chapter02"
+    IMG_DIR = ROOT / "zz-figures" / "chapter02"
     DATA_DIR.mkdir(parents=True, exist_ok=True)
     IMG_DIR.mkdir(parents=True, exist_ok=True)

     # 3.1 Chargement des jalons
-    meta     = pd.read_csv(DATA_DIR / "02_milestones_meta.csv")
-    T         = meta["T"].values
-    P_ref     = meta["P_ref"].values
-    cls       = meta["classe"].values
-    weights   = np.where(cls == "primaire", 1.0, 0.1)
+    meta = pd.read_csv(DATA_DIR / "02_milestones_meta.csv")
+    T = meta["T"].values
+    P_ref = meta["P_ref"].values
+    cls = meta["classe"].values
+    weights = np.where(cls == "primaire", 1.0, 0.1)
     prim_mask = cls == "primaire"
     thresh_primary = 0.01
-    thresh_order2  = 0.10
+    thresh_order2 = 0.10

     # 3.2 Définition de la grille et segmentation
     Tmin, Tmax, dlog, T_split = 1e-6, 14.0, 0.01, 0.15
+
     def make_grid(t0, t1):
         n = int(np.floor((np.log10(t1) - np.log10(t0)) / dlog)) + 1
         return np.logspace(np.log10(t0), np.log10(t1), n)

-    grid_low   = make_grid(Tmin, T_split)
-    mask_low   = T < T_split
-    P0         = float(P_ref[mask_low][0])
-    params_low = fit_segment(T, P_ref, mask_low, grid_low, P0,
-                             weights, prim_mask, thresh_primary)
-    P_split    = integrate(grid_low, params_low, P0)
+    grid_low = make_grid(Tmin, T_split)
+    mask_low = T < T_split
+    P0 = float(P_ref[mask_low][0])
+    params_low = fit_segment(
+        T, P_ref, mask_low, grid_low, P0, weights, prim_mask, thresh_primary
+    )
+    P_split = integrate(grid_low, params_low, P0)

     params_high = None
     if mask_low.size and (~mask_low).any():
-        grid_high   = make_grid(T_split, Tmax)
-        params_high = fit_segment(T, P_ref, ~mask_low, grid_high,
-                                  P_split[-1], weights, prim_mask, thresh_primary)
+        grid_high = make_grid(T_split, Tmax)
+        params_high = fit_segment(
+            T,
+            P_ref,
+            ~mask_low,
+            grid_high,
+            P_split[-1],
+            weights,
+            prim_mask,
+            thresh_primary,
+        )

     # 3.3 Fusion des segments et export de P(T)
     P_low = integrate(grid_low, params_low, P0)
     if params_high is not None:
         P_high = integrate(grid_high, params_high, P_split[-1])
-        T_all  = np.hstack([grid_low, grid_high])
-        P_all  = np.hstack([P_low, P_high])
+        T_all = np.hstack([grid_low, grid_high])
+        P_all = np.hstack([P_low, P_high])
     else:
         T_all, P_all = grid_low, P_low

@@ -128,16 +152,17 @@ def main(spectre=False):
     np.savetxt(
         DATA_DIR / "02_P_vs_T_grid_data.dat",
         np.column_stack((T_all, P_all)),
-        fmt="%.6e", header="T P_calc"
+        fmt="%.6e",
+        header="T P_calc",
     )

     # 3.4 Export de la dérivée lissée
     if params_high is not None:
-        dP_low   = dotP(grid_low,   *params_low)
-        dP_high  = dotP(grid_high,  *params_high)
-        T_der    = np.hstack([grid_low, grid_high])
-        dP_der   = np.hstack([dP_low, dP_high])
-        idx2     = np.argsort(T_der)
+        dP_low = dotP(grid_low, *params_low)
+        dP_high = dotP(grid_high, *params_high)
+        T_der = np.hstack([grid_low, grid_high])
+        dP_der = np.hstack([dP_low, dP_high])
+        idx2 = np.argsort(T_der)
         T_der, dP_der = T_der[idx2], dP_der[idx2]
     else:
         T_der, dP_der = grid_low, dotP(grid_low, *params_low)
@@ -145,7 +170,8 @@ def main(spectre=False):
     np.savetxt(
         DATA_DIR / "02_P_derivative_data.dat",
         np.column_stack((T_der, dP_der)),
-        fmt="%.6e", header="T dotP"
+        fmt="%.6e",
+        header="T dotP",
     )

     # 3.5 Recalcul des écarts et export des CSV
@@ -154,20 +180,21 @@ def main(spectre=False):
     _, unique_idx = np.unique(logT, return_index=True)
     interp_fn = PchipInterpolator(logT[unique_idx], logP[unique_idx], extrapolate=True)
     P_opt = 10 ** interp_fn(np.log10(T))
-    eps   = (P_opt - P_ref) / P_ref
-
-    pd.DataFrame({
-        "T":          T,
-        "P_ref":      P_ref,
-        "P_opt":      np.round(P_opt, 6),
-        "epsilon_i":  np.round(eps, 6),
-        "classe":     cls
-    }).to_csv(DATA_DIR / "02_timeline_milestones.csv", index=False)
+    eps = (P_opt - P_ref) / P_ref
+
+    pd.DataFrame(
+        {
+            "T": T,
+            "P_ref": P_ref,
+            "P_opt": np.round(P_opt, 6),
+            "epsilon_i": np.round(eps, 6),
+            "classe": cls,
+        }
+    ).to_csv(DATA_DIR / "02_timeline_milestones.csv", index=False)

-    pd.DataFrame({
-        "T":         T,
-        "epsilon_i": np.round(eps, 6)
-    }).to_csv(DATA_DIR / "02_relative_error_timeline.csv", index=False)
+    pd.DataFrame({"T": T, "epsilon_i": np.round(eps, 6)}).to_csv(
+        DATA_DIR / "02_relative_error_timeline.csv", index=False
+    )

     # Alerte ordre 2
     if (~prim_mask).any():
@@ -179,24 +206,25 @@ def main(spectre=False):
     json_out = {
         "T_split_Gyr": T_split,
         "segments": {
-            "low": dict(zip(
-                ["alpha0","alpha_inf","Tc","Delta","Tp"],
-                np.round(params_low, 6).tolist()
-            ))
-        },
-        "thresholds": {
-            "primary": thresh_primary,
-            "order2":  thresh_order2
+            "low": dict(
+                zip(
+                    ["alpha0", "alpha_inf", "Tc", "Delta", "Tp"],
+                    np.round(params_low, 6).tolist(),
+                )
+            )
         },
+        "thresholds": {"primary": thresh_primary, "order2": thresh_order2},
         "max_epsilon_primary": float(np.max(np.abs(eps[prim_mask]))),
-        "max_epsilon_order2":  float(eps_o2 if (~prim_mask).any() else 0.0)
+        "max_epsilon_order2": float(eps_o2 if (~prim_mask).any() else 0.0),
     }

     if params_high is not None:
-        json_out["segments"]["high"] = dict(zip(
-            ["alpha0","alpha_inf","Tc","Delta","Tp"],
-            np.round(params_high, 6).tolist()
-        ))
+        json_out["segments"]["high"] = dict(
+            zip(
+                ["alpha0", "alpha_inf", "Tc", "Delta", "Tp"],
+                np.round(params_high, 6).tolist(),
+            )
+        )

     with open(DATA_DIR / "02_optimal_parameters.json", "w", encoding="utf-8") as f:
         json.dump(json_out, f, ensure_ascii=False, indent=2)
@@ -207,26 +235,21 @@ def main(spectre=False):
     if spectre:
         # 4.1) Lecture des coefficients F et G
         fg_csv = DATA_DIR / "02_FG_series.csv"
-        df_fg  = pd.read_csv(fg_csv)
+        df_fg = pd.read_csv(fg_csv)

         # extraire c1, c1_2, c2, c2_2 depuis le CSV
-        c1   = float(df_fg[(df_fg.fonc == "F") & (df_fg.ordre == 1)].coeff)
+        c1 = float(df_fg[(df_fg.fonc == "F") & (df_fg.ordre == 1)].coeff)
         c1_2 = float(df_fg[(df_fg.fonc == "F") & (df_fg.ordre == 2)].coeff)
-        c2   = float(df_fg[(df_fg.fonc == "G") & (df_fg.ordre == 1)].coeff)
+        c2 = float(df_fg[(df_fg.fonc == "G") & (df_fg.ordre == 1)].coeff)
         c2_2 = float(df_fg[(df_fg.fonc == "G") & (df_fg.ordre == 2)].coeff)

         # 4.2) Construction du JSON de métadonnées du spectre
         spec = {
-            "label_eq":    "eq:spec_prim",
-            "formule":     "P_R(k;α)=A_s(α) k^{n_s(α)-1}",
+            "label_eq": "eq:spec_prim",
+            "formule": "P_R(k;α)=A_s(α) k^{n_s(α)-1}",
             "description": "Spectre primordial modifié MCGT – Paramètres Planck 2018",
-            "constantes":  {"A_s0": 2.10e-9, "ns0": 0.9649},
-            "coefficients": {
-                "c1":   c1,
-                "c1_2": c1_2,
-                "c2":   c2,
-                "c2_2": c2_2
-            }
+            "constantes": {"A_s0": 2.10e-9, "ns0": 0.9649},
+            "coefficients": {"c1": c1, "c1_2": c1_2, "c2": c2, "c2_2": c2_2},
         }
         out_spec = DATA_DIR / "02_primordial_spectrum_spec.json"
         with open(out_spec, "w", encoding="utf-8") as f:
@@ -235,11 +258,15 @@ def main(spectre=False):

         # 4.3) Tracé de la figure d’exemple
         subprocess.run(
-            ["python3", str(ROOT / "zz-scripts" / "chapter02" / "plot_fig00_spectrum.py")],
-            check=True
+            [
+                "python3",
+                str(ROOT / "zz-scripts" / "chapter02" / "plot_fig00_spectrum.py"),
+            ],
+            check=True,
         )
         logging.info("fig_00_spectre.png générée.")

+
 if __name__ == "__main__":
     args = parse_args()
     main(spectre=args.spectre)
diff --git a/zz-scripts/chapter02/plot_fig00_spectrum.py b/zz-scripts/chapter02/plot_fig00_spectrum.py
index 6d1a3f2..3fd4cb7 100755
--- a/zz-scripts/chapter02/plot_fig00_spectrum.py
+++ b/zz-scripts/chapter02/plot_fig00_spectrum.py
@@ -21,7 +21,7 @@ for alpha in alphas:
     ax.loglog(k, P_R(k, alpha), label=f"α = {alpha}")

 ax.set_xlabel("k [h·Mpc⁻¹]")
-ax.set_ylabel("P_R(k; α)", labelpad=12)    # labelpad pour décaler plus à droite
+ax.set_ylabel("P_R(k; α)", labelpad=12)  # labelpad pour décaler plus à droite
 ax.set_title("Spectre primordial MCGT")
 ax.legend(loc="upper right")
 ax.grid(True, which="both", linestyle="--", linewidth=0.5)
diff --git a/zz-scripts/chapter02/plot_fig01_P_vs_T_evolution.py b/zz-scripts/chapter02/plot_fig01_P_vs_T_evolution.py
index f7d5b7e..dc106da 100755
--- a/zz-scripts/chapter02/plot_fig01_P_vs_T_evolution.py
+++ b/zz-scripts/chapter02/plot_fig01_P_vs_T_evolution.py
@@ -1,5 +1,6 @@
 #!/usr/bin/env python3
 """Fig. 01 – Évolution de P(T) – Chapitre 2 (Validation chronologique)"""
+
 import matplotlib.pyplot as plt
 import numpy as np
 import pandas as pd
@@ -19,14 +20,14 @@ P_ref = results["P_ref"]

 # Plot
 plt.figure(dpi=300)
-plt.plot(T_dense, P_dense, '-', label=r'$P_{\rm calc}(T)$', color='orange')
-plt.scatter(T_ref, P_ref, marker='o', label=r'$P_{\rm ref}(T_i)$', color='grey')
-plt.xscale('log')
-plt.yscale('log')
-plt.xlabel('T (Gyr)')
-plt.ylabel('P(T)')
-plt.title('Fig. 01 – Évolution de P(T) – Chapitre 2')
-plt.grid(True, which='both', linestyle=':', linewidth=0.5)
+plt.plot(T_dense, P_dense, "-", label=r"$P_{\rm calc}(T)$", color="orange")
+plt.scatter(T_ref, P_ref, marker="o", label=r"$P_{\rm ref}(T_i)$", color="grey")
+plt.xscale("log")
+plt.yscale("log")
+plt.xlabel("T (Gyr)")
+plt.ylabel("P(T)")
+plt.title("Fig. 01 – Évolution de P(T) – Chapitre 2")
+plt.grid(True, which="both", linestyle=":", linewidth=0.5)
 plt.legend()
 plt.tight_layout()
 plt.savefig(FIG_DIR / "fig_01_P_vs_T_evolution.png")
diff --git a/zz-scripts/chapter02/plot_fig02_calibration.py b/zz-scripts/chapter02/plot_fig02_calibration.py
index 9a1e657..392b824 100755
--- a/zz-scripts/chapter02/plot_fig02_calibration.py
+++ b/zz-scripts/chapter02/plot_fig02_calibration.py
@@ -1,7 +1,7 @@
 #!/usr/bin/env python3
 """Fig. 02 – Diagramme de calibration (P_calc vs P_ref) – Chapitre 2"""
+
 import matplotlib.pyplot as plt
-import numpy as np
 import pandas as pd
 from pathlib import Path

@@ -18,16 +18,16 @@ P_calc = df["P_opt"]

 # Plot
 plt.figure(dpi=300)
-plt.scatter(P_ref, P_calc, marker='o', color='grey', label='Jalons')
+plt.scatter(P_ref, P_calc, marker="o", color="grey", label="Jalons")
 lim_min = min(P_ref.min(), P_calc.min())
 lim_max = max(P_ref.max(), P_calc.max())
-plt.plot([lim_min, lim_max], [lim_min, lim_max], '--', color='black', label='Identité')
-plt.xscale('log')
-plt.yscale('log')
-plt.xlabel(r'$P_{\rm ref}$')
-plt.ylabel(r'$P_{\rm calc}$')
-plt.title('Fig. 02 – Diagramme de calibration – Chapitre 2')
-plt.grid(True, which='both', linestyle=':', linewidth=0.5)
+plt.plot([lim_min, lim_max], [lim_min, lim_max], "--", color="black", label="Identité")
+plt.xscale("log")
+plt.yscale("log")
+plt.xlabel(r"$P_{\rm ref}$")
+plt.ylabel(r"$P_{\rm calc}$")
+plt.title("Fig. 02 – Diagramme de calibration – Chapitre 2")
+plt.grid(True, which="both", linestyle=":", linewidth=0.5)
 plt.legend()
 plt.tight_layout()
 plt.savefig(FIG_DIR / "fig_02_calibration.png")
diff --git a/zz-scripts/chapter02/plot_fig03_relative_errors.py b/zz-scripts/chapter02/plot_fig03_relative_errors.py
index 5526658..a39fffa 100755
--- a/zz-scripts/chapter02/plot_fig03_relative_errors.py
+++ b/zz-scripts/chapter02/plot_fig03_relative_errors.py
@@ -1,8 +1,8 @@
 #!/usr/bin/env python3
 """Fig. 03 – Écarts relatifs $\varepsilon_i$ – Chapitre 2"""
+
 import matplotlib.pyplot as plt
 import pandas as pd
-import numpy as np
 from pathlib import Path

 # Paths
@@ -23,19 +23,21 @@ order2 = cls != "primaire"

 # Plot
 plt.figure(dpi=300)
-plt.scatter(T[primary], eps[primary], marker='o', label='Jalons primaires', color='black')
-plt.scatter(T[order2], eps[order2], marker='s', label='Jalons ordre 2', color='grey')
-plt.xscale('log')
-plt.yscale('symlog', linthresh=1e-3)
+plt.scatter(
+    T[primary], eps[primary], marker="o", label="Jalons primaires", color="black"
+)
+plt.scatter(T[order2], eps[order2], marker="s", label="Jalons ordre 2", color="grey")
+plt.xscale("log")
+plt.yscale("symlog", linthresh=1e-3)
 # Threshold lines
-plt.axhline(0.01, linestyle='--', linewidth=0.8, color='blue', label='Seuil 1%')
-plt.axhline(-0.01, linestyle='--', linewidth=0.8, color='blue')
-plt.axhline(0.10, linestyle=':', linewidth=0.8, color='red', label='Seuil 10%')
-plt.axhline(-0.10, linestyle=':', linewidth=0.8, color='red')
-plt.xlabel('T (Gyr)')
-plt.ylabel(r'$\varepsilon_i$')
-plt.title('Fig. 03 – Écarts relatifs $\varepsilon_i$ – Chapitre 2')
-plt.grid(True, which='both', linestyle=':', linewidth=0.5)
+plt.axhline(0.01, linestyle="--", linewidth=0.8, color="blue", label="Seuil 1%")
+plt.axhline(-0.01, linestyle="--", linewidth=0.8, color="blue")
+plt.axhline(0.10, linestyle=":", linewidth=0.8, color="red", label="Seuil 10%")
+plt.axhline(-0.10, linestyle=":", linewidth=0.8, color="red")
+plt.xlabel("T (Gyr)")
+plt.ylabel(r"$\varepsilon_i$")
+plt.title("Fig. 03 – Écarts relatifs $\varepsilon_i$ – Chapitre 2")
+plt.grid(True, which="both", linestyle=":", linewidth=0.5)
 plt.legend()
 plt.tight_layout()
 plt.savefig(FIG_DIR / "fig_03_relative_errors.png")
diff --git a/zz-scripts/chapter02/plot_fig04_pipeline_diagram.py b/zz-scripts/chapter02/plot_fig04_pipeline_diagram.py
index 9096756..8d7248e 100755
--- a/zz-scripts/chapter02/plot_fig04_pipeline_diagram.py
+++ b/zz-scripts/chapter02/plot_fig04_pipeline_diagram.py
@@ -1,9 +1,9 @@
 #!/usr/bin/env python3
 """Fig. 04 – Schéma de la chaîne de calibration – Chapitre 2"""
+
 import matplotlib.pyplot as plt
 from matplotlib.patches import FancyBboxPatch
 from pathlib import Path
-import numpy as np

 # Paths
 ROOT = Path(__file__).resolve().parents[2]
@@ -11,31 +11,36 @@ FIG_DIR = ROOT / "zz-figures" / "chapter02"
 FIG_DIR.mkdir(parents=True, exist_ok=True)

 fig, ax = plt.subplots(figsize=(8, 4), dpi=300)
-ax.axis('off')
+ax.axis("off")

 # Define steps (text, x-center, y-center)
 steps = [
     ("Lecture des jalons\n$(T_i, P_{\\rm ref})$", 0.1, 0.5),
     ("Interpolation & intégration\n(02_P_vs_T_grid_data.dat)", 0.35, 0.5),
     ("Optimisation\n(segmentation & pénalités)", 0.6, 0.5),
-    ("Export JSON &\nécarts", 0.85, 0.5)
+    ("Export JSON &\nécarts", 0.85, 0.5),
 ]
 width, height = 0.2, 0.15

 # Draw boxes and texts
 for text, xc, yc in steps:
-    box = FancyBboxPatch((xc-width/2, yc-height/2), width, height,
-                         boxstyle="round,pad=0.3", edgecolor="black", facecolor="white")
+    box = FancyBboxPatch(
+        (xc - width / 2, yc - height / 2),
+        width,
+        height,
+        boxstyle="round,pad=0.3",
+        edgecolor="black",
+        facecolor="white",
+    )
     ax.add_patch(box)
-    ax.text(xc, yc, text, ha='center', va='center', fontsize=8)
+    ax.text(xc, yc, text, ha="center", va="center", fontsize=8)

 # Draw arrows
-for i in range(len(steps)-1):
-    x0 = steps[i][1] + width/2
-    x1 = steps[i+1][1] - width/2
+for i in range(len(steps) - 1):
+    x0 = steps[i][1] + width / 2
+    x1 = steps[i + 1][1] - width / 2
     y = steps[i][2]
-    ax.annotate("", xy=(x1, y), xytext=(x0, y),
-                arrowprops=dict(arrowstyle="->", lw=1))
+    ax.annotate("", xy=(x1, y), xytext=(x0, y), arrowprops=dict(arrowstyle="->", lw=1))

 plt.title("Fig. 04 – Schéma de la chaîne de calibration\nChapitre 2", pad=20)
 plt.tight_layout()
diff --git a/zz-scripts/chapter02/plot_fig05_FG_series.py b/zz-scripts/chapter02/plot_fig05_FG_series.py
index fcc7f1a..9f189f1 100755
--- a/zz-scripts/chapter02/plot_fig05_FG_series.py
+++ b/zz-scripts/chapter02/plot_fig05_FG_series.py
@@ -8,35 +8,36 @@ Produit :
 Données sources :
 - zz-data/chapter02/02_As_ns_vs_alpha.csv
 """
-import numpy as np
+
 import pandas as pd
 import matplotlib.pyplot as plt
 from pathlib import Path

 # Constantes Planck 2018
 A_S0 = 2.10e-9
-NS0  = 0.9649
+NS0 = 0.9649

 # Chemins
-ROOT     = Path(__file__).resolve().parents[2]
-DATA_IN  = ROOT / "zz-data" / "chapter02" / "02_As_ns_vs_alpha.csv"
+ROOT = Path(__file__).resolve().parents[2]
+DATA_IN = ROOT / "zz-data" / "chapter02" / "02_As_ns_vs_alpha.csv"
 OUT_PLOT = ROOT / "zz-figures" / "chapter02" / "fig_05_FG_series.png"

+
 def main():
     # Lecture des données
     df = pd.read_csv(DATA_IN)
     alpha = df["alpha"].values
-    As    = df["A_s"].values
-    ns    = df["n_s"].values
+    As = df["A_s"].values
+    ns = df["n_s"].values

     # Calcul des séries
     Fm1 = As / A_S0 - 1.0
-    Gm  = ns - NS0
+    Gm = ns - NS0

     # Tracé
     plt.figure()
     plt.plot(alpha, Fm1, marker="o", linestyle="-", label=r"$F(\alpha)-1$")
-    plt.plot(alpha, Gm,  marker="s", linestyle="--", label=r"$G(\alpha)$")
+    plt.plot(alpha, Gm, marker="s", linestyle="--", label=r"$G(\alpha)$")
     plt.xlabel(r"$\alpha$")
     plt.ylabel("Valeur")
     plt.title("Séries $F(\\alpha)-1$ et $G(\\alpha)$")
@@ -47,5 +48,6 @@ def main():
     plt.close()
     print(f"Figure enregistrée → {OUT_PLOT}")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter02/plot_fig06_alpha_fit.py b/zz-scripts/chapter02/plot_fig06_alpha_fit.py
index 4dd6f8a..e3a14f2 100755
--- a/zz-scripts/chapter02/plot_fig06_alpha_fit.py
+++ b/zz-scripts/chapter02/plot_fig06_alpha_fit.py
@@ -9,48 +9,49 @@ Données sources :
 - zz-data/chapter02/02_As_ns_vs_alpha.csv
 - zz-data/chapter02/02_primordial_spectrum_spec.json
 """
+
 import json
 from pathlib import Path

-import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt

 # Répertoires racine
-ROOT     = Path(__file__).resolve().parents[2]
-DATA_DIR = ROOT / "zz-data"   / "chapter02"
-FIG_DIR  = ROOT / "zz-figures"   / "chapter02"
-DATA_IN  = DATA_DIR / "02_As_ns_vs_alpha.csv"
-SPEC_JS  = DATA_DIR / "02_primordial_spectrum_spec.json"
-OUT_PLOT = FIG_DIR  / "fig_06_fit_alpha.png"
+ROOT = Path(__file__).resolve().parents[2]
+DATA_DIR = ROOT / "zz-data" / "chapter02"
+FIG_DIR = ROOT / "zz-figures" / "chapter02"
+DATA_IN = DATA_DIR / "02_As_ns_vs_alpha.csv"
+SPEC_JS = DATA_DIR / "02_primordial_spectrum_spec.json"
+OUT_PLOT = FIG_DIR / "fig_06_fit_alpha.png"
+

 def main():
     # Lecture des données Brutes
-    df    = pd.read_csv(DATA_IN)
+    df = pd.read_csv(DATA_IN)
     alpha = df["alpha"].values
-    As    = df["A_s"].values
-    ns    = df["n_s"].values
+    As = df["A_s"].values
+    ns = df["n_s"].values

     # Lecture des coefficients depuis le JSON
-    spec   = json.loads(Path(SPEC_JS).read_text())
-    A_s0   = spec["constantes"]["A_s0"]
-    ns0    = spec["constantes"]["ns0"]
+    spec = json.loads(Path(SPEC_JS).read_text())
+    A_s0 = spec["constantes"]["A_s0"]
+    ns0 = spec["constantes"]["ns0"]
     coeffs = spec["coefficients"]
-    c1     = coeffs["c1"]
-    c1_2   = coeffs.get("c1_2", 0.0)
-    c2     = coeffs["c2"]
-    c2_2   = coeffs.get("c2_2", 0.0)
+    c1 = coeffs["c1"]
+    c1_2 = coeffs.get("c1_2", 0.0)
+    c2 = coeffs["c2"]
+    c2_2 = coeffs.get("c2_2", 0.0)

     # Calcul des courbes ajustées (ordre 2)
-    As_fit = A_s0 * (1 + c1*alpha + c1_2*alpha**2)
-    ns_fit = ns0 +     c2*alpha + c2_2*alpha**2
+    As_fit = A_s0 * (1 + c1 * alpha + c1_2 * alpha**2)
+    ns_fit = ns0 + c2 * alpha + c2_2 * alpha**2

     # Tracé
     plt.figure(figsize=(6, 6))

     # 1) A_s(α)
     ax1 = plt.subplot(2, 1, 1)
-    ax1.plot(alpha, As,    marker="o", linestyle="None", label="Données")
+    ax1.plot(alpha, As, marker="o", linestyle="None", label="Données")
     ax1.plot(alpha, As_fit, linestyle="-", linewidth=1.5, label="Fit ordre 2")
     ax1.set_ylabel(r"$A_s(\alpha)$")
     ax1.grid(True, which="both", ls=":")
@@ -58,7 +59,7 @@ def main():

     # 2) n_s(α)
     ax2 = plt.subplot(2, 1, 2)
-    ax2.plot(alpha, ns,    marker="s", linestyle="None", label="Données")
+    ax2.plot(alpha, ns, marker="s", linestyle="None", label="Données")
     ax2.plot(alpha, ns_fit, linestyle="-", linewidth=1.5, label="Fit ordre 2")
     ax2.set_xlabel(r"$\alpha$")
     ax2.set_ylabel(r"$n_s(\alpha)$")
@@ -74,5 +75,6 @@ def main():
     plt.close()
     print(f"Figure enregistrée → {OUT_PLOT}")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter02/primordial_spectrum.py b/zz-scripts/chapter02/primordial_spectrum.py
index f146395..b5bf025 100755
--- a/zz-scripts/chapter02/primordial_spectrum.py
+++ b/zz-scripts/chapter02/primordial_spectrum.py
@@ -17,7 +17,7 @@ SPEC_FILE = DATA_DIR / "spec_spectre.json"

 def P_R(k: np.ndarray, alpha: float) -> np.ndarray:
     """
-    Spectre primordial MCGT :
+    Spectre primordial MCGT :
         P_R(k; alpha) = A_s(alpha) * k^(n_s(alpha) - 1)

     où :
diff --git a/zz-scripts/chapter02/requirements.txt b/zz-scripts/chapter02/requirements.txt
index dd6ef43..05a284f 100755
--- a/zz-scripts/chapter02/requirements.txt
+++ b/zz-scripts/chapter02/requirements.txt
@@ -1,79 +1,79 @@
 # Exigences pour le Chapitre 2 (Validation chronologique)

 1. **Environnement système**
-   * Système : Ubuntu 20.04 (WSL ou natif) ou équivalent Linux
-   * Espace disque : ≥ 200 Mo libre
+   * Système : Ubuntu 20.04 (WSL ou natif) ou équivalent Linux
+   * Espace disque : ≥ 200 Mo libre

 2. **Environnement Python**
-   * Python ≥ 3.9
-   * Créer et activer un environnement virtuel spécifique :
-
-       cd ~/MCGT
-       python3 -m venv venv2
+   * Python ≥ 3.9
+   * Créer et activer un environnement virtuel spécifique :
+
+       cd ~/MCGT
+       python3 -m venv venv2
        source venv2/bin/activate
-
-   * Installer les dépendances :
-
-       pip install --upgrade pip
+
+   * Installer les dépendances :
+
+       pip install --upgrade pip
        pip install numpy>=1.20 pandas>=1.2 scipy>=1.6 matplotlib>=3.3 sympy nbconvert
-
+

 3. **Données brutes & scripts d’extraction**
-   * **Jalons bibliographiques**
-     – Fichier : `zz-data/chapter02/02_milestones_meta.csv`
-     – Colonnes : `T,P_ref,classe`
-   * **Extraction symbolique F/G**
-     – Notebook : `zz-scripts/chapter02/extract_sympy_FG.ipynb`
-       – Lit `02_As_ns_vs_alpha.csv` → produit `02_FG_series.csv`
-   * **Échantillonnage spectre**
-     – Script : `zz-scripts/chapter02/sampler_PR_grid.py`
-       → génère `zz-data/chapter02/02_P_R_sampling.csv`
-   * **Ajustement A_s, n_s**
-     – Script : `zz-scripts/chapter02/fit_As_ns.py` (ou équivalent)
-       → produit `zz-data/chapter02/02_As_ns_vs_alpha.csv`
+   * **Jalons bibliographiques**
+     – Fichier : `zz-data/chapter02/02_milestones_meta.csv`
+     – Colonnes : `T,P_ref,classe`
+   * **Extraction symbolique F/G**
+     – Notebook : `zz-scripts/chapter02/extract_sympy_FG.ipynb`
+       – Lit `02_As_ns_vs_alpha.csv` → produit `02_FG_series.csv`
+   * **Échantillonnage spectre**
+     – Script : `zz-scripts/chapter02/sampler_PR_grid.py`
+       → génère `zz-data/chapter02/02_P_R_sampling.csv`
+   * **Ajustement A_s, n_s**
+     – Script : `zz-scripts/chapter02/fit_As_ns.py` (ou équivalent)
+       → produit `zz-data/chapter02/02_As_ns_vs_alpha.csv`

 4. **Structure du dépôt**

-    MCGT/
-    ├─ 02-chronological-validation/     # sources LaTeX, GUIDE
-    ├─ zz-data/chapter02/               # milestones_meta.csv, .dat, .csv, .json générés
-    ├─ zz-scripts/chapter02/            # generate_data_chapter02.py, sampler_PR_grid.py, fit_As_ns.py, plot_fig00_spectrum.py, extract_sympy_FG.ipynb
-    ├─ zz-figures/chapter02/            # fig_00_spectrum.png, fig_01_…, fig_04…
+    MCGT/
+    ├─ 02-chronological-validation/     # sources LaTeX, GUIDE
+    ├─ zz-data/chapter02/               # milestones_meta.csv, .dat, .csv, .json générés
+    ├─ zz-scripts/chapter02/            # generate_data_chapter02.py, sampler_PR_grid.py, fit_As_ns.py, plot_fig00_spectrum.py, extract_sympy_FG.ipynb
+    ├─ zz-figures/chapter02/            # fig_00_spectrum.png, fig_01_…, fig_04…
     └─ venv2/                           # environnement virtuel Python
-
+

 5. **Exécution du pipeline**
-   1. *Génération et validation chronologique*
-
-       source venv2/bin/activate
+   1. *Génération et validation chronologique*
+
+       source venv2/bin/activate
        python zz-scripts/chapter02/generate_data_chapter02.py
-
-   2. *Spectre primordial (+ figure)*
-
+
+   2. *Spectre primordial (+ figure)*
+
        python zz-scripts/chapter02/generate_data_chapter02.py --spectre
-
-   3. *Échantillonnage P_R(k,α)*
-
+
+   3. *Échantillonnage P_R(k,α)*
+
        python zz-scripts/chapter02/sampler_PR_grid.py
-
-   4. *Ajustement A_s(α), n_s(α)*
-
-       jupyter nbconvert --execute --inplace zz-scripts/chapter02/extract_sympy_FG.ipynb
-       (ou)
+
+   4. *Ajustement A_s(α), n_s(α)*
+
+       jupyter nbconvert --execute --inplace zz-scripts/chapter02/extract_sympy_FG.ipynb
+       (ou)
        python zz-scripts/chapter02/fit_As_ns.py
-
-   5. *Tracé des figures additionnelles (F/G)*
-
-       python zz-scripts/chapter02/plot_fig05_FG_series.py
+
+   5. *Tracé des figures additionnelles (F/G)*
+
+       python zz-scripts/chapter02/plot_fig05_FG_series.py
        python zz-scripts/chapter02/plot_fig06_fit_alpha.py
-
+

 6. **Compilation LaTeX**
-
-    cd 02-chronological-validation/
-    pdflatex -interaction=nonstopmode 02_chronological_validation_conceptual.tex
+
+    cd 02-chronological-validation/
+    pdflatex -interaction=nonstopmode 02_chronological_validation_conceptual.tex
     pdflatex -interaction=nonstopmode 02_chronological_validation_details.tex
-
+

 7. **Tests & validations**

@@ -94,14 +94,14 @@
    * S’assurer qu’il n’y a pas de `NaN` ou `Inf` dans les CSV
    * Contrôler :

-     * `max|epsilon_primary| < 0.01` et `max|epsilon_order2| < 0.10` dans `02_optimal_parameters.json`
+     * `max|epsilon_primary| < 0.01` et `max|epsilon_order2| < 0.10` dans `02_optimal_parameters.json`
      * `A_s(0)=2.10e-9`, `n_s(0)=0.9649` dans `02_As_ns_vs_alpha.csv`
    * Ouvrir chaque figure PNG dans `zz-figures/chapter02/` pour un contrôle visuel :

-     * `fig_00_spectrum.png`
-     * `fig_01_P_vs_T_evolution.png`
-     * `fig_02_calibration.png`
-     * `fig_03_relative_errors.png`
-     * `fig_04_schema_pipeline.png`
-     * `fig_05_FG_series.png`
+     * `fig_00_spectrum.png`
+     * `fig_01_P_vs_T_evolution.png`
+     * `fig_02_calibration.png`
+     * `fig_03_relative_errors.png`
+     * `fig_04_schema_pipeline.png`
+     * `fig_05_FG_series.png`
      * `fig_06_fit_alpha.png`
diff --git a/zz-scripts/chapter03/generate_data_chapter03.py b/zz-scripts/chapter03/generate_data_chapter03.py
index 4ba9c24..ae75d84 100755
--- a/zz-scripts/chapter03/generate_data_chapter03.py
+++ b/zz-scripts/chapter03/generate_data_chapter03.py
@@ -5,7 +5,7 @@
 Chapitre 3 – Pipeline intégral (v3.2.0)
 --------------------------------------
 Stabilité de f(R) : génération des données numériques pour les figures
-et les tableaux du Chapitre 3.
+et les tableaux du Chapitre 3.

 """

@@ -40,17 +40,19 @@ log = logging.getLogger(__name__)
 # 2. Cosmologie : inversion T↔z
 # ----------------------------------------------------------------------
 H0_km_s_Mpc = 67.66
-Mpc_to_km   = 3.0856775814913673e19  # km dans 1 Mpc
-sec_per_Gyr = 3.1536e16             # s dans 1 Gyr
-H0 = H0_km_s_Mpc / Mpc_to_km * sec_per_Gyr   # ≈0.069 Gyr⁻¹
+Mpc_to_km = 3.0856775814913673e19  # km dans 1 Mpc
+sec_per_Gyr = 3.1536e16  # s dans 1 Gyr
+H0 = H0_km_s_Mpc / Mpc_to_km * sec_per_Gyr  # ≈0.069 Gyr⁻¹
 Om0, Ol0 = 0.3111, 0.6889

+
 def T_of_z(z: float) -> float:
     """Âge de l’Univers (Gyr) à redshift z dans un ΛCDM plat."""
-    integrand = lambda zp: 1/((1+zp)*H0*np.sqrt(Om0*(1+zp)**3 + Ol0))
+    integrand = lambda zp: 1 / ((1 + zp) * H0 * np.sqrt(Om0 * (1 + zp) ** 3 + Ol0))
     T, _ = quad(integrand, z, 1e5)
     return T

+
 def z_of_T(T: float) -> float:
     """Inverse de T_of_z; si T≥T0 renvoie 0."""
     T0 = T_of_z(0.0)
@@ -59,31 +61,34 @@ def z_of_T(T: float) -> float:
     # approximation à petit T
     thr = 1e-2
     if T < thr:
-        return max(((2/(3*H0*np.sqrt(Om0)))/T)**(2/3) - 1, 0.0)
+        return max(((2 / (3 * H0 * np.sqrt(Om0))) / T) ** (2 / 3) - 1, 0.0)
     # sinon root-finding
     f = lambda z: T_of_z(z) - T
     zmax = 1e6
-    if f(0)*f(zmax) > 0:
+    if f(0) * f(zmax) > 0:
         zmax *= 10
     return brentq(f, 0.0, zmax)

+
 # ----------------------------------------------------------------------
 # 3. Outils partagés (grille log-lin)
 # ----------------------------------------------------------------------
 def build_loglin_grid(fmin: float, fmax: float, dlog: float) -> np.ndarray:
     if fmin <= 0 or fmax <= 0 or fmax <= fmin:
         raise ValueError("fmin>0, fmax>fmin requis.")
-    n = int(np.floor((np.log10(fmax)-np.log10(fmin))/dlog)) + 1
-    return 10 ** (np.log10(fmin) + np.arange(n)*dlog)
+    n = int(np.floor((np.log10(fmax) - np.log10(fmin)) / dlog)) + 1
+    return 10 ** (np.log10(fmin) + np.arange(n) * dlog)

-def check_log_spacing(g: np.ndarray, atol: float=1e-12) -> bool:
+
+def check_log_spacing(g: np.ndarray, atol: float = 1e-12) -> bool:
     d = np.diff(np.log10(g))
     return np.allclose(d, d[0], atol=atol)

+
 # ----------------------------------------------------------------------
 # 4. Jalons : copie si besoin
 # ----------------------------------------------------------------------
-def ensure_jalons(src: Path|None) -> Path:
+def ensure_jalons(src: Path | None) -> Path:
     dst = Path("zz-data") / "chapter03" / "03_ricci_fR_milestones.csv"
     dst.parent.mkdir(parents=True, exist_ok=True)
     if dst.exists():
@@ -95,20 +100,20 @@ def ensure_jalons(src: Path|None) -> Path:
     log.info("Jalons copiés → %s", dst)
     return dst

+
 # ----------------------------------------------------------------------
 # 5. CLI & lecture INI
 # ----------------------------------------------------------------------
 def parse_cli() -> argparse.Namespace:
-    p = argparse.ArgumentParser(
-        description="Génère les données du Chapitre 3."
-    )
+    p = argparse.ArgumentParser(description="Génère les données du Chapitre 3.")
     p.add_argument("--config", default="gw_phase.ini", help="INI avec [scan]")
     p.add_argument("--npts", type=int, help="nombre de points fixe")
     p.add_argument("--copy-jalons", help="chemin vers jalons si absent")
     p.add_argument("--dry-run", action="store_true", help="ne pas écrire")
     return p.parse_args()

-def read_scan_section(path: Path) -> tuple[float,float,float]:
+
+def read_scan_section(path: Path) -> tuple[float, float, float]:
     cfg = configparser.ConfigParser()
     if not cfg.read(path) or "scan" not in cfg:
         log.error("Impossible de lire la section [scan] de %s", path)
@@ -120,6 +125,7 @@ def read_scan_section(path: Path) -> tuple[float,float,float]:
         log.error("Valeurs invalides dans [scan] : %s", e)
         sys.exit(1)

+
 # ----------------------------------------------------------------------
 # 6. Construction des grilles T, z et R
 # ----------------------------------------------------------------------
@@ -146,39 +152,38 @@ def build_T_z_R_grids(fmin: float, fmax: float, dlog: float, npts: int | None):

     log.info(
         "Grille R/R₀ unique prête : %d points (%.3e → %.3e).",
-        R_unique.size, R_unique.min(), R_unique.max()
+        R_unique.size,
+        R_unique.min(),
+        R_unique.max(),
     )
     return freqs, T_grid, z_grid, R_unique

+
 # ----------------------------------------------------------------------
 # 7. Calcul de stabilité
 # ----------------------------------------------------------------------
 def calculer_stabilite(jalons: pd.DataFrame, Rgrid: np.ndarray):
     logRj = np.log10(jalons["R_over_R0"])
-    fR_i  = PchipInterpolator(logRj, np.log10(jalons["f_R"]),  extrapolate=True)
+    fR_i = PchipInterpolator(logRj, np.log10(jalons["f_R"]), extrapolate=True)
     fRR_i = PchipInterpolator(logRj, np.log10(jalons["f_RR"]), extrapolate=True)

     logRg = np.log10(Rgrid)
-    fRg  = 10**fR_i(logRg)
-    fRRg = 10**fRR_i(logRg)
-    ms2  = (fRg - Rgrid*fRRg) / (3*fRRg)
-
-    df = pd.DataFrame({
-        "R_over_R0":  Rgrid,
-        "f_R":        fRg,
-        "f_RR":       fRRg,
-        "m_s2_over_R0": ms2
-    })
-    dom = pd.DataFrame({
-        "beta":      Rgrid,
-        "gamma_min": np.zeros_like(ms2),
-        "gamma_max": ms2.clip(max=1e8)
-    })
+    fRg = 10 ** fR_i(logRg)
+    fRRg = 10 ** fRR_i(logRg)
+    ms2 = (fRg - Rgrid * fRRg) / (3 * fRRg)
+
+    df = pd.DataFrame(
+        {"R_over_R0": Rgrid, "f_R": fRg, "f_RR": fRRg, "m_s2_over_R0": ms2}
+    )
+    dom = pd.DataFrame(
+        {"beta": Rgrid, "gamma_min": np.zeros_like(ms2), "gamma_max": ms2.clip(max=1e8)}
+    )
     frt = dom.query("gamma_min==gamma_max").rename(
-        columns={"gamma_max":"gamma_limit"}
-    )[['beta','gamma_limit']]
+        columns={"gamma_max": "gamma_limit"}
+    )[["beta", "gamma_limit"]]
     return df, dom, frt

+
 # ----------------------------------------------------------------------
 # 8. Exports CSV & métadonnées
 # ----------------------------------------------------------------------
@@ -196,12 +201,13 @@ def exporter_csv(df: pd.DataFrame, dom: pd.DataFrame, frt: pd.DataFrame, dry: bo
         "files": [
             "03_fR_stability_data.csv",
             "03_fR_stability_domain.csv",
-            "03_fR_stability_boundary.csv"
-        ]
+            "03_fR_stability_boundary.csv",
+        ],
     }
     (out / "03_fR_stability_meta.json").write_text(json.dumps(meta, indent=2))
     log.info("Données principales et métadonnées écrites.")

+
 # ----------------------------------------------------------------------
 # 9. Génération des fichiers R ↔ z et R ↔ T  (section remise à jour)
 # ----------------------------------------------------------------------
@@ -239,13 +245,12 @@ def exporter_jalons_inverses(
     p_z = PchipInterpolator(
         np.log10(df_z["R_over_R0"]),
         df_z["z"],
-        extrapolate=False,            # <- évite les z = 0 parasites
+        extrapolate=False,  # <- évite les z = 0 parasites
     )

     jal_z = jalons.copy()
-    mask_in = (
-        (jal_z["R_over_R0"] >= df_z["R_over_R0"].min())
-        & (jal_z["R_over_R0"] <= df_z["R_over_R0"].max())
+    mask_in = (jal_z["R_over_R0"] >= df_z["R_over_R0"].min()) & (
+        jal_z["R_over_R0"] <= df_z["R_over_R0"].max()
     )
     jal_z.loc[mask_in, "z"] = p_z(np.log10(jal_z.loc[mask_in, "R_over_R0"]))

@@ -255,24 +260,20 @@ def exporter_jalons_inverses(
     # ------------------------------------------------------------------
     # 9-A  Interpolation R → z  (monotone, sans extrapolation)
     # ------------------------------------------------------------------
-    df_zfull = pd.DataFrame({
-        "R_over_R0": df_R["R_over_R0"],
-        "z":         zgrid
-    }).query("z > 0")                                   # z strictement positifs
+    df_zfull = pd.DataFrame({"R_over_R0": df_R["R_over_R0"], "z": zgrid}).query(
+        "z > 0"
+    )  # z strictement positifs

     # on garde, pour chaque R, le z le plus grand (plus ancien)
     df_zfull.sort_values(["R_over_R0", "z"], ascending=[True, False], inplace=True)
     df_z = df_zfull.drop_duplicates("R_over_R0").sort_values("R_over_R0")

-    p_z = PchipInterpolator(
-        np.log10(df_z["R_over_R0"]),
-        df_z["z"],
-        extrapolate=False
-    )
+    p_z = PchipInterpolator(np.log10(df_z["R_over_R0"]), df_z["z"], extrapolate=False)

     jal_z = jalons.copy()
-    in_domain = jal_z["R_over_R0"].between(df_z["R_over_R0"].min(),
-                                           df_z["R_over_R0"].max())
+    in_domain = jal_z["R_over_R0"].between(
+        df_z["R_over_R0"].min(), df_z["R_over_R0"].max()
+    )
     jal_z.loc[in_domain, "z"] = p_z(np.log10(jal_z.loc[in_domain, "R_over_R0"]))

     # on ne garde que les jalons réellement interpolés
@@ -296,10 +297,10 @@ def exporter_jalons_inverses(
     p_T = PchipInterpolator(
         np.log10(df_T["R_over_R0"]),
         np.log10(df_T["T_Gyr"]),
-        extrapolate=True,             # extrapolation OK pour T
+        extrapolate=True,  # extrapolation OK pour T
     )

-    jal_T = jal_z.copy()             # même sous-ensemble que pour z
+    jal_T = jal_z.copy()  # même sous-ensemble que pour z
     jal_T["T_Gyr"] = 10 ** p_T(np.log10(jal_T["R_over_R0"]))

     # Assurer la décroissance de T (le passé ne doit pas dépasser le présent)
@@ -320,16 +321,16 @@ def main() -> None:
     fmin, fmax, dlog = read_scan_section(Path(args.config))

     # 10.1 prépare toutes les grilles
-    freqs, Tgrid, zgrid, Rgrid = build_T_z_R_grids(fmin,fmax,dlog,args.npts)
+    freqs, Tgrid, zgrid, Rgrid = build_T_z_R_grids(fmin, fmax, dlog, args.npts)

     # 10.2 charge les jalons
     jalon_path = ensure_jalons(Path(args.copy_jalons) if args.copy_jalons else None)
     jalons = (
         pd.read_csv(jalon_path)
-          .rename(columns=str.strip)
-          .query("R_over_R0>0")
-          .drop_duplicates("R_over_R0")
-          .sort_values("R_over_R0")
+        .rename(columns=str.strip)
+        .query("R_over_R0>0")
+        .drop_duplicates("R_over_R0")
+        .sort_values("R_over_R0")
     )

     # 10.3 calcul de stabilité
@@ -343,5 +344,6 @@ def main() -> None:

     log.info("Pipeline Chapitre 3 terminé.")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter03/plot_fig01_fR_stability_domain.py b/zz-scripts/chapter03/plot_fig01_fR_stability_domain.py
index 8c266b2..f7a655e 100755
--- a/zz-scripts/chapter03/plot_fig01_fR_stability_domain.py
+++ b/zz-scripts/chapter03/plot_fig01_fR_stability_domain.py
@@ -18,10 +18,8 @@ Sortie :
 from pathlib import Path
 import logging

-import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt
-from matplotlib.ticker import FixedLocator, FuncFormatter

 # ----------------------------------------------------------------------
 # Configuration logging
@@ -33,8 +31,8 @@ log = logging.getLogger(__name__)
 # Chemins
 # ----------------------------------------------------------------------
 DATA_FILE = Path("zz-data") / "chapter03" / "03_fR_stability_domain.csv"
-FIG_DIR   = Path("zz-figures") / "chapter03"
-FIG_PATH  = FIG_DIR / "fig_01_fR_stability_domain.png"
+FIG_DIR = Path("zz-figures") / "chapter03"
+FIG_PATH = FIG_DIR / "fig_01_fR_stability_domain.png"


 def main() -> None:
@@ -62,17 +60,11 @@ def main() -> None:
         df["gamma_max"],
         color="lightgray",
         alpha=0.5,
-        label="Domaine de stabilité"
+        label="Domaine de stabilité",
     )

     # Repère β = 1
-    ax.axvline(
-        1.0,
-        color="gray",
-        linestyle="--",
-        linewidth=1.0,
-        label=r"$\beta = 1$"
-    )
+    ax.axvline(1.0, color="gray", linestyle="--", linewidth=1.0, label=r"$\beta = 1$")

     # Échelles log-log
     ax.set_xscale("log")
@@ -91,23 +83,25 @@ def main() -> None:
     # ------------------------------------------------------------------
     mask = (df["beta"] >= 0.5) & (df["beta"] <= 2.0)
     if mask.any():
-        ax_in = fig.add_axes([0.60, 0.30, 0.35, 0.35])
+        ax_in = fig.add_axes([0.60, 0.30, 0.35, 0.35])

         # Tracé γ_max (et γ_min si ≠0)
         ax_in.plot(
-            df.loc[mask, "beta"],
-            df.loc[mask, "gamma_max"],
-            color="black", lw=1.2
+            df.loc[mask, "beta"], df.loc[mask, "gamma_max"], color="black", lw=1.2
         )
         if (df.loc[mask, "gamma_min"] > 0).any():
             ax_in.plot(
-                df.loc[mask, "beta"],
-                df.loc[mask, "gamma_min"],
-                color="black", lw=1.2
+                df.loc[mask, "beta"], df.loc[mask, "gamma_min"], color="black", lw=1.2
             )

         # Échelles : X linéaire, Y log
-        from matplotlib.ticker import FixedLocator, FuncFormatter, LogLocator, ScalarFormatter, NullFormatter
+        from matplotlib.ticker import (
+            FixedLocator,
+            FuncFormatter,
+            LogLocator,
+            ScalarFormatter,
+            NullFormatter,
+        )

         ax_in.set_xscale("linear")
         ax_in.set_xlim(0.5, 2.0)
diff --git a/zz-scripts/chapter03/plot_fig02_fR_fRR_vs_f.py b/zz-scripts/chapter03/plot_fig02_fR_fRR_vs_f.py
index f1239c9..a0c1db2 100755
--- a/zz-scripts/chapter03/plot_fig02_fR_fRR_vs_f.py
+++ b/zz-scripts/chapter03/plot_fig02_fR_fRR_vs_f.py
@@ -30,8 +30,9 @@ log = logging.getLogger(__name__)
 # Chemins
 # ----------------------------------------------------------------------
 DATA_FILE = Path("zz-data") / "chapter03" / "03_fR_stability_data.csv"
-FIG_DIR   = Path("zz-figures") / "chapter03"
-FIG_PATH  = FIG_DIR / "fig_02_fR_fRR_vs_R.png"
+FIG_DIR = Path("zz-figures") / "chapter03"
+FIG_PATH = FIG_DIR / "fig_02_fR_fRR_vs_R.png"
+

 # ----------------------------------------------------------------------
 # Main
@@ -54,8 +55,10 @@ def main() -> None:

     # 3. Graphique principal
     fig, ax = plt.subplots(dpi=300, figsize=(6, 4))
-    ax.loglog(df["R_over_R0"], df["f_R"],  color="tab:blue",  lw=1.5, label=r"$f_R(R)$")
-    ax.loglog(df["R_over_R0"], df["f_RR"], color="tab:orange", lw=1.5, label=r"$f_{RR}(R)$")
+    ax.loglog(df["R_over_R0"], df["f_R"], color="tab:blue", lw=1.5, label=r"$f_R(R)$")
+    ax.loglog(
+        df["R_over_R0"], df["f_RR"], color="tab:orange", lw=1.5, label=r"$f_{RR}(R)$"
+    )

     ax.set_xlabel(r"$R/R_0$")
     ax.set_ylabel(r"$f_R,\;f_{RR}$")
@@ -64,10 +67,7 @@ def main() -> None:

     # 4. Légende à mi-hauteur complètement à gauche
     ax.legend(
-        loc="center left",
-        bbox_to_anchor=(0.01, 0.5),
-        framealpha=0.8,
-        edgecolor="black"
+        loc="center left", bbox_to_anchor=(0.01, 0.5), framealpha=0.8, edgecolor="black"
     )

     # 5. Inset zoom sur f_RR (premiers 50 points)
@@ -82,7 +82,10 @@ def main() -> None:
     ax_in.set_xlim(df_zoom["R_over_R0"].min(), df_zoom["R_over_R0"].max())

     # graduations x (4 points logarithmiques)
-    lmin, lmax = np.log10(df_zoom["R_over_R0"].min()), np.log10(df_zoom["R_over_R0"].max())
+    lmin, lmax = (
+        np.log10(df_zoom["R_over_R0"].min()),
+        np.log10(df_zoom["R_over_R0"].max()),
+    )
     xticks = 10 ** np.linspace(lmin, lmax, 4)
     ax_in.xaxis.set_major_locator(FixedLocator(xticks))
     ax_in.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{x:.0f}"))
diff --git a/zz-scripts/chapter03/plot_fig03_ms2_R0_vs_f.py b/zz-scripts/chapter03/plot_fig03_ms2_R0_vs_f.py
index 76f41c6..bdee59e 100755
--- a/zz-scripts/chapter03/plot_fig03_ms2_R0_vs_f.py
+++ b/zz-scripts/chapter03/plot_fig03_ms2_R0_vs_f.py
@@ -18,9 +18,6 @@ import logging

 import pandas as pd
 import matplotlib.pyplot as plt
-from matplotlib.ticker import (
-    LogLocator, FuncFormatter, NullLocator, ScalarFormatter
-)

 # ----------------------------------------------------------------------
 # Logging
@@ -32,8 +29,9 @@ log = logging.getLogger(__name__)
 # Chemins
 # ----------------------------------------------------------------------
 DATA_FILE = Path("zz-data") / "chapter03" / "03_fR_stability_data.csv"
-FIG_DIR   = Path("zz-figures") / "chapter03"
-FIG_PATH  = FIG_DIR / "fig_03_ms2_R0_vs_R.png"
+FIG_DIR = Path("zz-figures") / "chapter03"
+FIG_PATH = FIG_DIR / "fig_03_ms2_R0_vs_R.png"
+

 def main() -> None:
     # 1. Chargement
@@ -53,7 +51,9 @@ def main() -> None:
     ax.loglog(
         df["R_over_R0"],
         df["m_s2_over_R0"],
-        color="tab:blue", lw=1.5, label=r"$m_s^2/R_0$"
+        color="tab:blue",
+        lw=1.5,
+        label=r"$m_s^2/R_0$",
     )
     ax.set_xlabel(r"$R/R_0$")
     ax.set_ylabel(r"$m_s^{2}/R_0$")
@@ -69,17 +69,13 @@ def main() -> None:

         # tracé
         ax_in.loglog(
-            df_zoom["R_over_R0"],
-            df_zoom["m_s2_over_R0"],
-            color="tab:blue",
-            lw=1.2
+            df_zoom["R_over_R0"], df_zoom["m_s2_over_R0"], color="tab:blue", lw=1.2
         )

         # limites
         ax_in.set_xlim(1e4, 1e6)
         ax_in.set_ylim(
-            df_zoom["m_s2_over_R0"].min() * 0.9,
-            df_zoom["m_s2_over_R0"].max() * 1.1
+            df_zoom["m_s2_over_R0"].min() * 0.9, df_zoom["m_s2_over_R0"].max() * 1.1
         )

         # Graduations X : 3 points fixes [1e4,1e5,1e6]
@@ -97,7 +93,7 @@ def main() -> None:
         ax_in.yaxis.set_major_locator(LogLocator(base=10, numticks=4))
         ax_in.yaxis.set_minor_locator(NullLocator())
         sf = ScalarFormatter(useMathText=True)
-        sf.set_scientific(False)      # supprime le ×10ⁿ
+        sf.set_scientific(False)  # supprime le ×10ⁿ
         ax_in.yaxis.set_major_formatter(sf)
         ax_in.tick_params(axis="y", which="major", pad=2)

@@ -110,5 +106,6 @@ def main() -> None:
     plt.close(fig)
     log.info("Figure enregistrée → %s", FIG_PATH)

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter03/plot_fig04_fR_fRR_vs_f.py b/zz-scripts/chapter03/plot_fig04_fR_fRR_vs_f.py
index f3f9af2..3f3ea58 100755
--- a/zz-scripts/chapter03/plot_fig04_fR_fRR_vs_f.py
+++ b/zz-scripts/chapter03/plot_fig04_fR_fRR_vs_f.py
@@ -33,8 +33,9 @@ log = logging.getLogger(__name__)
 # Chemins
 # ----------------------------------------------------------------------
 DATA_FILE = Path("zz-data") / "chapter03" / "03_fR_stability_data.csv"
-FIG_DIR   = Path("zz-figures") / "chapter03"
-FIG_PATH  = FIG_DIR / "fig_04_fR_fRR_vs_R.png"
+FIG_DIR = Path("zz-figures") / "chapter03"
+FIG_PATH = FIG_DIR / "fig_04_fR_fRR_vs_R.png"
+

 # ----------------------------------------------------------------------
 # Main
@@ -56,7 +57,7 @@ def main() -> None:
     FIG_DIR.mkdir(parents=True, exist_ok=True)

     # 3. Création de la figure
-    fig, ax1 = plt.subplots(dpi=300, figsize=(6,4))
+    fig, ax1 = plt.subplots(dpi=300, figsize=(6, 4))
     fig.suptitle(r"$f_R$ et $f_{RR}$ en fonction de $R/R_0$ (double axe)", y=0.98)

     # axe X en log
@@ -64,9 +65,8 @@ def main() -> None:
     ax1.set_xlabel(r"$R/R_0$")

     # 4. Tracé de f_R sur l'axe de gauche
-    ln1, = ax1.loglog(
-        df["R_over_R0"], df["f_R"],
-        color="tab:blue", lw=1.5, label=r"$f_R(R)$"
+    (ln1,) = ax1.loglog(
+        df["R_over_R0"], df["f_R"], color="tab:blue", lw=1.5, label=r"$f_R(R)$"
     )
     ax1.set_ylabel(r"$f_R$", color="tab:blue")
     ax1.tick_params(axis="y", labelcolor="tab:blue")
@@ -75,9 +75,8 @@ def main() -> None:
     # 5. Tracé de f_RR sur l'axe de droite
     ax2 = ax1.twinx()
     ax2.set_yscale("log")
-    ln2, = ax2.loglog(
-        df["R_over_R0"], df["f_RR"],
-        color="tab:orange", lw=1.5, label=r"$f_{RR}(R)$"
+    (ln2,) = ax2.loglog(
+        df["R_over_R0"], df["f_RR"], color="tab:orange", lw=1.5, label=r"$f_{RR}(R)$"
     )
     ax2.set_ylabel(r"$f_{RR}$", color="tab:orange")
     ax2.tick_params(axis="y", labelcolor="tab:orange")
@@ -89,13 +88,14 @@ def main() -> None:

     # 7. Légende explicite
     handles = [ln1, ln2, ln3]
-    labels  = [h.get_label() for h in handles]
+    labels = [h.get_label() for h in handles]
     ax1.legend(
-        handles, labels,
+        handles,
+        labels,
         loc="upper left",
-        bbox_to_anchor=(0.25, 0.50),
+        bbox_to_anchor=(0.25, 0.50),
         framealpha=0.9,
-        edgecolor="black"
+        edgecolor="black",
     )

     # 8. Mise en forme finale et sauvegarde
diff --git a/zz-scripts/chapter03/plot_fig05_interpolated_milestones.py b/zz-scripts/chapter03/plot_fig05_interpolated_milestones.py
index 664bd60..bec566c 100755
--- a/zz-scripts/chapter03/plot_fig05_interpolated_milestones.py
+++ b/zz-scripts/chapter03/plot_fig05_interpolated_milestones.py
@@ -33,11 +33,12 @@ log = logging.getLogger(__name__)
 # ----------------------------------------------------------------------
 # Chemins
 # ----------------------------------------------------------------------
-DATA_DIR  = Path("zz-data") / "chapter03"
-RAW_FILE  = DATA_DIR / "03_ricci_fR_milestones.csv"
+DATA_DIR = Path("zz-data") / "chapter03"
+RAW_FILE = DATA_DIR / "03_ricci_fR_milestones.csv"
 GRID_FILE = DATA_DIR / "03_fR_stability_data.csv"
-FIG_DIR   = Path("zz-figures") / "chapter03"
-FIG_PATH  = FIG_DIR / "fig_05_interpolated_milestones.png"
+FIG_DIR = Path("zz-figures") / "chapter03"
+FIG_PATH = FIG_DIR / "fig_05_interpolated_milestones.png"
+

 def main() -> None:
     # 1. Lecture des données
@@ -46,7 +47,7 @@ def main() -> None:
         return

     jalons = pd.read_csv(RAW_FILE)
-    grid   = pd.read_csv(GRID_FILE)
+    grid = pd.read_csv(GRID_FILE)

     # On garde seulement R>0 pour log–log
     jalons = jalons[jalons["R_over_R0"] > 0].sort_values("R_over_R0")
@@ -59,30 +60,31 @@ def main() -> None:

     # 3. Construire l’interpolateur PCHIP sur log10(R/R0)
     logR_j = np.log10(jalons["R_over_R0"].values)
-    p_fR   = PchipInterpolator(logR_j, np.log10(jalons["f_R"].values),  extrapolate=True)
-    p_fRR  = PchipInterpolator(logR_j, np.log10(jalons["f_RR"].values), extrapolate=True)
+    p_fR = PchipInterpolator(logR_j, np.log10(jalons["f_R"].values), extrapolate=True)
+    p_fRR = PchipInterpolator(logR_j, np.log10(jalons["f_RR"].values), extrapolate=True)

     # 4. Grille dense en R/R0 pour tracer la courbe lisse
     logR_min, logR_max = logR_j.min(), logR_j.max()
     logR_dense = np.linspace(logR_min, logR_max, 400)
-    R_dense    = 10**logR_dense
-    fR_dense   = 10**p_fR(logR_dense)
-    fRR_dense  = 10**p_fRR(logR_dense)
+    R_dense = 10**logR_dense
+    fR_dense = 10 ** p_fR(logR_dense)
+    fRR_dense = 10 ** p_fRR(logR_dense)

     # 5. Tracé
-    fig, ax1 = plt.subplots(dpi=300, figsize=(6,4))
+    fig, ax1 = plt.subplots(dpi=300, figsize=(6, 4))

     #   5a. Courbe PCHIP fR
     color1 = "tab:blue"
-    ax1.plot(
-        R_dense, fR_dense,
-        color=color1, lw=1.5, label=r"PCHIP $f_R$"
-    )
+    ax1.plot(R_dense, fR_dense, color=color1, lw=1.5, label=r"PCHIP $f_R$")
     #   5b. Points jalons fR
     ax1.scatter(
-        jalons["R_over_R0"], jalons["f_R"],
-        c=color1, marker="o", s=40, alpha=0.8,
-        label=r"Jalons $f_R$"
+        jalons["R_over_R0"],
+        jalons["f_R"],
+        c=color1,
+        marker="o",
+        s=40,
+        alpha=0.8,
+        label=r"Jalons $f_R$",
     )

     ax1.set_xscale("log")
@@ -96,14 +98,22 @@ def main() -> None:
     ax2 = ax1.twinx()
     color2 = "tab:orange"
     ax2.plot(
-        R_dense, fRR_dense,
-        color=color2, lw=1.5, linestyle="--", label=r"PCHIP $f_{RR}$"
+        R_dense,
+        fRR_dense,
+        color=color2,
+        lw=1.5,
+        linestyle="--",
+        label=r"PCHIP $f_{RR}$",
     )
     #   5d. Points jalons fRR
     ax2.scatter(
-        jalons["R_over_R0"], jalons["f_RR"],
-        c=color2, marker="s", s=50, alpha=0.8,
-        label=r"Jalons $f_{RR}$"
+        jalons["R_over_R0"],
+        jalons["f_RR"],
+        c=color2,
+        marker="s",
+        s=50,
+        alpha=0.8,
+        label=r"Jalons $f_{RR}$",
     )
     ax2.set_yscale("log")
     ax2.set_ylabel(r"$f_{RR}$", color=color2)
@@ -112,12 +122,7 @@ def main() -> None:
     # 6. Légende commune
     h1, l1 = ax1.get_legend_handles_labels()
     h2, l2 = ax2.get_legend_handles_labels()
-    ax1.legend(
-        h1 + h2, l1 + l2,
-        loc="best",
-        framealpha=0.8,
-        edgecolor="black"
-    )
+    ax1.legend(h1 + h2, l1 + l2, loc="best", framealpha=0.8, edgecolor="black")

     # 7. Titre
     ax1.set_title("Interpolation PCHIP vs points de jalons")
@@ -128,5 +133,6 @@ def main() -> None:
     plt.close(fig)
     log.info("Figure enregistrée → %s", FIG_PATH)

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter03/plot_fig06_grid_quality.py b/zz-scripts/chapter03/plot_fig06_grid_quality.py
index ca69616..6ff9092 100755
--- a/zz-scripts/chapter03/plot_fig06_grid_quality.py
+++ b/zz-scripts/chapter03/plot_fig06_grid_quality.py
@@ -35,8 +35,9 @@ log = logging.getLogger(__name__)
 # Chemins
 # ----------------------------------------------------------------------
 DATA_FILE = Path("zz-data") / "chapter03" / "03_fR_stability_data.csv"
-FIG_DIR   = Path("zz-figures") / "chapter03"
-FIG_PATH  = FIG_DIR / "fig_06_grid_quality.png"
+FIG_DIR = Path("zz-figures") / "chapter03"
+FIG_PATH = FIG_DIR / "fig_06_grid_quality.png"
+

 def main() -> None:
     # 1. Vérification du fichier de données
@@ -52,7 +53,7 @@ def main() -> None:
     grid = df["R_over_R0"].values

     # 3. Calcul des différences en log₁₀
-    logg  = np.log10(grid)
+    logg = np.log10(grid)
     diffs = np.diff(logg)

     # 4. Calcul du pas théorique
@@ -60,18 +61,18 @@ def main() -> None:
     dlog_th = (np.log10(grid[-1]) - np.log10(grid[0])) / (N - 1)

     # 5. Extraction des indices à tracer (on skippe les 50 premiers)
-    N          = len(grid)
-    idx_full   = np.arange(1, N)       # diffs[i] = logg[i+1] - logg[i]
+    N = len(grid)
+    idx_full = np.arange(1, N)  # diffs[i] = logg[i+1] - logg[i]
     diffs_full = diffs

-    mask_idx   = idx_full >= 50        # on ne garde que idx = 50…N−1
-    idx_plot   = idx_full[mask_idx]
+    mask_idx = idx_full >= 50  # on ne garde que idx = 50…N−1
+    idx_plot = idx_full[mask_idx]
     diffs_plot = diffs_full[mask_idx]

     # index local du saut le plus élevé :
     if diffs_plot.size > 0:
         i_bad = np.argmax(diffs_plot)
-        idx_plot   = np.delete(idx_plot,   i_bad)
+        idx_plot = np.delete(idx_plot, i_bad)
         diffs_plot = np.delete(diffs_plot, i_bad)

     # 6. Tracé
@@ -82,7 +83,7 @@ def main() -> None:
         marker="o",
         linestyle="-",
         markersize=3,
-        label="Grille R↔z uniforme"
+        label="Grille R↔z uniforme",
     )

     # ligne du pas théorique
@@ -92,7 +93,7 @@ def main() -> None:
         color="red",
         linestyle="--",
         linewidth=1.2,
-        label=rf"Pas théorique $\Delta\log_{{10}}={dlog_th:.3e}$"
+        label=rf"Pas théorique $\Delta\log_{{10}}={dlog_th:.3e}$",
     )

     ax.set_xlabel("Index de la grille")
@@ -108,5 +109,6 @@ def main() -> None:

     log.info("Figure enregistrée → %s", FIG_PATH)

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter03/plot_fig07_ricci_fR_vs_z.py b/zz-scripts/chapter03/plot_fig07_ricci_fR_vs_z.py
index 18fb349..328b6f4 100755
--- a/zz-scripts/chapter03/plot_fig07_ricci_fR_vs_z.py
+++ b/zz-scripts/chapter03/plot_fig07_ricci_fR_vs_z.py
@@ -30,8 +30,9 @@ log = logging.getLogger(__name__)
 # Chemins
 # ----------------------------------------------------------------------
 DATA_FILE = Path("zz-data") / "chapter03" / "03_ricci_fR_vs_z.csv"
-FIG_DIR   = Path("zz-figures") / "chapter03"
-FIG_PATH  = FIG_DIR / "fig_07_ricci_fR_vs_z.png"
+FIG_DIR = Path("zz-figures") / "chapter03"
+FIG_PATH = FIG_DIR / "fig_07_ricci_fR_vs_z.png"
+

 # ----------------------------------------------------------------------
 # Main
@@ -63,14 +64,15 @@ def main() -> None:

     # 4. Tracer f_R sur l'axe de gauche
     ax1.scatter(
-        df["z"], df["f_R"],
-        color="tab:blue", marker="o", s=40, alpha=0.8,
-        label=r"$f_R$"
-    )
-    ax1.plot(
-        df["z"], df["f_R"],
-        color="tab:blue", lw=1, alpha=0.6
+        df["z"],
+        df["f_R"],
+        color="tab:blue",
+        marker="o",
+        s=40,
+        alpha=0.8,
+        label=r"$f_R$",
     )
+    ax1.plot(df["z"], df["f_R"], color="tab:blue", lw=1, alpha=0.6)
     ax1.set_ylabel(r"$f_R$", color="tab:blue")
     ax1.tick_params(axis="y", colors="tab:blue")
     ax1.set_yscale("log")
@@ -79,14 +81,15 @@ def main() -> None:
     # 5. Tracer f_RR sur un second axe de droite
     ax2 = ax1.twinx()
     ax2.scatter(
-        df["z"], df["f_RR"],
-        color="tab:orange", marker="s", s=40, alpha=0.8,
-        label=r"$f_{RR}$"
-    )
-    ax2.plot(
-        df["z"], df["f_RR"],
-        color="tab:orange", lw=1, alpha=0.6, linestyle="--"
+        df["z"],
+        df["f_RR"],
+        color="tab:orange",
+        marker="s",
+        s=40,
+        alpha=0.8,
+        label=r"$f_{RR}$",
     )
+    ax2.plot(df["z"], df["f_RR"], color="tab:orange", lw=1, alpha=0.6, linestyle="--")
     ax2.set_ylabel(r"$f_{RR}$", color="tab:orange")
     ax2.tick_params(axis="y", colors="tab:orange")
     ax2.set_yscale("log")
diff --git a/zz-scripts/chapter03/plot_fig08_ricci_fR_vs_T.py b/zz-scripts/chapter03/plot_fig08_ricci_fR_vs_T.py
index 5bf5118..d7e5cc2 100755
--- a/zz-scripts/chapter03/plot_fig08_ricci_fR_vs_T.py
+++ b/zz-scripts/chapter03/plot_fig08_ricci_fR_vs_T.py
@@ -17,7 +17,6 @@ Sortie :
 from pathlib import Path
 import logging

-import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt

@@ -31,8 +30,9 @@ log = logging.getLogger(__name__)
 # Chemins
 # ----------------------------------------------------------------------
 DATA_FILE = Path("zz-data") / "chapter03" / "03_ricci_fR_vs_T.csv"
-FIG_DIR   = Path("zz-figures") / "chapter03"
-FIG_PATH  = FIG_DIR / "fig_08_ricci_fR_vs_T.png"
+FIG_DIR = Path("zz-figures") / "chapter03"
+FIG_PATH = FIG_DIR / "fig_08_ricci_fR_vs_T.png"
+

 def main() -> None:
     # 1. Lecture des données
@@ -57,19 +57,12 @@ def main() -> None:
     FIG_DIR.mkdir(parents=True, exist_ok=True)

     # 4. Tracé principal avec double axe Y
-    fig, ax1 = plt.subplots(dpi=300, figsize=(6,4))
+    fig, ax1 = plt.subplots(dpi=300, figsize=(6, 4))

     # Axe de gauche : f_R
     color1 = "tab:blue"
-    ax1.scatter(
-        df["T_Gyr"], df["f_R"],
-        c=color1, marker="o", s=40,
-        label=r"$f_R$"
-    )
-    ax1.plot(
-        df["T_Gyr"], df["f_R"],
-        c=color1, lw=1, alpha=0.6
-    )
+    ax1.scatter(df["T_Gyr"], df["f_R"], c=color1, marker="o", s=40, label=r"$f_R$")
+    ax1.plot(df["T_Gyr"], df["f_R"], c=color1, lw=1, alpha=0.6)
     ax1.set_xscale("log")
     ax1.set_yscale("log")
     ax1.set_xlabel("Âge de l’Univers $T$ (Gyr)")
@@ -80,15 +73,8 @@ def main() -> None:
     # Axe de droite : f_RR
     ax2 = ax1.twinx()
     color2 = "tab:orange"
-    ax2.scatter(
-        df["T_Gyr"], df["f_RR"],
-        c=color2, marker="s", s=50,
-        label=r"$f_{RR}$"
-    )
-    ax2.plot(
-        df["T_Gyr"], df["f_RR"],
-        c=color2, lw=1, alpha=0.6, linestyle="--"
-    )
+    ax2.scatter(df["T_Gyr"], df["f_RR"], c=color2, marker="s", s=50, label=r"$f_{RR}$")
+    ax2.plot(df["T_Gyr"], df["f_RR"], c=color2, lw=1, alpha=0.6, linestyle="--")
     ax2.set_yscale("log")
     ax2.set_ylabel(r"$f_{RR}$", color=color2)
     ax2.tick_params(axis="y", labelcolor=color2)
@@ -101,7 +87,7 @@ def main() -> None:
         labels1 + labels2,
         loc="best",
         framealpha=0.8,
-        edgecolor="black"
+        edgecolor="black",
     )

     # 6. Titre
@@ -116,5 +102,6 @@ def main() -> None:
     plt.close(fig)
     log.info("Figure enregistrée → %s", FIG_PATH)

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter03/requirements.txt b/zz-scripts/chapter03/requirements.txt
index 062f563..9415656 100755
--- a/zz-scripts/chapter03/requirements.txt
+++ b/zz-scripts/chapter03/requirements.txt
@@ -2,11 +2,11 @@
 # Exigences pour le Chapitre 3 (Stabilité de f(R))

 1. **Environnement système**
-   * Système : Ubuntu 20.04 (WSL ou natif) ou équivalent Linux
-   * Espace disque : ≥ 200 Mo libre
+   * Système : Ubuntu 20.04 (WSL ou natif) ou équivalent Linux
+   * Espace disque : ≥ 200 Mo libre

 2. **Environnement Python**
-   * Python ≥ 3.9
+   * Python ≥ 3.9
    * Créer et activer un environnement virtuel dédié :
      ```bash
      cd ~/MCGT
@@ -20,12 +20,12 @@
      ```

 3. **Données brutes & scripts d’extraction**
-   * **Fichiers jalons et grille**
-     - `zz-data/chapter03/03_ricci_fR_milestones.csv`
-       • Colonnes :
-         • `R_over_R0` : rapport R/R₀ (—)
-         • `f_R`        : f′(R)       (—)
-         • `f_RR`       : f″(R)       (—)
+   * **Fichiers jalons et grille**
+     - `zz-data/chapter03/03_ricci_fR_milestones.csv`
+       • Colonnes :
+         • `R_over_R0` : rapport R/R₀ (—)
+         • `f_R`        : f′(R)       (—)
+         • `f_RR`       : f″(R)       (—)
      * Aucun autre fichier brut n’est requis. *

 4. **Structure du dépôt**
diff --git a/zz-scripts/chapter03/utils/convert_milestones.py b/zz-scripts/chapter03/utils/convert_milestones.py
index b7b8802..567d1b6 100755
--- a/zz-scripts/chapter03/utils/convert_milestones.py
+++ b/zz-scripts/chapter03/utils/convert_milestones.py
@@ -3,16 +3,18 @@ import argparse
 import pandas as pd
 from pathlib import Path

+
 def main():
-    p = argparse.ArgumentParser(description="Extract raw CSV from enriched milestones file")
+    p = argparse.ArgumentParser(
+        description="Extract raw CSV from enriched milestones file"
+    )
     p.add_argument(
-        "--src", required=True,
-        help="Path to 03_ricci_fR_milestones_enriched.csv"
+        "--src", required=True, help="Path to 03_ricci_fR_milestones_enriched.csv"
     )
     p.add_argument(
         "--dst",
         default="zz-data/chapter03/03_ricci_fR_milestones.csv",
-        help="Output path for the raw CSV"
+        help="Output path for the raw CSV",
     )
     args = p.parse_args()

@@ -25,5 +27,6 @@ def main():
     df[["R_over_R0", "f_R", "f_RR"]].to_csv(args.dst, index=False)
     print(f"Raw file generated: {args.dst}")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter04/generate_data_chapter04.py b/zz-scripts/chapter04/generate_data_chapter04.py
index 0709cfd..ad91464 100755
--- a/zz-scripts/chapter04/generate_data_chapter04.py
+++ b/zz-scripts/chapter04/generate_data_chapter04.py
@@ -10,31 +10,29 @@ import pandas as pd
 from scipy.interpolate import PchipInterpolator
 from scipy.signal import savgol_filter

+
 def main():
     import os
+
     print("[DBG] Entrée dans main(), cwd=", os.getcwd())

     # ----------------------------------------------------------------------
     # 0. Configuration des constantes et des chemins de fichiers
     # ----------------------------------------------------------------------
-    kappa     = 1e-35
-    Tp        = 0.087  # Gyr, transition logistique
-    data_dir  = "zz-data/chapter04"
+    kappa = 1e-35
+    Tp = 0.087  # Gyr, transition logistique
+    data_dir = "zz-data/chapter04"
     chap3_dir = "zz-data/chapter03"
-    p_file    = f"{data_dir}/04_P_vs_T.dat"
-    r_file    = f"{chap3_dir}/03_ricci_fR_vs_T.csv"
-    fr_file   = f"{chap3_dir}/03_fR_stability_data.csv"
+    p_file = f"{data_dir}/04_P_vs_T.dat"
+    r_file = f"{chap3_dir}/03_ricci_fR_vs_T.csv"
+    fr_file = f"{chap3_dir}/03_fR_stability_data.csv"
     output_file = f"{data_dir}/04_dimensionless_invariants.csv"

     # ----------------------------------------------------------------------
     # 1. Chargement des données P(T) brutes
     # ----------------------------------------------------------------------
     df_p = pd.read_csv(
-        p_file,
-        sep="\s+",
-        skiprows=1,
-        header=None,
-        names=["T_Gyr", "P_raw"]
+        p_file, sep="\s+", skiprows=1, header=None, names=["T_Gyr", "P_raw"]
     )
     df_p = df_p.astype({"T_Gyr": float, "P_raw": float})
     print("[DEBUG] df_p.columns:", df_p.columns.tolist())
@@ -45,43 +43,43 @@ def main():
     # 2. Construction de la grille log-uniforme (T ∈ [1e-6, 14] Gyr)
     # ----------------------------------------------------------------------
     Tmin, Tmax = 1e-6, 14.0
-    log_min    = np.log10(Tmin)
-    log_max    = np.log10(Tmax)
-    log_grid   = np.arange(log_min, log_max + 1e-12, 0.01)
-    T          = 10 ** log_grid
+    log_min = np.log10(Tmin)
+    log_max = np.log10(Tmax)
+    log_grid = np.arange(log_min, log_max + 1e-12, 0.01)
+    T = 10**log_grid

     # ----------------------------------------------------------------------
     # 3. Interpolation P(T) en log–log via PCHIP (extrapolation lisse)
     # ----------------------------------------------------------------------
-    logT_data  = np.log10(df_p["T_Gyr"])
-    logP_data  = np.log10(df_p["P_raw"])
+    logT_data = np.log10(df_p["T_Gyr"])
+    logP_data = np.log10(df_p["P_raw"])
     interp_logP = PchipInterpolator(logT_data, logP_data, extrapolate=True)
-    logP       = interp_logP(log_grid)
-    P          = 10 ** logP
+    logP = interp_logP(log_grid)
+    P = 10**logP

     # ----------------------------------------------------------------------
     # 4. Calcul et lissage de la dérivée dP/dT (Savitzky–Golay)
     # ----------------------------------------------------------------------
-    dP         = np.gradient(P, T)
-    dP_smooth  = savgol_filter(dP, window_length=21, polyorder=3, mode="interp")
+    dP = np.gradient(P, T)
+    dP_smooth = savgol_filter(dP, window_length=21, polyorder=3, mode="interp")

     # ----------------------------------------------------------------------
     # 5. Interpolation de R/R0 vs T (extrapolation lisse)
     # ----------------------------------------------------------------------
-    df_r       = pd.read_csv(r_file)
-    logT_r     = np.log10(df_r["T_Gyr"])
-    logR_data  = np.log10(df_r["R_over_R0"])
+    df_r = pd.read_csv(r_file)
+    logT_r = np.log10(df_r["T_Gyr"])
+    logR_data = np.log10(df_r["R_over_R0"])
     interp_logR = PchipInterpolator(logT_r, logR_data, extrapolate=True)
-    logR       = interp_logR(log_grid)
-    R_R0       = 10 ** logR
+    logR = interp_logR(log_grid)
+    R_R0 = 10**logR

     # ----------------------------------------------------------------------
     # 6. Interpolation log–log de f_R(R) pour préserver la précision ∼10⁻⁶
     # ----------------------------------------------------------------------
     # Lecture du CSV exact de f_R(R)
-    df_fr = pd.read_csv(fr_file, sep=',', header=0, usecols=['R_over_R0', 'f_R'])
-    R_fr   = df_fr['R_over_R0'].values
-    fR_fr  = df_fr['f_R'].values
+    df_fr = pd.read_csv(fr_file, sep=",", header=0, usecols=["R_over_R0", "f_R"])
+    R_fr = df_fr["R_over_R0"].values
+    fR_fr = df_fr["f_R"].values

     # DEBUG – contrôler les données brutes
     print("[DEBUG] df_fr.columns       :", df_fr.columns.tolist())
@@ -95,7 +93,7 @@ def main():

     # Appliquer sur la grille calculée R_R0
     logfR = interp_logf(np.log10(R_R0))
-    f_R   = 10 ** logfR
+    f_R = 10**logfR

     # Calcul de l’invariant I3 = f_R – 1
     I3 = f_R - 1
@@ -117,14 +115,10 @@ def main():
     # ----------------------------------------------------------------------
     # 8. Export du CSV final
     # ----------------------------------------------------------------------
-    df_out = pd.DataFrame({
-        "T_Gyr": T,
-        "I1":     I1,
-        "I2":     I2,
-        "I3":     I3
-    })
+    df_out = pd.DataFrame({"T_Gyr": T, "I1": I1, "I2": I2, "I3": I3})
     df_out.to_csv(output_file, index=False)
     print(f"[✔] Fichier exporté : {output_file}")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter04/plot_fig01_invariants_schematic.py b/zz-scripts/chapter04/plot_fig01_invariants_schematic.py
index 3cfea22..5849033 100755
--- a/zz-scripts/chapter04/plot_fig01_invariants_schematic.py
+++ b/zz-scripts/chapter04/plot_fig01_invariants_schematic.py
@@ -9,23 +9,23 @@ Script corrigé de tracé du schéma conceptuel des invariants adimensionnels
 – Sauvegarde la figure 800x500 px DPI 300
 """

-import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt

+
 def main():
     # ----------------------------------------------------------------------
     # 1. Chargement des données
     # ----------------------------------------------------------------------
-    data_file = 'zz-data/chapter04/04_dimensionless_invariants.csv'
+    data_file = "zz-data/chapter04/04_dimensionless_invariants.csv"
     df = pd.read_csv(data_file)
-    T  = df['T_Gyr'].values
-    I1 = df['I1'].values
-    I2 = df['I2'].values
-    I3 = df['I3'].values
+    T = df["T_Gyr"].values
+    I1 = df["I1"].values
+    I2 = df["I2"].values
+    I3 = df["I3"].values

     # Valeurs clés
-    Tp    = 0.087    # Gyr
+    Tp = 0.087  # Gyr
     I2_ref = 1e-35
     I3_ref = 1e-6

@@ -35,43 +35,44 @@ def main():
     fig, ax = plt.subplots(figsize=(8, 5), dpi=300)

     # Axe X en log
-    ax.set_xscale('log')
+    ax.set_xscale("log")

     # Axe Y en symlog pour gérer I3 autour de zéro
-    ax.set_yscale('symlog', linthresh=1e-7, linscale=1)
+    ax.set_yscale("symlog", linthresh=1e-7, linscale=1)

     # Tracés des invariants
-    ax.plot(T, I1, label=r'$I_1 = P/T$',   color='C0', linewidth=1.5)
-    ax.plot(T, I2, label=r'$I_2 = \kappa\,T^2$', color='C1', linewidth=1.5)
-    ax.plot(T, I3, label=r'$I_3 = f_R - 1$', color='C2', linewidth=1.5)
+    ax.plot(T, I1, label=r"$I_1 = P/T$", color="C0", linewidth=1.5)
+    ax.plot(T, I2, label=r"$I_2 = \kappa\,T^2$", color="C1", linewidth=1.5)
+    ax.plot(T, I3, label=r"$I_3 = f_R - 1$", color="C2", linewidth=1.5)

     # ----------------------------------------------------------------------
     # 3. Repères horizontaux
     # ----------------------------------------------------------------------
-    ax.axhline(I2_ref, color='C1', linestyle='--', label=r'$I_2 \approx 10^{-35}$')
-    ax.axhline(I3_ref, color='C2', linestyle='--', label=r'$I_3 \approx 10^{-6}$')
+    ax.axhline(I2_ref, color="C1", linestyle="--", label=r"$I_2 \approx 10^{-35}$")
+    ax.axhline(I3_ref, color="C2", linestyle="--", label=r"$I_3 \approx 10^{-6}$")

     # ----------------------------------------------------------------------
     # 4. Repère vertical de transition T_p
     # ----------------------------------------------------------------------
-    ax.axvline(Tp, color='orange', linestyle=':', label=r'$T_p=0.087\ \mathrm{Gyr}$')
+    ax.axvline(Tp, color="orange", linestyle=":", label=r"$T_p=0.087\ \mathrm{Gyr}$")

     # ----------------------------------------------------------------------
     # 5. Légendes, labels et grille
     # ----------------------------------------------------------------------
-    ax.set_xlabel(r'$T\ (\mathrm{Gyr})$')
-    ax.set_ylabel('Valeurs adimensionnelles')
-    ax.set_title('Fig. 01 – Schéma conceptuel des invariants adimensionnels')
-    ax.legend(loc='best', fontsize='small')
-    ax.grid(True, which='both', linestyle=':', linewidth=0.5)
+    ax.set_xlabel(r"$T\ (\mathrm{Gyr})$")
+    ax.set_ylabel("Valeurs adimensionnelles")
+    ax.set_title("Fig. 01 – Schéma conceptuel des invariants adimensionnels")
+    ax.legend(loc="best", fontsize="small")
+    ax.grid(True, which="both", linestyle=":", linewidth=0.5)

     # ----------------------------------------------------------------------
     # 6. Sauvegarde de la figure
     # ----------------------------------------------------------------------
-    output_fig = 'zz-figures/chapter04/fig_01_invariants_schematic.png'
+    output_fig = "zz-figures/chapter04/fig_01_invariants_schematic.png"
     plt.tight_layout()
     plt.savefig(output_fig)
     print(f"Fig. sauvegardée : {output_fig}")

-if __name__ == '__main__':
+
+if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter04/plot_fig02_invariants_histogram.py b/zz-scripts/chapter04/plot_fig02_invariants_histogram.py
index 13c2b78..6444465 100755
--- a/zz-scripts/chapter04/plot_fig02_invariants_histogram.py
+++ b/zz-scripts/chapter04/plot_fig02_invariants_histogram.py
@@ -13,15 +13,16 @@ import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt

+
 def main():
     # ----------------------------------------------------------------------
     # 1. Chargement des données
     # ----------------------------------------------------------------------
-    data_file = 'zz-data/chapter04/04_dimensionless_invariants.csv'
+    data_file = "zz-data/chapter04/04_dimensionless_invariants.csv"
     df = pd.read_csv(data_file)
-    logI2 = np.log10(df['I2'].values)
+    logI2 = np.log10(df["I2"].values)
     # Exclure I3 = 0 pour log10
-    I3_vals = df['I3'].values
+    I3_vals = df["I3"].values
     I3_nonzero = I3_vals[I3_vals != 0]
     logI3 = np.log10(np.abs(I3_nonzero))

@@ -31,31 +32,37 @@ def main():
     fig, ax = plt.subplots(figsize=(8, 5), dpi=300)

     # Définir les bins plus fins
-    bins = np.linspace(
-        min(logI2.min(), logI3.min()),
-        max(logI2.max(), logI3.max()),
-        40
-    )
+    bins = np.linspace(min(logI2.min(), logI3.min()), max(logI2.max(), logI3.max()), 40)

-    ax.hist(logI2, bins=bins, density=True, alpha=0.7, label=r'$\log_{10}I_2$', color='C1')
-    ax.hist(logI3, bins=bins, density=True, alpha=0.7, label=r'$\log_{10}\lvert I_3\rvert$', color='C2')
+    ax.hist(
+        logI2, bins=bins, density=True, alpha=0.7, label=r"$\log_{10}I_2$", color="C1"
+    )
+    ax.hist(
+        logI3,
+        bins=bins,
+        density=True,
+        alpha=0.7,
+        label=r"$\log_{10}\lvert I_3\rvert$",
+        color="C2",
+    )

     # ----------------------------------------------------------------------
     # 3. Labels, légende et grille
     # ----------------------------------------------------------------------
-    ax.set_xlabel(r'$\log_{10}\bigl(\mathrm{valeur\ de\ l’invariant}\bigr)$')
-    ax.set_ylabel('Densité normalisée')
-    ax.set_title('Fig. 02 – Histogramme des invariants adimensionnels')
-    ax.legend(fontsize='small')
-    ax.grid(True, which='both', linestyle=':', linewidth=0.5)
+    ax.set_xlabel(r"$\log_{10}\bigl(\mathrm{valeur\ de\ l’invariant}\bigr)$")
+    ax.set_ylabel("Densité normalisée")
+    ax.set_title("Fig. 02 – Histogramme des invariants adimensionnels")
+    ax.legend(fontsize="small")
+    ax.grid(True, which="both", linestyle=":", linewidth=0.5)

     # ----------------------------------------------------------------------
     # 4. Sauvegarde de la figure
     # ----------------------------------------------------------------------
-    output_fig = 'zz-figures/chapter04/fig_02_invariants_histogram.png'
+    output_fig = "zz-figures/chapter04/fig_02_invariants_histogram.png"
     plt.tight_layout()
     plt.savefig(output_fig)
     print(f"Fig. sauvegardée : {output_fig}")

-if __name__ == '__main__':
+
+if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter04/plot_fig03_invariants_vs_T.py b/zz-scripts/chapter04/plot_fig03_invariants_vs_T.py
index 42ad015..56e32eb 100755
--- a/zz-scripts/chapter04/plot_fig03_invariants_vs_T.py
+++ b/zz-scripts/chapter04/plot_fig03_invariants_vs_T.py
@@ -9,17 +9,17 @@ Script de tracé des invariants adimensionnels I1, I2 et I3 en fonction de T
 – Sauvegarde la figure en PNG 800×500 px, DPI 300
 """

-import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt

+
 def main():
     # 1. Chargement des données
-    df = pd.read_csv('zz-data/chapter04/04_dimensionless_invariants.csv')
-    T = df['T_Gyr'].values
-    I1 = df['I1'].values
-    I2 = df['I2'].values
-    I3 = df['I3'].values
+    df = pd.read_csv("zz-data/chapter04/04_dimensionless_invariants.csv")
+    T = df["T_Gyr"].values
+    I1 = df["I1"].values
+    I2 = df["I2"].values
+    I3 = df["I3"].values

     # Valeurs clés
     Tp = 0.087
@@ -28,29 +28,30 @@ def main():

     # 2. Création de la figure
     fig, ax = plt.subplots(figsize=(8, 5), dpi=300)
-    ax.set_xscale('log')
-    ax.set_yscale('symlog', linthresh=1e-7)
-    ax.plot(T, I1, color='C0', label=r'$I_1 = P/T$', linewidth=1.5)
-    ax.plot(T, I2, color='C1', label=r'$I_2 = \kappa T^2$', linewidth=1.5)
-    ax.plot(T, I3, color='C2', label=r'$I_3 = f_R - 1$', linewidth=1.5)
+    ax.set_xscale("log")
+    ax.set_yscale("symlog", linthresh=1e-7)
+    ax.plot(T, I1, color="C0", label=r"$I_1 = P/T$", linewidth=1.5)
+    ax.plot(T, I2, color="C1", label=r"$I_2 = \kappa T^2$", linewidth=1.5)
+    ax.plot(T, I3, color="C2", label=r"$I_3 = f_R - 1$", linewidth=1.5)

     # 3. Repères
-    ax.axhline(I2_ref, color='C1', linestyle='--', label=r'$I_2 \approx 10^{-35}$')
-    ax.axhline(I3_ref, color='C2', linestyle='--', label=r'$I_3 \approx 10^{-6}$')
-    ax.axvline(Tp, color='orange', linestyle=':', label=r'$T_p = 0.087\ \mathrm{Gyr}$')
+    ax.axhline(I2_ref, color="C1", linestyle="--", label=r"$I_2 \approx 10^{-35}$")
+    ax.axhline(I3_ref, color="C2", linestyle="--", label=r"$I_3 \approx 10^{-6}$")
+    ax.axvline(Tp, color="orange", linestyle=":", label=r"$T_p = 0.087\ \mathrm{Gyr}$")

     # 4. Labels et légende
-    ax.set_xlabel(r'$T\ (\mathrm{Gyr})$')
-    ax.set_ylabel('Invariant (valeur adimensionnelle)')
-    ax.set_title('Fig. 03 – Invariants adimensionnels $I_1$, $I_2$ et $I_3$ vs $T$')
-    ax.legend(fontsize='small')
-    ax.grid(True, which='both', linestyle=':', linewidth=0.5)
+    ax.set_xlabel(r"$T\ (\mathrm{Gyr})$")
+    ax.set_ylabel("Invariant (valeur adimensionnelle)")
+    ax.set_title("Fig. 03 – Invariants adimensionnels $I_1$, $I_2$ et $I_3$ vs $T$")
+    ax.legend(fontsize="small")
+    ax.grid(True, which="both", linestyle=":", linewidth=0.5)

     # 5. Sauvegarde
-    out = 'zz-figures/chapter04/fig_03_invariants_vs_T.png'
+    out = "zz-figures/chapter04/fig_03_invariants_vs_T.png"
     plt.tight_layout()
     plt.savefig(out)
     print(f"Figure enregistrée : {out}")

-if __name__ == '__main__':
+
+if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter04/plot_fig04_relative_deviations.py b/zz-scripts/chapter04/plot_fig04_relative_deviations.py
index e37f23c..d5a85c4 100755
--- a/zz-scripts/chapter04/plot_fig04_relative_deviations.py
+++ b/zz-scripts/chapter04/plot_fig04_relative_deviations.py
@@ -14,8 +14,8 @@ import pandas as pd
 import matplotlib.pyplot as plt
 import os

-def main():

+def main():
     # ----------------------------------------------------------------------
     # 1bis. Définition de la transition logistique
     # ----------------------------------------------------------------------
@@ -25,8 +25,8 @@ def main():
     # 1. Chargement des données
     # ----------------------------------------------------------------------
     possible_paths = [
-        'zz-data/chapter04/04_dimensionless_invariants.csv',
-        '/mnt/data/04_dimensionless_invariants.csv'
+        "zz-data/chapter04/04_dimensionless_invariants.csv",
+        "/mnt/data/04_dimensionless_invariants.csv",
     ]
     df = None
     for path in possible_paths:
@@ -37,13 +37,13 @@ def main():
     if df is None:
         raise FileNotFoundError(f"Aucun CSV trouvé parmi : {possible_paths}")

-    for col in ['T_Gyr', 'I2', 'I3']:
+    for col in ["T_Gyr", "I2", "I3"]:
         if col not in df.columns:
             raise KeyError(f"Colonne '{col}' manquante dans {path}")

-    T  = df['T_Gyr'].values
-    I2 = df['I2'].values
-    I3 = df['I3'].values
+    T = df["T_Gyr"].values
+    I2 = df["I2"].values
+    I3 = df["I3"].values

     # Références théoriques
     I2_ref = 1e-35
@@ -56,7 +56,7 @@ def main():
     eps3 = (I3 - I3_ref) / I3_ref

     # Pour ne faire apparaître que les écarts dans ±10 %, masquer le reste
-    tol = 0.10   # 10 %  → mettre 0.01 pour ±1 %
+    tol = 0.10  # 10 %  → mettre 0.01 pour ±1 %
     eps2_plot = np.where(np.abs(eps2) <= tol, eps2, np.nan)
     eps3_plot = np.where(np.abs(eps3) <= tol, eps3, np.nan)

@@ -64,62 +64,69 @@ def main():
     # 3. Création de la figure (zoomée sur ε ∈ [−0.2, 0.2])
     # ----------------------------------------------------------------------
     fig, ax = plt.subplots(figsize=(8, 5), dpi=300)
-    ax.set_xscale('log')
-    ax.plot(T, eps2_plot, color='C1',
-        label=r'$\varepsilon_2 = \frac{I_2 - I_{2,\mathrm{ref}}}{I_{2,\mathrm{ref}}}$')
-    ax.plot(T, eps3_plot, color='C2',
-        label=r'$\varepsilon_3 = \frac{I_3 - I_{3,\mathrm{ref}}}{I_{3,\mathrm{ref}}}$')
+    ax.set_xscale("log")
+    ax.plot(
+        T,
+        eps2_plot,
+        color="C1",
+        label=r"$\varepsilon_2 = \frac{I_2 - I_{2,\mathrm{ref}}}{I_{2,\mathrm{ref}}}$",
+    )
+    ax.plot(
+        T,
+        eps3_plot,
+        color="C2",
+        label=r"$\varepsilon_3 = \frac{I_3 - I_{3,\mathrm{ref}}}{I_{3,\mathrm{ref}}}$",
+    )

     # Seuils ±1% et ±10%
-    ax.axhline( 0.01, color='k', linestyle='--', label=r'$\pm1\%$')
-    ax.axhline(-0.01, color='k', linestyle='--')
-    ax.axhline( 0.10, color='gray', linestyle=':', label=r'$\pm10\%$')
-    ax.axhline(-0.10, color='gray', linestyle=':')
+    ax.axhline(0.01, color="k", linestyle="--", label=r"$\pm1\%$")
+    ax.axhline(-0.01, color="k", linestyle="--")
+    ax.axhline(0.10, color="gray", linestyle=":", label=r"$\pm10\%$")
+    ax.axhline(-0.10, color="gray", linestyle=":")

     # Zoom vertical ±0.2
     ax.set_ylim(-0.2, 0.2)

     # Graduations mineures sur l’axe T
     from matplotlib.ticker import LogLocator
-    ax.xaxis.set_minor_locator(LogLocator(base=10.0, subs=range(1,10)))
-    ax.xaxis.set_tick_params(which='minor', length=3)
+
+    ax.xaxis.set_minor_locator(LogLocator(base=10.0, subs=range(1, 10)))
+    ax.xaxis.set_tick_params(which="minor", length=3)

     # ----------------------------------------------------------------------
     # 4. Titres, légende, grille
     # ----------------------------------------------------------------------
-    ax.set_xlabel(r'$T\ (\mathrm{Gyr})$')
-    ax.set_ylabel(r'$\varepsilon_i$')
+    ax.set_xlabel(r"$T\ (\mathrm{Gyr})$")
+    ax.set_ylabel(r"$\varepsilon_i$")

     # Titre principal
-    ax.set_title('Fig. 04 – Écarts relatifs des invariants $I_2$ et $I_3$', pad=32)
+    ax.set_title("Fig. 04 – Écarts relatifs des invariants $I_2$ et $I_3$", pad=32)

     # Sous-titre pour préciser la plage zoomée
     ax.text(
-    0.5, 1.02,
-    "Plage zoomée : |εᵢ| ≤ 0,10",
-    transform=ax.transAxes,
-    ha='center',
-    va='bottom',
-    fontsize='small'
+        0.5,
+        1.02,
+        "Plage zoomée : |εᵢ| ≤ 0,10",
+        transform=ax.transAxes,
+        ha="center",
+        va="bottom",
+        fontsize="small",
     )
-    ax.grid(True, which='both', linestyle=':', linewidth=0.5, zorder=0)
+    ax.grid(True, which="both", linestyle=":", linewidth=0.5, zorder=0)
     ax.axvline(
-        Tp,
-        color='C3',
-        linestyle=':',
-        label=r'$T_p=0.087\ \mathrm{Gyr}$',
-        zorder=5
+        Tp, color="C3", linestyle=":", label=r"$T_p=0.087\ \mathrm{Gyr}$", zorder=5
     )

-    ax.legend(fontsize='small', loc='upper right')
+    ax.legend(fontsize="small", loc="upper right")

     # ----------------------------------------------------------------------
     # 5. Sauvegarde
     # ----------------------------------------------------------------------
-    output_fig = 'zz-figures/chapter04/fig_04_relative_deviations.png'
+    output_fig = "zz-figures/chapter04/fig_04_relative_deviations.png"
     plt.tight_layout()
     plt.savefig(output_fig)
     print(f"Figure sauvegardée : {output_fig}")

-if __name__=='__main__':
+
+if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter04/requirements.txt b/zz-scripts/chapter04/requirements.txt
index 2d66331..753261f 100755
--- a/zz-scripts/chapter04/requirements.txt
+++ b/zz-scripts/chapter04/requirements.txt
@@ -1,11 +1,11 @@
 # Exigences pour le Chapitre 4 (Invariants adimensionnels)

 1. **Environnement système**
-   * Système : Ubuntu 20.04 (WSL ou natif) ou équivalent Linux
+   * Système : Ubuntu 20.04 (WSL ou natif) ou équivalent Linux
    * Espace disque : ≥ 200 Mo libre

 2. **Environnement Python**
-   * Python ≥ 3.9
+   * Python ≥ 3.9
    * Créer et activer un environnement virtuel spécifique :
      ```bash
      cd ~/MCGT
@@ -19,14 +19,14 @@
      ```

 3. **Données brutes & scripts d’extraction**
-   * **P(T) brut**
-     – Fichier : `zz-data/chapter04/04_P_vs_T.dat`
+   * **P(T) brut**
+     – Fichier : `zz-data/chapter04/04_P_vs_T.dat`
      – Chargement et parsing par : `zz-scripts/chapter04/integrate_dimensionless_invariants.py`
-   * **R/R₀ vs T**
-     – Fichier : `zz-data/chapter03/03_r_over_r0.csv`
+   * **R/R₀ vs T**
+     – Fichier : `zz-data/chapter03/03_r_over_r0.csv`
      – Chargement et interpolation par le même script d’intégration
-   * **f_R exact vs R/R₀**
-     – Fichier : `zz-data/chapter03/03_ricci_fR_exact.csv`
+   * **f_R exact vs R/R₀**
+     – Fichier : `zz-data/chapter03/03_ricci_fR_exact.csv`
      – Chargement et interpolation lin–lin (ou log–log selon précision souhaitée)

 4. **Structure du dépôt**
@@ -102,5 +102,3 @@ MCGT/
    * Les noms de fichiers et répertoires doivent rester cohérents (utiliser `chapter04` et non `chapter4`).
    * Lorsque des CSV sont produits par des scripts antérieurs (chapitre 03), vérifier leur compatibilité de colonnes avant d’exécuter l’intégration.
    * Si le pipeline est exécuté dans un environnement CI (GitHub Actions / GitLab CI), veiller à installer `venv4` et copier les jeux de données requis dans le workspace avant d’exécuter les scripts.
-
-
diff --git a/zz-scripts/chapter05/generate_data_chapter05.py b/zz-scripts/chapter05/generate_data_chapter05.py
index 717781b..e5e1341 100755
--- a/zz-scripts/chapter05/generate_data_chapter05.py
+++ b/zz-scripts/chapter05/generate_data_chapter05.py
@@ -6,23 +6,23 @@ from scipy.signal import savgol_filter
 from pathlib import Path

 # — Répertoires —
-ROOT       = Path(__file__).resolve().parents[2]
-DATA_DIR   = ROOT / "zz-data" / "chapter05"
+ROOT = Path(__file__).resolve().parents[2]
+DATA_DIR = ROOT / "zz-data" / "chapter05"
 DATA_DIR.mkdir(parents=True, exist_ok=True)

 # — Fichiers d’entrée et de sortie (noms harmonisés en anglais) —
-JALONS_FILE  = DATA_DIR / "05_bbn_milestones.csv"
-GRILLE_FILE  = DATA_DIR / "05_bbn_grid.csv"
-PRED_FILE    = DATA_DIR / "05_bbn_data.csv"
-CHI2_FILE    = DATA_DIR / "05_chi2_bbn_vs_T.csv"
-DERIV_FILE   = DATA_DIR / "05_dchi2_vs_T.csv"
-PARAMS_FILE  = DATA_DIR / "05_bbn_params.json"
+JALONS_FILE = DATA_DIR / "05_bbn_milestones.csv"
+GRILLE_FILE = DATA_DIR / "05_bbn_grid.csv"
+PRED_FILE = DATA_DIR / "05_bbn_data.csv"
+CHI2_FILE = DATA_DIR / "05_chi2_bbn_vs_T.csv"
+DERIV_FILE = DATA_DIR / "05_dchi2_vs_T.csv"
+PARAMS_FILE = DATA_DIR / "05_bbn_params.json"

 # Seuils de classification (relative error)
 THRESHOLDS = {"primary": 0.01, "order2": 0.10}

 # 1) Chargement des jalons
-jalons    = pd.read_csv(JALONS_FILE)
+jalons = pd.read_csv(JALONS_FILE)
 jalons_DH = jalons.dropna(subset=["DH_obs"])
 jalons_Yp = jalons.dropna(subset=["Yp_obs"])

@@ -30,93 +30,85 @@ jalons_Yp = jalons.dropna(subset=["Yp_obs"])
 t_min, t_max = 1e-6, 14.0
 log_min, log_max = np.log10(t_min), np.log10(t_max)
 step = 0.01
-num = int(round((log_max-log_min)/step)) + 1
+num = int(round((log_max - log_min) / step)) + 1
 T = np.logspace(log_min, log_max, num=num)
 pd.DataFrame({"T_Gyr": T}).to_csv(GRILLE_FILE, index=False)

 # 3) Interpolations monotones (PCHIP) en log–log
 # Deutérium
 interp_DH = PchipInterpolator(
-    np.log10(jalons_DH["T_Gyr"]),
-    np.log10(jalons_DH["DH_obs"]),
-    extrapolate=True
+    np.log10(jalons_DH["T_Gyr"]), np.log10(jalons_DH["DH_obs"]), extrapolate=True
 )
-DH_calc = 10**interp_DH(np.log10(T))
+DH_calc = 10 ** interp_DH(np.log10(T))

 # Hélium-4
 if len(jalons_Yp) > 1:
     interp_Yp = PchipInterpolator(
-        np.log10(jalons_Yp["T_Gyr"]),
-        np.log10(jalons_Yp["Yp_obs"]),
-        extrapolate=True
+        np.log10(jalons_Yp["T_Gyr"]), np.log10(jalons_Yp["Yp_obs"]), extrapolate=True
     )
-    Yp_calc = 10**interp_Yp(np.log10(T))
+    Yp_calc = 10 ** interp_Yp(np.log10(T))
 else:
     # Si un seul point, on met une constante
     Yp_calc = np.full_like(T, jalons_Yp["Yp_obs"].iloc[0])

 # 4) Sauvegarde des prédictions
-df_pred = pd.DataFrame({
-    "T_Gyr":   T,
-    "DH_calc": DH_calc,
-    "Yp_calc": Yp_calc
-})
+df_pred = pd.DataFrame({"T_Gyr": T, "DH_calc": DH_calc, "Yp_calc": Yp_calc})
 df_pred.to_csv(PRED_FILE, index=False)

 # 5) Calcul du χ² total (DH + Yp)
 chi2_vals = []
 for dh_c, yp_c in zip(DH_calc, Yp_calc):
-    c1 = ((dh_c - jalons_DH["DH_obs"])**2 / jalons_DH["sigma_DH"]**2).sum()
-    c2 = ((yp_c - jalons_Yp["Yp_obs"])**2 / jalons_Yp["sigma_Yp"]**2).sum()
+    c1 = ((dh_c - jalons_DH["DH_obs"]) ** 2 / jalons_DH["sigma_DH"] ** 2).sum()
+    c2 = ((yp_c - jalons_Yp["Yp_obs"]) ** 2 / jalons_Yp["sigma_Yp"] ** 2).sum()
     chi2_vals.append(c1 + c2)

-pd.DataFrame({
-    "T_Gyr":               T,
-    "chi2_nucleosynthesis": chi2_vals
-}).to_csv(CHI2_FILE, index=False)
+pd.DataFrame({"T_Gyr": T, "chi2_nucleosynthesis": chi2_vals}).to_csv(
+    CHI2_FILE, index=False
+)

 # 6) Dérivée et lissage de χ²
-dchi2_raw    = np.gradient(chi2_vals, T)
+dchi2_raw = np.gradient(chi2_vals, T)
 # Fenêtre impair ≤ 21 pour éviter oversmoothing
-win = min(21, (len(dchi2_raw)//2)*2 + 1)
+win = min(21, (len(dchi2_raw) // 2) * 2 + 1)
 dchi2_smooth = savgol_filter(dchi2_raw, win, polyorder=3, mode="interp")
-pd.DataFrame({
-    "T_Gyr":        T,
-    "dchi2_smooth": dchi2_smooth
-}).to_csv(DERIV_FILE, index=False)
+pd.DataFrame({"T_Gyr": T, "dchi2_smooth": dchi2_smooth}).to_csv(DERIV_FILE, index=False)

 # 7) Calcul des tolérances ε = |pred–obs|/obs
 eps_records = []
 # pour chaque jalon (DH et Yp)
 for _, row in jalons.iterrows():
     if pd.notna(row["DH_obs"]):
-        dh_pred = 10**interp_DH(np.log10(row["T_Gyr"]))
+        dh_pred = 10 ** interp_DH(np.log10(row["T_Gyr"]))
         eps = abs(dh_pred - row["DH_obs"]) / row["DH_obs"]
-        eps_records.append({
-            "epsilon":  eps,
-            "sigma_rel": row["sigma_DH"] / row["DH_obs"]
-        })
+        eps_records.append(
+            {"epsilon": eps, "sigma_rel": row["sigma_DH"] / row["DH_obs"]}
+        )
     if pd.notna(row["Yp_obs"]):
         if len(jalons_Yp) > 1:
-            yp_pred = 10**interp_Yp(np.log10(row["T_Gyr"]))
+            yp_pred = 10 ** interp_Yp(np.log10(row["T_Gyr"]))
         else:
             yp_pred = jalons_Yp["Yp_obs"].iloc[0]
         eps = abs(yp_pred - row["Yp_obs"]) / row["Yp_obs"]
-        eps_records.append({
-            "epsilon":  eps,
-            "sigma_rel": row["sigma_Yp"] / row["Yp_obs"]
-        })
+        eps_records.append(
+            {"epsilon": eps, "sigma_rel": row["sigma_Yp"] / row["Yp_obs"]}
+        )

 df_eps = pd.DataFrame(eps_records)
 max_e1 = df_eps[df_eps["sigma_rel"] <= THRESHOLDS["primary"]]["epsilon"].max()
-max_e2 = df_eps[(df_eps["sigma_rel"] > THRESHOLDS["primary"]) &
-                (df_eps["sigma_rel"] <= THRESHOLDS["order2"])]["epsilon"].max()
+max_e2 = df_eps[
+    (df_eps["sigma_rel"] > THRESHOLDS["primary"])
+    & (df_eps["sigma_rel"] <= THRESHOLDS["order2"])
+]["epsilon"].max()

 # 8) Sauvegarde des paramètres
 with open(PARAMS_FILE, "w") as f:
-    json.dump({
-        "max_epsilon_primary": float(max_e1) if not pd.isna(max_e1) else None,
-        "max_epsilon_order2":  float(max_e2) if not pd.isna(max_e2) else None
-    }, f, indent=2)
+    json.dump(
+        {
+            "max_epsilon_primary": float(max_e1) if not pd.isna(max_e1) else None,
+            "max_epsilon_order2": float(max_e2) if not pd.isna(max_e2) else None,
+        },
+        f,
+        indent=2,
+    )

 print("✓ Chapitre 05 : données générées avec succès.")
diff --git a/zz-scripts/chapter05/plot_fig01_bbn_reaction_network.py b/zz-scripts/chapter05/plot_fig01_bbn_reaction_network.py
index 7ba06c8..5cf91dd 100755
--- a/zz-scripts/chapter05/plot_fig01_bbn_reaction_network.py
+++ b/zz-scripts/chapter05/plot_fig01_bbn_reaction_network.py
@@ -3,49 +3,54 @@ import matplotlib.pyplot as plt
 import numpy as np
 from pathlib import Path

+
 def draw_bbn_schema(save_path="zz-figures/chapter05/fig_01_bbn_reaction_network.png"):
     fig, ax = plt.subplots(figsize=(8, 4.2), facecolor="white")

     # Centres des boîtes (x, y)
     P = {
-        "n":   np.array((0.07, 0.58)),
-        "p":   np.array((0.07, 0.38)),
-        "D":   np.array((0.34, 0.48)),
-        "T":   np.array((0.56, 0.74)),
+        "n": np.array((0.07, 0.58)),
+        "p": np.array((0.07, 0.38)),
+        "D": np.array((0.34, 0.48)),
+        "T": np.array((0.56, 0.74)),
         "He3": np.array((0.56, 0.22)),
-        "He4": np.array((0.90, 0.48))
+        "He4": np.array((0.90, 0.48)),
     }

-    dx       = 0.04   # décalage horizontal flèches ←→ boîtes (légèrement augmenté)
-    pad_box  = 0.65    # padding interne des boîtes (boîtes « n », « p » plus larges)
+    dx = 0.04  # décalage horizontal flèches ←→ boîtes (légèrement augmenté)
+    pad_box = 0.65  # padding interne des boîtes (boîtes « n », « p » plus larges)

     # Dessin des boîtes
     for lab, pos in P.items():
-        ax.text(*pos, lab,
-                fontsize=14, ha="center", va="center",
-                bbox=dict(boxstyle=f"round,pad={pad_box}",
-                          fc="lightgray", ec="gray"))
+        ax.text(
+            *pos,
+            lab,
+            fontsize=14,
+            ha="center",
+            va="center",
+            bbox=dict(boxstyle=f"round,pad={pad_box}", fc="lightgray", ec="gray"),
+        )

     # Fonction utilitaire pour tracer une flèche décalée
     def arrow(src, dst):
         x0, y0 = P[src]
         x1, y1 = P[dst]
         start = (x0 + dx, y0) if x1 > x0 else (x0 - dx, y0)
-        end   = (x1 - dx, y1) if x1 > x0 else (x1 + dx, y1)
-        ax.annotate("", xy=end, xytext=start,
-                    arrowprops=dict(arrowstyle="->", lw=2))
+        end = (x1 - dx, y1) if x1 > x0 else (x1 + dx, y1)
+        ax.annotate("", xy=end, xytext=start, arrowprops=dict(arrowstyle="->", lw=2))

     # Flèches du réseau BBN
-    arrow("n",   "D")
-    arrow("p",   "D")
-    arrow("D",   "T")
-    arrow("D",   "He3")
-    arrow("T",   "He4")
+    arrow("n", "D")
+    arrow("p", "D")
+    arrow("D", "T")
+    arrow("D", "He3")
+    arrow("T", "He4")
     arrow("He3", "He4")

     # Titre rapproché
-    ax.set_title("Schéma des réactions de la nucléosynthèse primordiale",
-                 fontsize=14, pad=6)
+    ax.set_title(
+        "Schéma des réactions de la nucléosynthèse primordiale", fontsize=14, pad=6
+    )

     ax.axis("off")
     plt.tight_layout(pad=0.5)
@@ -53,5 +58,6 @@ def draw_bbn_schema(save_path="zz-figures/chapter05/fig_01_bbn_reaction_network.
     plt.savefig(save_path, dpi=300)
     plt.close()

+
 if __name__ == "__main__":
     draw_bbn_schema()
diff --git a/zz-scripts/chapter05/plot_fig02_dh_model_vs_obs.py b/zz-scripts/chapter05/plot_fig02_dh_model_vs_obs.py
index 78ad7b6..1ed6f58 100755
--- a/zz-scripts/chapter05/plot_fig02_dh_model_vs_obs.py
+++ b/zz-scripts/chapter05/plot_fig02_dh_model_vs_obs.py
@@ -29,7 +29,7 @@ else:
 interp = PchipInterpolator(
     np.log10(donnees["T_Gyr"].values),
     np.log10(donnees["DH_calc"].values),
-    extrapolate=False
+    extrapolate=False,
 )
 jalons["DH_calc"] = 10 ** interp(np.log10(jalons["T_Gyr"].values))

@@ -40,15 +40,17 @@ ax.set_yscale("log")

 # Barres d'erreur et points de calibration
 ax.errorbar(
-    jalons["DH_obs"], jalons["DH_calc"],
+    jalons["DH_obs"],
+    jalons["DH_calc"],
     yerr=jalons["sigma_DH"],
-    fmt='o', label="Points de calibration"
+    fmt="o",
+    label="Points de calibration",
 )

 # Droite d'identité y = x
 lims = [
     min(jalons["DH_obs"].min(), jalons["DH_calc"].min()),
-    max(jalons["DH_obs"].max(), jalons["DH_calc"].max())
+    max(jalons["DH_obs"].max(), jalons["DH_calc"].max()),
 ]
 ax.plot(lims, lims, ls="--", color="black", label="Identité")

@@ -60,10 +62,13 @@ if max_ep_order2 is not None:
     txt_lines.append(f"max ε_order2 = {max_ep_order2:.2e}")
 if txt_lines:
     ax.text(
-        0.05, 0.5, "\n".join(txt_lines),
+        0.05,
+        0.5,
+        "\n".join(txt_lines),
         transform=ax.transAxes,
-        va="center", ha="left",
-        bbox=dict(boxstyle="round", facecolor="white", alpha=0.5)
+        va="center",
+        ha="left",
+        bbox=dict(boxstyle="round", facecolor="white", alpha=0.5),
     )

 # Légendes et annotations
diff --git a/zz-scripts/chapter05/plot_fig03_yp_model_vs_obs.py b/zz-scripts/chapter05/plot_fig03_yp_model_vs_obs.py
index e37e820..a531869 100755
--- a/zz-scripts/chapter05/plot_fig03_yp_model_vs_obs.py
+++ b/zz-scripts/chapter05/plot_fig03_yp_model_vs_obs.py
@@ -3,7 +3,10 @@ import pandas as pd
 import matplotlib.pyplot as plt
 from pathlib import Path

-def tracer_fig03_yp_modele_contre_obs(save_path="zz-figures/chapter05/fig_03_yp_model_vs_obs.png"):
+
+def tracer_fig03_yp_modele_contre_obs(
+    save_path="zz-figures/chapter05/fig_03_yp_model_vs_obs.png",
+):
     # Racine du projet
     ROOT = Path.cwd()
     DATA_DIR = ROOT / "zz-data" / "chapter05"
@@ -11,8 +14,9 @@ def tracer_fig03_yp_modele_contre_obs(save_path="zz-figures/chapter05/fig_03_yp_
     FIG_DIR.mkdir(parents=True, exist_ok=True)

     # Lecture des données (ignorer les commentaires et espaces)
-    jalons = pd.read_csv(DATA_DIR / "05_bbn_milestones.csv",
-                         comment='#', skipinitialspace=True)
+    jalons = pd.read_csv(
+        DATA_DIR / "05_bbn_milestones.csv", comment="#", skipinitialspace=True
+    )
     data = pd.read_csv(DATA_DIR / "05_bbn_data.csv")

     # Conversion numérique
@@ -32,28 +36,28 @@ def tracer_fig03_yp_modele_contre_obs(save_path="zz-figures/chapter05/fig_03_yp_
     # Création de la figure
     fig, ax = plt.subplots(figsize=(8, 6))
     # Tracer les jalons avec barres d'erreur horizontales
-    ax.errorbar(Yp_obs, Yp_calc, xerr=sigma_Yp, fmt='o', capsize=3, label='Jalons Yp')
+    ax.errorbar(Yp_obs, Yp_calc, xerr=sigma_Yp, fmt="o", capsize=3, label="Jalons Yp")

     # Mettre en échelle log–log
-    ax.set_xscale('log')
-    ax.set_yscale('log')
+    ax.set_xscale("log")
+    ax.set_yscale("log")

     # Étendre la droite y = x sur toute la plage visible
-    ax.set_aspect('equal', 'box')
+    ax.set_aspect("equal", "box")
     # obtenir limites communes
     all_vals = np.concatenate([Yp_obs, Yp_calc])
     val_min, val_max = all_vals.min(), all_vals.max()
     # ajouter un petit margin
     margin = 0.1 * (val_max - val_min)
     lims = [val_min - margin, val_max + margin]
-    ax.plot(lims, lims, '--', color='gray', label=r'$y=x$')
+    ax.plot(lims, lims, "--", color="gray", label=r"$y=x$")
     ax.set_xlim(lims)
     ax.set_ylim(lims)

     # Labels et titre
-    ax.set_xlabel(r'$Y_{p,\rm obs}$')
-    ax.set_ylabel(r'$Y_{p,\rm calc}$')
-    ax.set_title('Comparaison Yp modèle vs observations')
+    ax.set_xlabel(r"$Y_{p,\rm obs}$")
+    ax.set_ylabel(r"$Y_{p,\rm calc}$")
+    ax.set_title("Comparaison Yp modèle vs observations")
     ax.legend()

     # Sauvegarde
@@ -61,5 +65,6 @@ def tracer_fig03_yp_modele_contre_obs(save_path="zz-figures/chapter05/fig_03_yp_
     plt.savefig(save_path, dpi=300)
     plt.close()

-if __name__ == '__main__':
+
+if __name__ == "__main__":
     tracer_fig03_yp_modele_contre_obs()
diff --git a/zz-scripts/chapter05/plot_fig04_chi2_vs_T.py b/zz-scripts/chapter05/plot_fig04_chi2_vs_T.py
index 708e0cd..be63992 100755
--- a/zz-scripts/chapter05/plot_fig04_chi2_vs_T.py
+++ b/zz-scripts/chapter05/plot_fig04_chi2_vs_T.py
@@ -6,27 +6,28 @@ from scipy.signal import savgol_filter
 from pathlib import Path

 # — Répertoires —
-ROOT     = Path(__file__).resolve().parents[2]
+ROOT = Path(__file__).resolve().parents[2]
 DATA_DIR = ROOT / "zz-data" / "chapter05"
-FIG_DIR  = ROOT / "zz-figures" / "chapter05"
+FIG_DIR = ROOT / "zz-figures" / "chapter05"
 FIG_DIR.mkdir(parents=True, exist_ok=True)

 # — 1) Chargement de χ² —
 chi2_file = DATA_DIR / "05_chi2_bbn_vs_T.csv"
-chi2_df   = pd.read_csv(chi2_file)
+chi2_df = pd.read_csv(chi2_file)

 # auto-détection de la colonne χ² (contient "chi2" mais pas "d" ou "deriv")
 chi2_col = next(
-    c for c in chi2_df.columns
+    c
+    for c in chi2_df.columns
     if "chi2" in c.lower() and not any(k in c.lower() for k in ("d", "deriv"))
 )

 # conversion et nettoyage
-chi2_df["T_Gyr"]         = pd.to_numeric(chi2_df["T_Gyr"], errors="coerce")
-chi2_df[chi2_col]        = pd.to_numeric(chi2_df[chi2_col], errors="coerce")
-chi2_df                  = chi2_df.dropna(subset=["T_Gyr", chi2_col])
-T                        = chi2_df["T_Gyr"].to_numpy()
-chi2                     = chi2_df[chi2_col].to_numpy()
+chi2_df["T_Gyr"] = pd.to_numeric(chi2_df["T_Gyr"], errors="coerce")
+chi2_df[chi2_col] = pd.to_numeric(chi2_df[chi2_col], errors="coerce")
+chi2_df = chi2_df.dropna(subset=["T_Gyr", chi2_col])
+T = chi2_df["T_Gyr"].to_numpy()
+chi2 = chi2_df[chi2_col].to_numpy()

 # incertitude : colonne 'chi2_err' si présente, sinon ±10 %
 if "chi2_err" in chi2_df.columns:
@@ -36,19 +37,20 @@ else:

 # — 2) Chargement de dχ²/dT —
 dchi_file = DATA_DIR / "05_dchi2_vs_T.csv"
-dchi_df   = pd.read_csv(dchi_file)
+dchi_df = pd.read_csv(dchi_file)

 # auto-détection de la colonne dérivée (contient "chi2" et "d" ou "deriv" ou "smooth")
 dchi_col = next(
-    c for c in dchi_df.columns
+    c
+    for c in dchi_df.columns
     if "chi2" in c.lower() and any(k in c.lower() for k in ("d", "deriv", "smooth"))
 )

-dchi_df["T_Gyr"]    = pd.to_numeric(dchi_df["T_Gyr"], errors="coerce")
-dchi_df[dchi_col]   = pd.to_numeric(dchi_df[dchi_col], errors="coerce")
-dchi_df             = dchi_df.dropna(subset=["T_Gyr", dchi_col])
-Td                  = dchi_df["T_Gyr"].to_numpy()
-dchi_raw            = dchi_df[dchi_col].to_numpy()
+dchi_df["T_Gyr"] = pd.to_numeric(dchi_df["T_Gyr"], errors="coerce")
+dchi_df[dchi_col] = pd.to_numeric(dchi_df[dchi_col], errors="coerce")
+dchi_df = dchi_df.dropna(subset=["T_Gyr", dchi_col])
+Td = dchi_df["T_Gyr"].to_numpy()
+dchi_raw = dchi_df[dchi_col].to_numpy()

 # — 3) Alignement + lissage —
 if dchi_raw.size == 0:
@@ -57,26 +59,20 @@ if dchi_raw.size == 0:
 else:
     # interpolation sur la même grille T
     if not np.allclose(Td, T):
-        dchi = np.interp(
-            np.log10(T),
-            np.log10(Td),
-            dchi_raw,
-            left=np.nan,
-            right=np.nan
-        )
+        dchi = np.interp(np.log10(T), np.log10(Td), dchi_raw, left=np.nan, right=np.nan)
     else:
         dchi = dchi_raw.copy()
     # lissage Savitzky–Golay (fenêtre impaire ≤ 7)
     if len(dchi) >= 5:
-        win = min(7, (len(dchi)//2)*2 + 1)
+        win = min(7, (len(dchi) // 2) * 2 + 1)
         dchi = savgol_filter(dchi, window_length=win, polyorder=3, mode="interp")

 # échelle réduite pour lisibilité
 dchi_scaled = dchi / 1e4

 # — 4) Recherche du minimum de χ² —
-imin     = int(np.nanargmin(chi2))
-Tmin     = T[imin]
+imin = int(np.nanargmin(chi2))
+Tmin = T[imin]
 chi2_min = chi2[imin]

 # — 5) Tracé —
@@ -91,48 +87,49 @@ ax1.grid(which="both", ls=":", lw=0.5, alpha=0.5)

 # bande ±1σ
 ax1.fill_between(
-    T, chi2 - sigma, chi2 + sigma,
-    color="tab:blue", alpha=0.12, label=r"$\pm1\sigma$"
+    T, chi2 - sigma, chi2 + sigma, color="tab:blue", alpha=0.12, label=r"$\pm1\sigma$"
 )
 # courbe χ²
-l1, = ax1.plot(
-    T, chi2,
-    lw=2, color="tab:blue", label=r"$\chi^2$"
-)
+(l1,) = ax1.plot(T, chi2, lw=2, color="tab:blue", label=r"$\chi^2$")

 # axe secondaire pour la dérivée
 ax2 = ax1.twinx()
-ax2.set_ylabel(
-    r"$\mathrm{d}\chi^2/\mathrm{d}T$ (×$10^{-4}$)",
-    color="tab:orange"
-)
+ax2.set_ylabel(r"$\mathrm{d}\chi^2/\mathrm{d}T$ (×$10^{-4}$)", color="tab:orange")
 ax2.tick_params(axis="y", labelcolor="tab:orange")
-l2, = ax2.plot(
-    T, dchi_scaled,
-    lw=2, color="tab:orange",
-    label=r"$\mathrm{d}\chi^2/\mathrm{d}T/10^{4}$"
+(l2,) = ax2.plot(
+    T,
+    dchi_scaled,
+    lw=2,
+    color="tab:orange",
+    label=r"$\mathrm{d}\chi^2/\mathrm{d}T/10^{4}$",
 )

 # point + flèche sur le minimum
 ax1.scatter(Tmin, chi2_min, s=60, color="k", zorder=4)
 start = (Tmin * 0.2, chi2_min * 0.8)
 arrow = FancyArrowPatch(
-    start, (Tmin, chi2_min),
-    arrowstyle="->", mutation_scale=12,
-    connectionstyle="arc3,rad=-0.35", color="k"
+    start,
+    (Tmin, chi2_min),
+    arrowstyle="->",
+    mutation_scale=12,
+    connectionstyle="arc3,rad=-0.35",
+    color="k",
 )
 ax1.add_patch(arrow)
 ax1.annotate(
-    fr"Min $\chi^2={chi2_min:.1f}$\n$T={Tmin:.2f}$\,Gyr",
-    xy=(Tmin, chi2_min), xytext=start,
-    ha="left", va="center", fontsize=10
+    rf"Min $\chi^2={chi2_min:.1f}$\n$T={Tmin:.2f}$\,Gyr",
+    xy=(Tmin, chi2_min),
+    xytext=start,
+    ha="left",
+    va="center",
+    fontsize=10,
 )

 # légende combinée
 ax1.legend(
     handles=[l1, l2],
     labels=[r"$\chi^2$", r"$\mathrm{d}\chi^2/\mathrm{d}T/10^4$"],
-    loc="upper right"
+    loc="upper right",
 )

 fig.tight_layout()
diff --git a/zz-scripts/chapter05/requirements.txt b/zz-scripts/chapter05/requirements.txt
index cf7aa40..7b23c54 100755
--- a/zz-scripts/chapter05/requirements.txt
+++ b/zz-scripts/chapter05/requirements.txt
@@ -1,11 +1,11 @@
 # Exigences pour le Chapitre 5 (Nucléosynthèse primordiale)

 1. **Environnement système**
-   * Système : Ubuntu 20.04 (WSL ou natif) ou équivalent Linux
-   * Espace disque : ≥ 500 Mo libre
+   * Système : Ubuntu 20.04 (WSL ou natif) ou équivalent Linux
+   * Espace disque : ≥ 500 Mo libre

 2. **Environnement Python**
-   * Python ≥ 3.9
+   * Python ≥ 3.9
    * Créer et activer un environnement virtuel spécifique :
      ```
      cd ~/MCGT
@@ -19,13 +19,13 @@
      ```

 3. **Données brutes & scripts d’extraction**
-   * **Jalons BBN**
-     – Fichier cible : `zz-data/chapter05/05_nucleosynthesis_milestones.csv`
-     – Extraction depuis sources Observations D/H & Yₚ via :
-       `zz-scripts/chapter05/utils/extract_bbn_milestones.py`
-   * **χ² vs T**
-     – Fichier cible : `zz-data/chapter05/05_chi2_nucleosynthesis_vs_T.csv`
-     – Calcul / export par :
+   * **Jalons BBN**
+     – Fichier cible : `zz-data/chapter05/05_nucleosynthesis_milestones.csv`
+     – Extraction depuis sources Observations D/H & Yₚ via :
+       `zz-scripts/chapter05/utils/extract_bbn_milestones.py`
+   * **χ² vs T**
+     – Fichier cible : `zz-data/chapter05/05_chi2_nucleosynthesis_vs_T.csv`
+     – Calcul / export par :
        `zz-scripts/chapter05/utils/extract_chi2_nucleosynthesis.py`

 4. **Structure du dépôt**
@@ -78,4 +78,3 @@ MCGT/
    * S’assurer qu’il n’y a pas de `NaN` ni `Inf` dans les CSV
    * `max_epsilon_primary` < 0.01 et `max_epsilon_order2` < 0.10 dans `05_nucleosynthesis_params.json`
    * Ouvrir chaque figure PNG dans `zz-figures/chapter05/` pour un contrôle visuel
-
diff --git a/zz-scripts/chapter06/generate_data_chapter06.py b/zz-scripts/chapter06/generate_data_chapter06.py
index 2397314..74cf691 100755
--- a/zz-scripts/chapter06/generate_data_chapter06.py
+++ b/zz-scripts/chapter06/generate_data_chapter06.py
@@ -1,5 +1,5 @@
 #!/usr/bin/env python3
-#---IMPORTS & CONFIGURATION---
+# ---IMPORTS & CONFIGURATION---

 import argparse
 import logging
@@ -8,91 +8,105 @@ import json
 import numpy as np
 import pandas as pd
 import camb
-from joblib import Parallel, delayed

 # Configuration du logging
-logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
+logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")

 # Parser CLI
 parser = argparse.ArgumentParser(
     description="Chapter 6 pipeline: generate CMB spectra for MCGT"
 )
-parser.add_argument('--alpha', type=float, default=0.0,
-                    help='Modulation amplitude α')
-parser.add_argument('--q0star', type=float, default=0.0,
-                    help='Effective curvature parameter q0star (Ω_k)')
-parser.add_argument('--export-derivative', action='store_true',
-                    help='Export derivative Δχ²/Δℓ')
+parser.add_argument("--alpha", type=float, default=0.0, help="Modulation amplitude α")
+parser.add_argument(
+    "--q0star",
+    type=float,
+    default=0.0,
+    help="Effective curvature parameter q0star (Ω_k)",
+)
+parser.add_argument(
+    "--export-derivative", action="store_true", help="Export derivative Δχ²/Δℓ"
+)
 args = parser.parse_args()

-ALPHA  = args.alpha
+ALPHA = args.alpha
 Q0STAR = args.q0star

 # Project root directory
-ROOT     = Path(__file__).resolve().parents[2]
+ROOT = Path(__file__).resolve().parents[2]

 # Config and data directories (English names)
-CONF_DIR = ROOT / 'zz-configuration'
-DATA_DIR = ROOT / 'zz-data' / 'chapter06'
-INI_DIR  = ROOT / '06-cmb'
+CONF_DIR = ROOT / "zz-configuration"
+DATA_DIR = ROOT / "zz-data" / "chapter06"
+INI_DIR = ROOT / "06-cmb"
 DATA_DIR.mkdir(parents=True, exist_ok=True)

-#---LOAD CHAPTER-2 SPECTRUM COEFFICIENTS---
+# ---LOAD CHAPTER-2 SPECTRUM COEFFICIENTS---

 # Note: chapter02 path uses English folder name 'chapter02' and spec file
-SPEC2_FILE = ROOT / 'zz-data' / 'chapter02' / '02_spec_spectrum.json'
-with open(SPEC2_FILE, 'r', encoding='utf-8') as f:
+SPEC2_FILE = ROOT / "zz-data" / "chapter02" / "02_spec_spectrum.json"
+with open(SPEC2_FILE, "r", encoding="utf-8") as f:
     spec2 = json.load(f)

-A_S0 = spec2['constantes']['A_s0'] if 'constantes' in spec2 else spec2.get('constants', {}).get('A_s0')
-NS0  = spec2['constantes']['ns0']  if 'constantes' in spec2 else spec2.get('constants', {}).get('ns0')
-C1   = spec2.get('coefficients', {}).get('c1')
-C2   = spec2.get('coefficients', {}).get('c2')
+A_S0 = (
+    spec2["constantes"]["A_s0"]
+    if "constantes" in spec2
+    else spec2.get("constants", {}).get("A_s0")
+)
+NS0 = (
+    spec2["constantes"]["ns0"]
+    if "constantes" in spec2
+    else spec2.get("constants", {}).get("ns0")
+)
+C1 = spec2.get("coefficients", {}).get("c1")
+C2 = spec2.get("coefficients", {}).get("c2")

 logging.info(f"Chapter02 spectrum loaded: A_s0={A_S0}, ns0={NS0}, c1={C1}, c2={C2}")
 logging.info(f"MCGT parameters: alpha={ALPHA}, q0star={Q0STAR}")

-#---OPTIONAL EXPORT: A_s(α) and n_s(α) over alpha grid---
+# ---OPTIONAL EXPORT: A_s(α) and n_s(α) over alpha grid---

 alpha_vals = np.arange(-0.1, 0.1001, 0.01)
-df_alpha = pd.DataFrame({
-    'alpha': alpha_vals,
-    'A_s':    A_S0 * (1 + C1 * alpha_vals),
-    'n_s':    NS0 + C2 * alpha_vals
-})
-OUT_ALPHA = DATA_DIR / '06_alpha_evolution.csv'
+df_alpha = pd.DataFrame(
+    {
+        "alpha": alpha_vals,
+        "A_s": A_S0 * (1 + C1 * alpha_vals),
+        "n_s": NS0 + C2 * alpha_vals,
+    }
+)
+OUT_ALPHA = DATA_DIR / "06_alpha_evolution.csv"
 df_alpha.to_csv(OUT_ALPHA, index=False)
 logging.info(f"06_alpha_evolution.csv generated → {OUT_ALPHA}")

-#---CMB CONSTANTS---
+# ---CMB CONSTANTS---

-ELL_MIN         = 2
-ELL_MAX         = 3000
-PK_KMAX         = 10.0
-DERIV_WINDOW    = 7
+ELL_MIN = 2
+ELL_MAX = 3000
+PK_KMAX = 10.0
+DERIV_WINDOW = 7
 DERIV_POLYORDER = 3

 # Base cosmological parameters (Planck 2018)
 cosmo_params = {
-    'H0':    67.36,
-    'ombh2': 0.02237,
-    'omch2': 0.1200,
-    'tau':   0.0544,
-    'omk':   0.0,
-    'mnu':   0.06,
+    "H0": 67.36,
+    "ombh2": 0.02237,
+    "omch2": 0.1200,
+    "tau": 0.0544,
+    "omk": 0.0,
+    "mnu": 0.06,
 }

 # Output files (English names)
-CLS_LCDM_DAT      = DATA_DIR / '06_cls_lcdm_spectrum.dat'
-CLS_MCGT_DAT      = DATA_DIR / '06_cls_spectrum.dat'
-DELTA_CLS_CSV     = DATA_DIR / '06_delta_cls.csv'
-DELTA_CLS_REL_CSV = DATA_DIR / '06_delta_cls_relative.csv'
-JSON_PARAMS       = DATA_DIR / '06_params_cmb.json'
-CSV_RS_SCAN       = DATA_DIR / '06_delta_rs_scan.csv'
-CSV_RS_SCAN_FULL  = DATA_DIR / '06_delta_rs_scan_2d.csv'
-CSV_CHI2_2D       = DATA_DIR / '06_cmb_chi2_scan_2d.csv'
+CLS_LCDM_DAT = DATA_DIR / "06_cls_lcdm_spectrum.dat"
+CLS_MCGT_DAT = DATA_DIR / "06_cls_spectrum.dat"
+DELTA_CLS_CSV = DATA_DIR / "06_delta_cls.csv"
+DELTA_CLS_REL_CSV = DATA_DIR / "06_delta_cls_relative.csv"
+JSON_PARAMS = DATA_DIR / "06_params_cmb.json"
+CSV_RS_SCAN = DATA_DIR / "06_delta_rs_scan.csv"
+CSV_RS_SCAN_FULL = DATA_DIR / "06_delta_rs_scan_2d.csv"
+CSV_CHI2_2D = DATA_DIR / "06_cmb_chi2_scan_2d.csv"
+
+# ---MCGT PHYSICAL INJECTION FUNCTION FOR CAMB---

-#---MCGT PHYSICAL INJECTION FUNCTION FOR CAMB---

 def tweak_for_mcgt(pars, alpha, q0star):
     """
@@ -102,33 +116,30 @@ def tweak_for_mcgt(pars, alpha, q0star):
       • (Optional) inject ΔT_m(k) from file '06_delta_Tm_scan.csv'
     """
     # 1) Modulate primordial spectrum
-    pars.InitPower.set_params(
-        As = A_S0 * (1 + C1 * alpha),
-        ns = NS0 + C2 * alpha
-    )
+    pars.InitPower.set_params(As=A_S0 * (1 + C1 * alpha), ns=NS0 + C2 * alpha)

     # 2) Update curvature and base cosmology
     pars.set_cosmology(
-        H0    = cosmo_params['H0'],
-        ombh2 = cosmo_params['ombh2'],
-        omch2 = cosmo_params['omch2'],
-        tau   = cosmo_params['tau'],
-        omk   = q0star,
-        mnu   = cosmo_params['mnu']
+        H0=cosmo_params["H0"],
+        ombh2=cosmo_params["ombh2"],
+        omch2=cosmo_params["omch2"],
+        tau=cosmo_params["tau"],
+        omk=q0star,
+        mnu=cosmo_params["mnu"],
     )

     # 3) Optional post-processing for matter transfer ΔT_m(k)
     def post_process(results):
         try:
-            tm_obj  = results.get_matter_transfer_data()
-            k_vals  = tm_obj.q
+            tm_obj = results.get_matter_transfer_data()
+            k_vals = tm_obj.q
             tm_data = tm_obj.transfer_data[0, :, 0]
-            path = DATA_DIR / '06_delta_Tm_scan.csv'
+            path = DATA_DIR / "06_delta_Tm_scan.csv"
             if path.exists():
-                delta_k, dTm = np.loadtxt(path, delimiter=',', skiprows=1, unpack=True)
+                delta_k, dTm = np.loadtxt(path, delimiter=",", skiprows=1, unpack=True)
                 tm_data += np.interp(k_vals, delta_k, dTm)
                 tm_obj.transfer_data[0, :, 0] = tm_data
-                if hasattr(results, 'replace_transfer'):
+                if hasattr(results, "replace_transfer"):
                     results.replace_transfer(0, tm_data)
         except Exception:
             # If matter transfer access fails, silently continue
@@ -137,8 +148,9 @@ def tweak_for_mcgt(pars, alpha, q0star):

     pars.post_process = post_process

-#---1. LOAD pdot_plateau_z (configuration)---
-PDOT_FILE = CONF_DIR / 'pdot_plateau_z.dat'
+
+# ---1. LOAD pdot_plateau_z (configuration)---
+PDOT_FILE = CONF_DIR / "pdot_plateau_z.dat"
 logging.info("1) Reading pdot_plateau_z.dat …")
 z_h, pdot = np.loadtxt(PDOT_FILE, unpack=True)
 if z_h.size == 0 or pdot.size == 0:
@@ -146,28 +158,33 @@ if z_h.size == 0 or pdot.size == 0:

 z_grid = np.linspace(0, 50, 100)  # redshift grid for matter_power

-#---2. ΛCDM Cℓ SPECTRUM (CAMB)---
+# ---2. ΛCDM Cℓ SPECTRUM (CAMB)---
 logging.info("2) Computing ΛCDM spectrum …")
 pars0 = camb.CAMBparams()
 pars0.set_for_lmax(ELL_MAX, max_eta_k=40000)
 pars0.set_cosmology(
-    H0    = cosmo_params['H0'],
-    ombh2 = cosmo_params['ombh2'],
-    omch2 = cosmo_params['omch2'],
-    tau   = cosmo_params['tau'],
-    omk   = cosmo_params['omk'],
-    mnu   = cosmo_params['mnu']
+    H0=cosmo_params["H0"],
+    ombh2=cosmo_params["ombh2"],
+    omch2=cosmo_params["omch2"],
+    tau=cosmo_params["tau"],
+    omk=cosmo_params["omk"],
+    mnu=cosmo_params["mnu"],
 )
 pars0.InitPower.set_params(As=A_S0, ns=NS0)
-res0  = camb.get_results(pars0)
-cmb0  = res0.get_cmb_power_spectra(pars0, lmax=ELL_MAX)['total'][:, 0]
-cls0  = cmb0[:ELL_MAX+1]
-ells  = np.arange(cls0.size)
-np.savetxt(CLS_LCDM_DAT, np.column_stack([ells, cls0]),
-           header="# ell   Cl_LCDM", comments='', fmt='%d %.6e')
+res0 = camb.get_results(pars0)
+cmb0 = res0.get_cmb_power_spectra(pars0, lmax=ELL_MAX)["total"][:, 0]
+cls0 = cmb0[: ELL_MAX + 1]
+ells = np.arange(cls0.size)
+np.savetxt(
+    CLS_LCDM_DAT,
+    np.column_stack([ells, cls0]),
+    header="# ell   Cl_LCDM",
+    comments="",
+    fmt="%d %.6e",
+)
 logging.info(f"ΛCDM spectrum saved → {CLS_LCDM_DAT}")

-#---3. MCGT Cℓ SPECTRUM (α, q0*)---
+# ---3. MCGT Cℓ SPECTRUM (α, q0*)---
 logging.info("3) Computing MCGT spectrum …")
 pars1 = camb.CAMBparams()
 pars1.set_for_lmax(ELL_MAX, max_eta_k=40000)
@@ -176,56 +193,61 @@ tweak_for_mcgt(pars1, alpha=ALPHA, q0star=Q0STAR)
 # Configure matter power for MCGT
 pars1.set_matter_power(redshifts=z_grid, kmax=PK_KMAX)
 res1 = camb.get_results(pars1)
-if hasattr(pars1, 'post_process'):
+if hasattr(pars1, "post_process"):
     res1 = pars1.post_process(res1)
-cmb1 = res1.get_cmb_power_spectra(pars1, lmax=ELL_MAX)['total'][:, 0]
-cls1 = cmb1[:ells.size]
+cmb1 = res1.get_cmb_power_spectra(pars1, lmax=ELL_MAX)["total"][:, 0]
+cls1 = cmb1[: ells.size]
 np.savetxt(
     CLS_MCGT_DAT,
     np.column_stack([ells, cls1]),
-    header="# ell   Cl_MCGT", comments='', fmt='%d %.6e'
+    header="# ell   Cl_MCGT",
+    comments="",
+    fmt="%d %.6e",
 )
 logging.info(f"MCGT spectrum saved → {CLS_MCGT_DAT}")

-#---4. ΔCℓ & relative ΔCℓ---
+# ---4. ΔCℓ & relative ΔCℓ---
 logging.info("4) Computing ΔCℓ …")
-delta     = cls1 - cls0
+delta = cls1 - cls0
 delta_rel = np.divide(delta, cls0, out=np.zeros_like(delta), where=cls0 > 0)
-dfd = pd.DataFrame({'ell': ells, 'delta_Cl': delta, 'delta_Cl_rel': delta_rel})
-dfd[['ell', 'delta_Cl']].to_csv(DELTA_CLS_CSV, index=False)
-dfd[['ell', 'delta_Cl_rel']].to_csv(DELTA_CLS_REL_CSV, index=False)
+dfd = pd.DataFrame({"ell": ells, "delta_Cl": delta, "delta_Cl_rel": delta_rel})
+dfd[["ell", "delta_Cl"]].to_csv(DELTA_CLS_CSV, index=False)
+dfd[["ell", "delta_Cl_rel"]].to_csv(DELTA_CLS_REL_CSV, index=False)
 logging.info(f"ΔCℓ → {DELTA_CLS_CSV}, {DELTA_CLS_REL_CSV}")

-#---5. SAVE PARAMETERS---
+# ---5. SAVE PARAMETERS---
 logging.info("5) Saving parameters …")
 params_out = {
-    'alpha':               ALPHA,
-    'q0star':              Q0STAR,
-    'ell_min':             ELL_MIN,
-    'ell_max':             ELL_MAX,
-    'n_points':            int(len(ells)),
-    'thresholds':          {'primary': 0.01, 'order2': 0.10},
-    'derivative_window':   DERIV_WINDOW,
-    'derivative_polyorder':DERIV_POLYORDER,
-    **{k: cosmo_params[k] for k in ['H0', 'ombh2', 'omch2', 'tau', 'mnu']},
-    'As0':                 A_S0,
-    'ns0':                 NS0,
-    'c1':                  C1,
-    'c2':                  C2,
-    'max_delta_Cl_rel':    float(np.nanmax(np.abs(delta_rel)))
+    "alpha": ALPHA,
+    "q0star": Q0STAR,
+    "ell_min": ELL_MIN,
+    "ell_max": ELL_MAX,
+    "n_points": int(len(ells)),
+    "thresholds": {"primary": 0.01, "order2": 0.10},
+    "derivative_window": DERIV_WINDOW,
+    "derivative_polyorder": DERIV_POLYORDER,
+    **{k: cosmo_params[k] for k in ["H0", "ombh2", "omch2", "tau", "mnu"]},
+    "As0": A_S0,
+    "ns0": NS0,
+    "c1": C1,
+    "c2": C2,
+    "max_delta_Cl_rel": float(np.nanmax(np.abs(delta_rel))),
 }
-with open(JSON_PARAMS, 'w') as f:
+with open(JSON_PARAMS, "w") as f:
     json.dump(params_out, f, indent=2)
 logging.info(f"JSON parameters → {JSON_PARAMS}")

-#---6. SCAN Δr_s AS FUNCTION OF q0*---
+# ---6. SCAN Δr_s AS FUNCTION OF q0*---
 logging.info("6) Scanning Δr_s …")
+
+
 def compute_rs(alpha, q0star):
     p = camb.CAMBparams()
     p.set_for_lmax(ELL_MAX, max_eta_k=40000)
     tweak_for_mcgt(p, alpha=alpha, q0star=q0star)
     return camb.get_results(p).get_derived_params()["rdrag"]

+
 # reference r_s at (ALPHA, Q0STAR)
 rs_ref = compute_rs(ALPHA, Q0STAR)

@@ -233,88 +255,89 @@ q0_grid = np.linspace(-0.1, 0.1, 41)
 rows_rs = []
 for q0 in q0_grid:
     rs_i = compute_rs(ALPHA, q0)
-    rows_rs.append({
-        "q0star":       q0,
-        "r_s":          rs_i,
-        "delta_rs_rel": (rs_i - rs_ref) / rs_ref
-    })
+    rows_rs.append(
+        {"q0star": q0, "r_s": rs_i, "delta_rs_rel": (rs_i - rs_ref) / rs_ref}
+    )
 df_rs = pd.DataFrame(rows_rs)
-df_rs[["q0star", "delta_rs_rel"]].to_csv(CSV_RS_SCAN,      index=False)
-df_rs.to_csv(CSV_RS_SCAN_FULL,                    index=False)
+df_rs[["q0star", "delta_rs_rel"]].to_csv(CSV_RS_SCAN, index=False)
+df_rs.to_csv(CSV_RS_SCAN_FULL, index=False)
 logging.info(f"Δr_s scan (1D)     → {CSV_RS_SCAN}")
 logging.info(f"Δr_s full scan     → {CSV_RS_SCAN_FULL}")

-#---7. 2D SCAN (α, q0*): cosmic-variance χ²---
+# ---7. 2D SCAN (α, q0*): cosmic-variance χ²---
 logging.info("7) 2D Δχ² scan (cosmic variance) …")
+
+
 def compute_chi2_cv(alpha, q0star):
     p1 = camb.CAMBparams()
     p1.set_for_lmax(ELL_MAX, max_eta_k=40000)
     tweak_for_mcgt(p1, alpha=alpha, q0star=q0star)
     p1.set_matter_power(redshifts=z_grid, kmax=PK_KMAX)
     res_mcgt = camb.get_results(p1)
-    if hasattr(p1, 'post_process'):
+    if hasattr(p1, "post_process"):
         res_mcgt = p1.post_process(res_mcgt)
-    cls_mcgt = res_mcgt.get_cmb_power_spectra(p1, lmax=ells.size-1)['total'][:,0]
-    cls_mcgt = cls_mcgt[:ells.size]
+    cls_mcgt = res_mcgt.get_cmb_power_spectra(p1, lmax=ells.size - 1)["total"][:, 0]
+    cls_mcgt = cls_mcgt[: ells.size]

-    Delta   = cls_mcgt - cls0
-    var = 2.0 * cls0**2 / (2*ells + 1)
+    Delta = cls_mcgt - cls0
+    var = 2.0 * cls0**2 / (2 * ells + 1)
     mask = (ells >= ELL_MIN) & (var > 0)
-    chi2 = np.sum((Delta[mask]**2) / var[mask])
+    chi2 = np.sum((Delta[mask] ** 2) / var[mask])
     return float(chi2)

+
 alpha_grid = np.linspace(-0.1, 0.1, 21)
-q0_grid2  = np.linspace(-0.1, 0.1, 21)
-rows2d    = []
+q0_grid2 = np.linspace(-0.1, 0.1, 21)
+rows2d = []
 for a in alpha_grid:
     for q in q0_grid2:
-        rows2d.append({'alpha': a, 'q0star': q, 'chi2': compute_chi2_cv(a, q)})
+        rows2d.append({"alpha": a, "q0star": q, "chi2": compute_chi2_cv(a, q)})

-df2d = pd.DataFrame(rows2d, columns=['alpha','q0star','chi2'])
+df2d = pd.DataFrame(rows2d, columns=["alpha", "q0star", "chi2"])
 df2d.to_csv(CSV_CHI2_2D, index=False)
 logging.info(f"2D Δχ² scan → {CSV_CHI2_2D}")

-#---8. ΔT_m(k) BETWEEN MCGT AND ΛCDM---
+# ---8. ΔT_m(k) BETWEEN MCGT AND ΛCDM---

 logging.info("8) Exporting ΔT_m(k) …")
 # ΛCDM matter transfer
 pars0_tm = camb.CAMBparams()
 pars0_tm.set_cosmology(
-    H0    = cosmo_params['H0'],
-    ombh2 = cosmo_params['ombh2'],
-    omch2 = cosmo_params['omch2'],
-    tau   = cosmo_params['tau'],
-    omk   = cosmo_params['omk'],
-    mnu   = cosmo_params['mnu']
+    H0=cosmo_params["H0"],
+    ombh2=cosmo_params["ombh2"],
+    omch2=cosmo_params["omch2"],
+    tau=cosmo_params["tau"],
+    omk=cosmo_params["omk"],
+    mnu=cosmo_params["mnu"],
 )
 pars0_tm.InitPower.set_params(As=A_S0, ns=NS0)
 pars0_tm.set_matter_power(redshifts=[0], kmax=PK_KMAX)
-tm0_obj  = camb.get_results(pars0_tm).get_matter_transfer_data()
-k_vals   = tm0_obj.q
+tm0_obj = camb.get_results(pars0_tm).get_matter_transfer_data()
+k_vals = tm0_obj.q
 tm0_data = tm0_obj.transfer_data[0, :, 0]

 pars1_tm = camb.CAMBparams()
 tweak_for_mcgt(pars1_tm, alpha=ALPHA, q0star=Q0STAR)
 pars1_tm.set_matter_power(redshifts=[0], kmax=PK_KMAX)
-if hasattr(pars1_tm, 'post_process'):
+if hasattr(pars1_tm, "post_process"):
     camb_results = pars1_tm.post_process(camb.get_results(pars1_tm))
 else:
     camb_results = camb.get_results(pars1_tm)
-tm1_obj         = camb_results.get_matter_transfer_data()
-k1              = tm1_obj.q
-tm1_data        = tm1_obj.transfer_data[0, :, 0]
+tm1_obj = camb_results.get_matter_transfer_data()
+k1 = tm1_obj.q
+tm1_data = tm1_obj.transfer_data[0, :, 0]
 tm1_data_interp = np.interp(k_vals, k1, tm1_data)

 delta_Tm = tm1_data_interp - tm0_data

-OUT_TMDAT = DATA_DIR / '06_delta_Tm_scan.csv'
-with open(OUT_TMDAT, 'w') as f:
+OUT_TMDAT = DATA_DIR / "06_delta_Tm_scan.csv"
+with open(OUT_TMDAT, "w") as f:
     f.write("# k, delta_Tm\n")
     for k, dTm in zip(k_vals, delta_Tm):
         f.write(f"{k:.6e}, {dTm:.6e}\n")
 logging.info(f"ΔT_m(k) exported → {OUT_TMDAT}")

-#---9. (Optional) duplicate alpha-evolution (kept for compatibility)---
+# ---9. (Optional) duplicate alpha-evolution (kept for compatibility)---
 logging.info("9) (Optional) Regenerating 06_alpha_evolution.csv …")
 df_alpha.to_csv(OUT_ALPHA, index=False)
 logging.info(f"06_alpha_evolution.csv overwritten → {OUT_ALPHA}")
diff --git a/zz-scripts/chapter06/generate_pdot_plateau_vs_z.py b/zz-scripts/chapter06/generate_pdot_plateau_vs_z.py
index 18380ef..1c67f73 100755
--- a/zz-scripts/chapter06/generate_pdot_plateau_vs_z.py
+++ b/zz-scripts/chapter06/generate_pdot_plateau_vs_z.py
@@ -4,40 +4,45 @@ import numpy as np
 import argparse

 # --- Configuration logging ---
-logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
+logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")

 # --- Chemins ---
-ROOT     = Path(__file__).resolve().parents[2]
+ROOT = Path(__file__).resolve().parents[2]
 # English folder name for configuration
-CONF_DIR = ROOT / 'zz-config'
-OUT_FILE = CONF_DIR / 'pdot_plateau_z.dat'
+CONF_DIR = ROOT / "zz-config"
+OUT_FILE = CONF_DIR / "pdot_plateau_z.dat"
 OUT_FILE.parent.mkdir(parents=True, exist_ok=True)

 # --- Paramètres cosmologiques par défaut ---
-H0    = 67.4
+H0 = 67.4
 ombh2 = 0.0224
 omch2 = 0.12
-tau   = 0.06
+tau = 0.06

 # --- CLI ---
 parser = argparse.ArgumentParser(description="Génère pdot_plateau_z.dat")
-parser.add_argument('--zmin',    type=float, default=1e-4, help="Redshift minimal")
-parser.add_argument('--zmax',    type=float, default=1e5,   help="Redshift maximal")
-parser.add_argument('--npoints', type=int,   default=1000,  help="Nombre de points")
+parser.add_argument("--zmin", type=float, default=1e-4, help="Redshift minimal")
+parser.add_argument("--zmax", type=float, default=1e5, help="Redshift maximal")
+parser.add_argument("--npoints", type=int, default=1000, help="Nombre de points")
 args = parser.parse_args()

 # --- Grille de redshift log-uniforme ---
-log_z  = np.linspace(np.log10(args.zmin), np.log10(args.zmax), args.npoints)
+log_z = np.linspace(np.log10(args.zmin), np.log10(args.zmax), args.npoints)
 z_grid = 10**log_z

 # --- Calcul de H(z)/H0 pour univers plat ---
 h = H0 / 100.0
 Omega_m = (ombh2 + omch2) / (h**2)
 Omega_L = 1.0 - Omega_m
-Hz_over_H0 = np.sqrt(Omega_m * (1 + z_grid)**3 + Omega_L)
+Hz_over_H0 = np.sqrt(Omega_m * (1 + z_grid) ** 3 + Omega_L)

 # --- Export ---
 header = "# z    H_over_H0"
-np.savetxt(OUT_FILE, np.column_stack([z_grid, Hz_over_H0]),
-           header=header, comments='', fmt='%.6e  %.6e')
+np.savetxt(
+    OUT_FILE,
+    np.column_stack([z_grid, Hz_over_H0]),
+    header=header,
+    comments="",
+    fmt="%.6e  %.6e",
+)
 logging.info(f"Fichier généré → {OUT_FILE}")
diff --git a/zz-scripts/chapter06/plot_fig01_cmb_dataflow_diagram.py b/zz-scripts/chapter06/plot_fig01_cmb_dataflow_diagram.py
index 729d8c5..4c2ec60 100755
--- a/zz-scripts/chapter06/plot_fig01_cmb_dataflow_diagram.py
+++ b/zz-scripts/chapter06/plot_fig01_cmb_dataflow_diagram.py
@@ -4,10 +4,10 @@ import matplotlib.pyplot as plt
 from matplotlib.patches import Rectangle, FancyArrowPatch

 # --- Logging ---
-logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
+logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")

 # --- Paths ---
-ROOT    = Path(__file__).resolve().parents[2]
+ROOT = Path(__file__).resolve().parents[2]
 FIG_DIR = ROOT / "zz-figures" / "chapter06"
 FIG_DIR.mkdir(parents=True, exist_ok=True)
 OUT_PNG = FIG_DIR / "fig_01_schema_data_flow_cmb.png"
@@ -17,51 +17,75 @@ fig, ax = plt.subplots(figsize=(10, 6), dpi=300)
 ax.axis("off")
 fig.suptitle(
     "Pipeline de génération des données CMB (Chapitre 6)",
-    fontsize=14, fontweight="bold", y=0.96
+    fontsize=14,
+    fontweight="bold",
+    y=0.96,
 )

 # --- Block parameters ---
-W, H   = 0.26, 0.20       # largeur/hauteur des blocs
-Ymid   = 0.45             # position centrale en Y
-DY     = 0.25             # décalage vertical standard
+W, H = 0.26, 0.20  # largeur/hauteur des blocs
+Ymid = 0.45  # position centrale en Y
+DY = 0.25  # décalage vertical standard

 # --- Blocks definitions ---
 blocks = {
-    "in":   (0.05,      Ymid,       "pdot_plateau_z.dat",                     "#d7d7d7"),
-    "scr":  (0.36,      Ymid,       "generate_chapter06_data.py",           "#a9dfbf"),
-    "data": (0.67,      Ymid + DY,  "06_cls_*.dat\n06_delta_*.csv\n06_delta_rs_*.csv\n06_cmb_chi2_scan2D.csv\n06_params_cmb.json", "#d7d7d7"),
-    "fig":  (0.67,      Ymid - DY,  "fig_02.png\nfig_03.png\nfig_04.png\nfig_05.png",               "#d7d7d7"),
+    "in": (0.05, Ymid, "pdot_plateau_z.dat", "#d7d7d7"),
+    "scr": (0.36, Ymid, "generate_chapter06_data.py", "#a9dfbf"),
+    "data": (
+        0.67,
+        Ymid + DY,
+        "06_cls_*.dat\n06_delta_*.csv\n06_delta_rs_*.csv\n06_cmb_chi2_scan2D.csv\n06_params_cmb.json",
+        "#d7d7d7",
+    ),
+    "fig": (
+        0.67,
+        Ymid - DY,
+        "fig_02.png\nfig_03.png\nfig_04.png\nfig_05.png",
+        "#d7d7d7",
+    ),
 }

 # --- Draw blocks ---
 for key, (x, y, label, color) in blocks.items():
-    ax.add_patch(Rectangle((x, y), W, H,
-                           facecolor=color, edgecolor="k", lw=1.2))
-    ax.text(x + W/2, y + H/2, label,
-            ha="center", va="center",
-            fontsize=8, family="monospace")
+    ax.add_patch(Rectangle((x, y), W, H, facecolor=color, edgecolor="k", lw=1.2))
+    ax.text(
+        x + W / 2,
+        y + H / 2,
+        label,
+        ha="center",
+        va="center",
+        fontsize=8,
+        family="monospace",
+    )
+

 # --- Arrow helpers ---
-def east_center(x, y): return (x + W, y + H/2)
-def west_center(x, y): return (x,     y + H/2)
+def east_center(x, y):
+    return (x + W, y + H / 2)
+
+
+def west_center(x, y):
+    return (x, y + H / 2)
+

 def draw_arrow(start, end, text, x_off=0, y_off=0):
-    ax.add_patch(FancyArrowPatch(
-        start, end,
-        arrowstyle="-|>", mutation_scale=15,
-        lw=1.3, color="k"
-    ))
-    xm = 0.5*(start[0] + end[0]) + x_off
-    ym = 0.5*(start[1] + end[1]) + y_off
+    ax.add_patch(
+        FancyArrowPatch(
+            start, end, arrowstyle="-|>", mutation_scale=15, lw=1.3, color="k"
+        )
+    )
+    xm = 0.5 * (start[0] + end[0]) + x_off
+    ym = 0.5 * (start[1] + end[1]) + y_off
     ax.text(xm, ym, text, ha="center", va="center", fontsize=9)

+
 # --- Draw arrows with adjusted offsets ---
 # 1) input -> script : label déplacé vers le bas
 draw_arrow(
     east_center(*blocks["in"][:2]),
     west_center(*blocks["scr"][:2]),
     "1. Lecture pdot",
-    y_off=-DY/1.8
+    y_off=-DY / 1.8,
 )

 # 2) script -> data
@@ -69,17 +93,17 @@ draw_arrow(
     east_center(*blocks["scr"][:2]),
     west_center(*blocks["data"][:2]),
     "2. Génération données",
-    x_off=+DY/3,
-    y_off=-DY/8
+    x_off=+DY / 3,
+    y_off=-DY / 8,
 )

-# 3) script
+# 3) script
 draw_arrow(
     east_center(*blocks["scr"][:2]),
     west_center(*blocks["fig"][:2]),
     "3. Export PNG",
-    x_off=+DY/4,
-    y_off=+DY/8
+    x_off=+DY / 4,
+    y_off=+DY / 8,
 )

 # --- Finalize and save ---
diff --git a/zz-scripts/chapter06/plot_fig02_cls_lcdm_vs_mcgt.py b/zz-scripts/chapter06/plot_fig02_cls_lcdm_vs_mcgt.py
index 87a7233..bc8453f 100755
--- a/zz-scripts/chapter06/plot_fig02_cls_lcdm_vs_mcgt.py
+++ b/zz-scripts/chapter06/plot_fig02_cls_lcdm_vs_mcgt.py
@@ -2,108 +2,123 @@
 """
 Script de tracé fig_02_cls_lcdm_vs_mcgt pour Chapitre 6 (Rayonnement CMB)
 """
-#--- IMPORTS & CONFIGURATION ---
+
+# --- IMPORTS & CONFIGURATION ---
 import logging
 from pathlib import Path
 import json
-import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt
 from mpl_toolkits.axes_grid1.inset_locator import inset_axes

 # Logging
-logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
+logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")

 # Paths
-ROOT     = Path(__file__).resolve().parents[2]
-DATA_DIR = ROOT / 'zz-data' / 'chapter06'
-FIG_DIR  = ROOT / 'zz-figures' / 'chapter06'
+ROOT = Path(__file__).resolve().parents[2]
+DATA_DIR = ROOT / "zz-data" / "chapter06"
+FIG_DIR = ROOT / "zz-figures" / "chapter06"
 FIG_DIR.mkdir(parents=True, exist_ok=True)

-CLS_LCDM_DAT = DATA_DIR / '06_cls_lcdm_spectrum.dat'
-CLS_MCGT_DAT = DATA_DIR / '06_cls_spectrum.dat'
-JSON_PARAMS  = DATA_DIR / '06_params_cmb.json'
-OUT_PNG      = FIG_DIR  / 'fig_02_cls_lcdm_vs_mcgt.png'
+CLS_LCDM_DAT = DATA_DIR / "06_cls_lcdm_spectrum.dat"
+CLS_MCGT_DAT = DATA_DIR / "06_cls_spectrum.dat"
+JSON_PARAMS = DATA_DIR / "06_params_cmb.json"
+OUT_PNG = FIG_DIR / "fig_02_cls_lcdm_vs_mcgt.png"

 # Load injection parameters
-with open(JSON_PARAMS, 'r', encoding='utf-8') as f:
+with open(JSON_PARAMS, "r", encoding="utf-8") as f:
     params = json.load(f)
-ALPHA  = params.get('alpha', None)
-Q0STAR = params.get('q0star', None)
+ALPHA = params.get("alpha", None)
+Q0STAR = params.get("q0star", None)
 logging.info(f"Tracé fig_02 avec α={ALPHA}, q0*={Q0STAR}")

 # Load and merge spectra
-cols_l   = ['ell', 'Cl_LCDM']
-cols_m   = ['ell', 'Cl_MCGT']
-df_lcdm  = pd.read_csv(CLS_LCDM_DAT, sep=r'\s+', names=cols_l, comment='#')
-df_mcgt  = pd.read_csv(CLS_MCGT_DAT, sep=r'\s+', names=cols_m, comment='#')
-df       = pd.merge(df_lcdm, df_mcgt, on='ell')
-df       = df[df['ell'] >= 2]
+cols_l = ["ell", "Cl_LCDM"]
+cols_m = ["ell", "Cl_MCGT"]
+df_lcdm = pd.read_csv(CLS_LCDM_DAT, sep=r"\s+", names=cols_l, comment="#")
+df_mcgt = pd.read_csv(CLS_MCGT_DAT, sep=r"\s+", names=cols_m, comment="#")
+df = pd.merge(df_lcdm, df_mcgt, on="ell")
+df = df[df["ell"] >= 2]

-ells      = df['ell'].values
-cl0       = df['Cl_LCDM'].values
-cl1       = df['Cl_MCGT'].values
+ells = df["ell"].values
+cl0 = df["Cl_LCDM"].values
+cl1 = df["Cl_MCGT"].values
 delta_rel = (cl1 - cl0) / cl0

 # Plot main comparison
 fig, ax = plt.subplots(figsize=(10, 6), dpi=300, constrained_layout=True)
-ax.plot(ells, cl0, linestyle='--', linewidth=2, label=r'$\Lambda$CDM', alpha=0.7)
-ax.plot(ells, cl1, linestyle='-',  linewidth=2, label='MCGT',      alpha=0.7, color='tab:orange')
+ax.plot(ells, cl0, linestyle="--", linewidth=2, label=r"$\Lambda$CDM", alpha=0.7)
+ax.plot(
+    ells, cl1, linestyle="-", linewidth=2, label="MCGT", alpha=0.7, color="tab:orange"
+)

 # Shade region where MCGT > ΛCDM
-ax.fill_between(ells, cl0, cl1, where=cl1>cl0, color='tab:red', alpha=0.15)
+ax.fill_between(ells, cl0, cl1, where=cl1 > cl0, color="tab:red", alpha=0.15)

-ax.set_xscale('log')
-ax.set_yscale('log')
+ax.set_xscale("log")
+ax.set_yscale("log")
 ax.set_xlim(2, 3000)
 ymin = min(cl0.min(), cl1.min()) * 0.8
 ymax = max(cl0.max(), cl1.max()) * 1.2
 ax.set_ylim(ymin, ymax)

-ax.set_xlabel(r'Multipôle $\ell$')
-ax.set_ylabel(r'$C_{\ell}\;[\mu\mathrm{K}^2]$')
-ax.grid(True, which='both', linestyle=':', linewidth=0.5)
-ax.legend(loc='upper right', frameon=False)
+ax.set_xlabel(r"Multipôle $\ell$")
+ax.set_ylabel(r"$C_{\ell}\;[\mu\mathrm{K}^2]$")
+ax.grid(True, which="both", linestyle=":", linewidth=0.5)
+ax.legend(loc="upper right", frameon=False)

 # Inset 1: relative difference ΔCℓ / Cℓ (bas-gauche, décalé à droite et en haut)
 axins1 = inset_axes(
     ax,
-    width="85%", height="85%",
-    bbox_to_anchor=(0.06, 0.06, 0.30, 0.35),
+    width="85%",
+    height="85%",
+    bbox_to_anchor=(0.06, 0.06, 0.30, 0.35),
     bbox_transform=ax.transAxes,
-    borderpad=0
+    borderpad=0,
 )
-axins1.plot(ells, delta_rel, linestyle='-', color='tab:green')
-axins1.set_xscale('log')
+axins1.plot(ells, delta_rel, linestyle="-", color="tab:green")
+axins1.set_xscale("log")
 axins1.set_ylim(-0.02, 0.02)
-axins1.set_xlabel(r'$\ell$', fontsize=8)
-axins1.set_ylabel(r'$\Delta C_{\ell}/C_{\ell}$', fontsize=8)
-axins1.grid(True, which='both', linestyle=':', linewidth=0.5)
+axins1.set_xlabel(r"$\ell$", fontsize=8)
+axins1.set_ylabel(r"$\Delta C_{\ell}/C_{\ell}$", fontsize=8)
+axins1.grid(True, which="both", linestyle=":", linewidth=0.5)
 axins1.tick_params(labelsize=7)

 # Inset 2: zoom ℓ ≃ 200–300, placé juste à droite du premier inset
 axins2 = inset_axes(
     ax,
-    width="85%", height="85%",
-    bbox_to_anchor=(0.5, 0.06, 0.30, 0.35),  # on décale x0 de ~0.32 pour se caler à droite
+    width="85%",
+    height="85%",
+    bbox_to_anchor=(
+        0.5,
+        0.06,
+        0.30,
+        0.35,
+    ),  # on décale x0 de ~0.32 pour se caler à droite
     bbox_transform=ax.transAxes,
-    borderpad=0
+    borderpad=0,
 )
 mask_zoom = (ells > 200) & (ells < 300)
-axins2.plot(ells[mask_zoom], cl0[mask_zoom], '--', linewidth=1, alpha=0.7)
-axins2.plot(ells[mask_zoom], cl1[mask_zoom],  '-', linewidth=1, alpha=0.7, color='tab:orange')
-axins2.set_xscale('log')
-axins2.set_yscale('log')
-axins2.set_title(r'Zoom $200<\ell<300$', fontsize=8)
-axins2.grid(True, which='both', linestyle=':', linewidth=0.5)
+axins2.plot(ells[mask_zoom], cl0[mask_zoom], "--", linewidth=1, alpha=0.7)
+axins2.plot(
+    ells[mask_zoom], cl1[mask_zoom], "-", linewidth=1, alpha=0.7, color="tab:orange"
+)
+axins2.set_xscale("log")
+axins2.set_yscale("log")
+axins2.set_title(r"Zoom $200<\ell<300$", fontsize=8)
+axins2.grid(True, which="both", linestyle=":", linewidth=0.5)
 axins2.tick_params(labelsize=7)

 # Annotate parameters
 if ALPHA is not None and Q0STAR is not None:
     ax.text(
-        0.03, 0.95,
-        rf'$\alpha={ALPHA},\ q_0^*={Q0STAR}$',
-        transform=ax.transAxes, ha='left', va='top', fontsize=9
+        0.03,
+        0.95,
+        rf"$\alpha={ALPHA},\ q_0^*={Q0STAR}$",
+        transform=ax.transAxes,
+        ha="left",
+        va="top",
+        fontsize=9,
     )

 plt.savefig(OUT_PNG)
diff --git a/zz-scripts/chapter06/plot_fig03_delta_cls_relative.py b/zz-scripts/chapter06/plot_fig03_delta_cls_relative.py
index 70ec038..81185fe 100755
--- a/zz-scripts/chapter06/plot_fig03_delta_cls_relative.py
+++ b/zz-scripts/chapter06/plot_fig03_delta_cls_relative.py
@@ -5,7 +5,8 @@ Script de tracé fig_03_delta_cls_rel pour Chapitre 6 (Rayonnement CMB)
 Tracé de la différence relative ΔCℓ/Cℓ en fonction du multipôle ℓ,
 avec annotation des paramètres MCGT (α, q0star).
 """
-#--- IMPORTS & CONFIGURATION ---
+
+# --- IMPORTS & CONFIGURATION ---
 import logging
 from pathlib import Path
 import json
@@ -14,49 +15,62 @@ import pandas as pd
 import matplotlib.pyplot as plt

 # Logging
-logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
+logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")

 # Paths
-ROOT                = Path(__file__).resolve().parents[2]
-DATA_DIR            = ROOT / 'zz-data' / 'chapter06'
-FIG_DIR             = ROOT / 'zz-figures' / 'chapter06'
-DELTA_CLS_REL_CSV   = DATA_DIR / '06_delta_cls_relative.csv'
-JSON_PARAMS         = DATA_DIR / '06_params_cmb.json'
-OUT_PNG             = FIG_DIR  / 'fig_03_delta_cls_rel.png'
+ROOT = Path(__file__).resolve().parents[2]
+DATA_DIR = ROOT / "zz-data" / "chapter06"
+FIG_DIR = ROOT / "zz-figures" / "chapter06"
+DELTA_CLS_REL_CSV = DATA_DIR / "06_delta_cls_relative.csv"
+JSON_PARAMS = DATA_DIR / "06_params_cmb.json"
+OUT_PNG = FIG_DIR / "fig_03_delta_cls_rel.png"
 FIG_DIR.mkdir(parents=True, exist_ok=True)

 # Load injection parameters
-with open(JSON_PARAMS, 'r', encoding='utf-8') as f:
+with open(JSON_PARAMS, "r", encoding="utf-8") as f:
     params = json.load(f)
-ALPHA   = params.get('alpha', None)
-Q0STAR  = params.get('q0star', None)
+ALPHA = params.get("alpha", None)
+Q0STAR = params.get("q0star", None)
 logging.info(f"Tracé fig_03 avec α={ALPHA}, q0*={Q0STAR}")

 # Load data
 df = pd.read_csv(DELTA_CLS_REL_CSV)
-ells       = df['ell'].values
-delta_rel  = df['delta_Cl_rel'].values
+ells = df["ell"].values
+delta_rel = df["delta_Cl_rel"].values

 # Plot
 fig, ax = plt.subplots(figsize=(10, 6), dpi=300)
-ax.plot(ells, delta_rel, linestyle='-', linewidth=2, color='tab:green', label=r'$\Delta C_\ell/C_\ell$')
-ax.axhline(0, color='black', linestyle='--', linewidth=1)
+ax.plot(
+    ells,
+    delta_rel,
+    linestyle="-",
+    linewidth=2,
+    color="tab:green",
+    label=r"$\Delta C_\ell/C_\ell$",
+)
+ax.axhline(0, color="black", linestyle="--", linewidth=1)

-ax.set_xscale('log')
+ax.set_xscale("log")
 ax.set_xlim(2, 3000)
 ymax = np.max(np.abs(delta_rel)) * 1.1
 ax.set_ylim(-ymax, ymax)

-ax.set_xlabel(r'Multipôle $\ell$')
-ax.set_ylabel(r'$\Delta C_{\ell}/C_{\ell}$')
-ax.grid(True, which='both', linestyle=':', linewidth=0.5)
-ax.legend(frameon=False, loc='upper right')
+ax.set_xlabel(r"Multipôle $\ell$")
+ax.set_ylabel(r"$\Delta C_{\ell}/C_{\ell}$")
+ax.grid(True, which="both", linestyle=":", linewidth=0.5)
+ax.legend(frameon=False, loc="upper right")

 # Annotate parameters
 if ALPHA is not None and Q0STAR is not None:
-    ax.text(0.03, 0.95,
-            fr'$\alpha={ALPHA},\ q_0^*={Q0STAR}$',
-            transform=ax.transAxes, ha='left', va='top', fontsize=9)
+    ax.text(
+        0.03,
+        0.95,
+        rf"$\alpha={ALPHA},\ q_0^*={Q0STAR}$",
+        transform=ax.transAxes,
+        ha="left",
+        va="top",
+        fontsize=9,
+    )

 plt.tight_layout()
 plt.savefig(OUT_PNG)
diff --git a/zz-scripts/chapter06/plot_fig04_delta_rs_vs_params.py b/zz-scripts/chapter06/plot_fig04_delta_rs_vs_params.py
index b7c2341..bb28ce9 100755
--- a/zz-scripts/chapter06/plot_fig04_delta_rs_vs_params.py
+++ b/zz-scripts/chapter06/plot_fig04_delta_rs_vs_params.py
@@ -4,7 +4,8 @@ Script de tracé fig_04_delta_rs_vs_params pour Chapitre 6 (Rayonnement CMB)
 ───────────────────────────────────────────────────────────────
 Tracé de la variation relative Δr_s/r_s en fonction du paramètre q0star.
 """
-#--- IMPORTS & CONFIGURATION ---
+
+# --- IMPORTS & CONFIGURATION ---
 import logging
 from pathlib import Path
 import json
@@ -12,48 +13,54 @@ import pandas as pd
 import matplotlib.pyplot as plt

 # Logging
-logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
+logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")

 # Paths
-ROOT        = Path(__file__).resolve().parents[2]
-DATA_DIR    = ROOT / 'zz-data' / 'chapter06'
-FIG_DIR     = ROOT / 'zz-figures' / 'chapter06'
-DATA_CSV    = DATA_DIR / '06_delta_rs_scan.csv'
-JSON_PARAMS = DATA_DIR / '06_params_cmb.json'
-OUT_PNG     = FIG_DIR  / 'fig_04_delta_rs_vs_params.png'
+ROOT = Path(__file__).resolve().parents[2]
+DATA_DIR = ROOT / "zz-data" / "chapter06"
+FIG_DIR = ROOT / "zz-figures" / "chapter06"
+DATA_CSV = DATA_DIR / "06_delta_rs_scan.csv"
+JSON_PARAMS = DATA_DIR / "06_params_cmb.json"
+OUT_PNG = FIG_DIR / "fig_04_delta_rs_vs_params.png"
 FIG_DIR.mkdir(parents=True, exist_ok=True)

 # Load scan data
 df = pd.read_csv(DATA_CSV)
-x  = df['q0star'].values
-y  = df['delta_rs_rel'].values
+x = df["q0star"].values
+y = df["delta_rs_rel"].values

 # Load injection parameters for annotation
-with open(JSON_PARAMS, 'r', encoding='utf-8') as f:
+with open(JSON_PARAMS, "r", encoding="utf-8") as f:
     params = json.load(f)
-ALPHA   = params.get('alpha', None)
-Q0STAR  = params.get('q0star', None)
+ALPHA = params.get("alpha", None)
+Q0STAR = params.get("q0star", None)
 logging.info(f"Tracé fig_04 avec α={ALPHA}, q0*={Q0STAR}")

 # Plot
 fig, ax = plt.subplots(figsize=(10, 6), dpi=300)
-ax.scatter(x, y, marker='o', s=20, alpha=0.8, label=r'$\Delta r_s / r_s$')
+ax.scatter(x, y, marker="o", s=20, alpha=0.8, label=r"$\Delta r_s / r_s$")

 # Tolérances ±1%
-ax.axhline( 0.01, color='k', linestyle=':', linewidth=1)
-ax.axhline(-0.01, color='k', linestyle=':', linewidth=1)
+ax.axhline(0.01, color="k", linestyle=":", linewidth=1)
+ax.axhline(-0.01, color="k", linestyle=":", linewidth=1)

 # Axes et légende
-ax.set_xlabel(r'$q_0^\star$', fontsize=11)
-ax.set_ylabel(r'$\Delta r_s / r_s$', fontsize=11)
-ax.grid(which='both', linestyle=':', linewidth=0.5)
+ax.set_xlabel(r"$q_0^\star$", fontsize=11)
+ax.set_ylabel(r"$\Delta r_s / r_s$", fontsize=11)
+ax.grid(which="both", linestyle=":", linewidth=0.5)
 ax.legend(frameon=False, fontsize=9)

 # Annotation des paramètres
 if ALPHA is not None and Q0STAR is not None:
-    ax.text(0.05, 0.95,
-            fr'$\alpha={ALPHA},\ q_0^*={Q0STAR}$',
-            transform=ax.transAxes, ha='left', va='top', fontsize=9)
+    ax.text(
+        0.05,
+        0.95,
+        rf"$\alpha={ALPHA},\ q_0^*={Q0STAR}$",
+        transform=ax.transAxes,
+        ha="left",
+        va="top",
+        fontsize=9,
+    )

 plt.tight_layout()
 plt.savefig(OUT_PNG)
diff --git a/zz-scripts/chapter06/plot_fig05_delta_chi2_heatmap.py b/zz-scripts/chapter06/plot_fig05_delta_chi2_heatmap.py
index 30dc842..0addb0c 100755
--- a/zz-scripts/chapter06/plot_fig05_delta_chi2_heatmap.py
+++ b/zz-scripts/chapter06/plot_fig05_delta_chi2_heatmap.py
@@ -4,7 +4,8 @@ Script de tracé fig_05_heatmap_delta_chi2 pour Chapitre 6 (Rayonnement CMB)
 ───────────────────────────────────────────────────────────────
 Affiche la carte de chaleur 2D de Δχ² en fonction de α et q0star.
 """
-#--- IMPORTS & CONFIGURATION ---
+
+# --- IMPORTS & CONFIGURATION ---
 import logging
 from pathlib import Path
 import json
@@ -13,66 +14,65 @@ import pandas as pd
 import matplotlib.pyplot as plt

 # Logging
-logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
+logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")

 # Paths
-ROOT         = Path(__file__).resolve().parents[2]
-DATA_DIR     = ROOT / 'zz-data' / 'chapter06'
-FIG_DIR      = ROOT / 'zz-figures' / 'chapter06'
-DATA_CSV     = DATA_DIR / '06_cmb_chi2_scan2D.csv'
-JSON_PARAMS  = DATA_DIR / '06_params_cmb.json'
-OUT_PNG      = FIG_DIR  / 'fig_05_heatmap_delta_chi2.png'
+ROOT = Path(__file__).resolve().parents[2]
+DATA_DIR = ROOT / "zz-data" / "chapter06"
+FIG_DIR = ROOT / "zz-figures" / "chapter06"
+DATA_CSV = DATA_DIR / "06_cmb_chi2_scan2D.csv"
+JSON_PARAMS = DATA_DIR / "06_params_cmb.json"
+OUT_PNG = FIG_DIR / "fig_05_heatmap_delta_chi2.png"
 FIG_DIR.mkdir(parents=True, exist_ok=True)

 # Load injection parameters for annotation
-with open(JSON_PARAMS, 'r', encoding='utf-8') as f:
+with open(JSON_PARAMS, "r", encoding="utf-8") as f:
     params = json.load(f)
-ALPHA   = params.get('alpha', None)
-Q0STAR  = params.get('q0star', None)
+ALPHA = params.get("alpha", None)
+Q0STAR = params.get("q0star", None)
 logging.info(f"Tracé fig_05 avec α={ALPHA}, q0*={Q0STAR}")

 # Load scan 2D data
 df = pd.read_csv(DATA_CSV)
-alphas = np.sort(df['alpha'].unique())
-q0s    = np.sort(df['q0star'].unique())
+alphas = np.sort(df["alpha"].unique())
+q0s = np.sort(df["q0star"].unique())

 # Pivot into matrix
 chi2_mat = (
-    df.pivot(index='q0star', columns='alpha', values='chi2')
-      .loc[q0s, alphas]
-      .values
+    df.pivot(index="q0star", columns="alpha", values="chi2").loc[q0s, alphas].values
 )

 # Compute cell edges for pcolormesh
 da = alphas[1] - alphas[0]
-dq = q0s[1]   - q0s[0]
-alpha_edges = np.concatenate([alphas - da/2, [alphas[-1] + da/2]])
-q0_edges    = np.concatenate([q0s    - dq/2, [q0s[-1]     + dq/2]])
+dq = q0s[1] - q0s[0]
+alpha_edges = np.concatenate([alphas - da / 2, [alphas[-1] + da / 2]])
+q0_edges = np.concatenate([q0s - dq / 2, [q0s[-1] + dq / 2]])

 # Create figure
 fig, ax = plt.subplots(figsize=(10, 6), dpi=300)
-pcm = ax.pcolormesh(
-    alpha_edges, q0_edges, chi2_mat,
-    shading='auto', cmap='viridis'
-)
-cbar = fig.colorbar(pcm, ax=ax, label=r'$\Delta\chi^2$')
+pcm = ax.pcolormesh(alpha_edges, q0_edges, chi2_mat, shading="auto", cmap="viridis")
+cbar = fig.colorbar(pcm, ax=ax, label=r"$\Delta\chi^2$")

 # Aesthetics
-ax.set_title(r"Carte de chaleur $\Delta\chi^2$ (Chapitre 6)", fontsize=14, fontweight='bold')
+ax.set_title(
+    r"Carte de chaleur $\Delta\chi^2$ (Chapitre 6)", fontsize=14, fontweight="bold"
+)
 ax.set_xlabel(r"$\alpha$")
 ax.set_ylabel(r"$q_0^*$")
-ax.grid(which='major', linestyle=':', linewidth=0.5)
-ax.grid(which='minor', linestyle=':', linewidth=0.3)
+ax.grid(which="major", linestyle=":", linewidth=0.5)
+ax.grid(which="minor", linestyle=":", linewidth=0.3)
 ax.minorticks_on()

 # Annotate parameters
 if ALPHA is not None and Q0STAR is not None:
     ax.text(
-        0.03, 0.95,
-        fr'$\alpha={ALPHA},\ q_0^*={Q0STAR}$',
+        0.03,
+        0.95,
+        rf"$\alpha={ALPHA},\ q_0^*={Q0STAR}$",
         transform=ax.transAxes,
-        ha='left', va='top',
-        fontsize=9
+        ha="left",
+        va="top",
+        fontsize=9,
     )

 plt.tight_layout()
diff --git a/zz-scripts/chapter07/generate_data_chapter07.py b/zz-scripts/chapter07/generate_data_chapter07.py
index 9e7bda5..8afee53 100755
--- a/zz-scripts/chapter07/generate_data_chapter07.py
+++ b/zz-scripts/chapter07/generate_data_chapter07.py
@@ -69,94 +69,126 @@ class PhaseParams:

 def load_config(ini_path: Path) -> PhaseParams:
     cfg = configparser.ConfigParser(
-        interpolation=None,
-        inline_comment_prefixes=('#', ';')
+        interpolation=None, inline_comment_prefixes=("#", ";")
     )
-    cfg.read(ini_path, encoding='utf-8')
+    cfg.read(ini_path, encoding="utf-8")

     # 1) section cosmologie
-    cos = cfg['cosmologie']
-    H0    = cos.getfloat('H0')
-    ombh2 = cos.getfloat('ombh2')
-    omch2 = cos.getfloat('omch2')
-    omk   = cos.getfloat('omk')
-    tau   = cos.getfloat('tau')
-    mnu   = cos.getfloat('mnu')
-    As0   = cos.getfloat('As0')
-    ns0   = cos.getfloat('ns0')
+    cos = cfg["cosmologie"]
+    H0 = cos.getfloat("H0")
+    ombh2 = cos.getfloat("ombh2")
+    omch2 = cos.getfloat("omch2")
+    omk = cos.getfloat("omk")
+    tau = cos.getfloat("tau")
+    mnu = cos.getfloat("mnu")
+    As0 = cos.getfloat("As0")
+    ns0 = cos.getfloat("ns0")

     # 2) section scan ou fallback grille
-    if 'scan' in cfg and 'k_min' in cfg['scan']:
-        s = cfg['scan']
-        k_min = float(s['k_min']); k_max = float(s['k_max'])
-        dlog  = float(s.get('dlog', s.get('dlog_k'))); n_k = int(s['n_k'])
-        a_min = float(s['a_min']); a_max = float(s['a_max']); n_a = int(s['n_a'])
+    if "scan" in cfg and "k_min" in cfg["scan"]:
+        s = cfg["scan"]
+        k_min = float(s["k_min"])
+        k_max = float(s["k_max"])
+        dlog = float(s.get("dlog", s.get("dlog_k")))
+        n_k = int(s["n_k"])
+        a_min = float(s["a_min"])
+        a_max = float(s["a_max"])
+        n_a = int(s["n_a"])
     else:
-        g1 = cfg['grille1D']; g2 = cfg['grille2D']
-        k_min = g1.getfloat('k_min'); k_max = g1.getfloat('k_max')
-        dlog  = g1.getfloat('dlog_k');       n_k = g1.getint('n_k')
-        a_min = g2.getfloat('a_min');        a_max = g2.getfloat('a_max')
-        n_a   = g2.getint('n_a')
-        s = cfg['scan']
+        g1 = cfg["grille1D"]
+        g2 = cfg["grille2D"]
+        k_min = g1.getfloat("k_min")
+        k_max = g1.getfloat("k_max")
+        dlog = g1.getfloat("dlog_k")
+        n_k = g1.getint("n_k")
+        a_min = g2.getfloat("a_min")
+        a_max = g2.getfloat("a_max")
+        n_a = g2.getint("n_a")
+        s = cfg["scan"]

     # 3) x_split depuis scan ou segmentation
-    raw_xsplit = s.get('x_split')
-    x_split = float(raw_xsplit) if raw_xsplit is not None else cfg['segmentation'].getfloat('x_split')
+    raw_xsplit = s.get("x_split")
+    x_split = (
+        float(raw_xsplit)
+        if raw_xsplit is not None
+        else cfg["segmentation"].getfloat("x_split")
+    )
     k_split = x_split  # aliased for compute_delta_phi

     # 4) lissage (section [lissage] ou fallback)
-    if 'lissage' in cfg:
-        l = cfg['lissage']
-        window  = int(l.get('derivative_window', l.get('window')))
-        polyord = int(l.get('derivative_polyorder', l.get('polyorder')))
+    if "lissage" in cfg:
+        l = cfg["lissage"]
+        window = int(l.get("derivative_window", l.get("window")))
+        polyord = int(l.get("derivative_polyorder", l.get("polyorder")))
     else:
-        window  = int(s['derivative_window'])
-        polyord = int(s['derivative_polyorder'])
+        window = int(s["derivative_window"])
+        polyord = int(s["derivative_polyorder"])

     # 5) tolérances (section [tolerances] ou fallback)
-    if 'tolerances' in cfg:
-        t    = cfg['tolerances']
-        tol1 = float(t.get('primary', t.get('tolerance_primary')))
-        tol2 = float(t.get('order2', t.get('tolerance_order2')))
+    if "tolerances" in cfg:
+        t = cfg["tolerances"]
+        tol1 = float(t.get("primary", t.get("tolerance_primary")))
+        tol2 = float(t.get("order2", t.get("tolerance_order2")))
     else:
-        tol1 = float(s.get('tolerance_primary', s.get('primary')))
-        tol2 = float(s.get('tolerance_order2', s.get('order2')))
+        tol1 = float(s.get("tolerance_primary", s.get("primary")))
+        tol2 = float(s.get("tolerance_order2", s.get("order2")))

     # 6) knobs scan
-    k0              = float(s['k0'])
-    decay           = float(s['decay'])
-    cs2_param       = float(s['cs2_param'])
-    delta_phi_param = float(s['delta_phi_param'])
-    phi0_init       = float(s['phi0_init'])
-    phi_inf         = float(s['phi_inf'])
-    a_char          = float(s['a_char'])
-    m_phi           = float(s['m_phi'])
-    m_eff_const     = float(s['m_eff_const'])
+    k0 = float(s["k0"])
+    decay = float(s["decay"])
+    cs2_param = float(s["cs2_param"])
+    delta_phi_param = float(s["delta_phi_param"])
+    phi0_init = float(s["phi0_init"])
+    phi_inf = float(s["phi_inf"])
+    a_char = float(s["a_char"])
+    m_phi = float(s["m_phi"])
+    m_eff_const = float(s["m_eff_const"])

     # 7) dynamique phi (optionnel)
-    if 'dynamique_phi' in cfg:
-        dyn = cfg['dynamique_phi']
-        a_eq         = float(dyn.get('a_eq', phi0_init))
-        freeze_scale = float(dyn.get('freeze_scale', 1.0))
-        Phi0         = float(dyn.get('Phi0', phi0_init))
+    if "dynamique_phi" in cfg:
+        dyn = cfg["dynamique_phi"]
+        a_eq = float(dyn.get("a_eq", phi0_init))
+        freeze_scale = float(dyn.get("freeze_scale", 1.0))
+        Phi0 = float(dyn.get("Phi0", phi0_init))
     else:
         a_eq = phi0_init
         freeze_scale = 1.0
         Phi0 = phi0_init

     return PhaseParams(
-        H0=H0, ombh2=ombh2, omch2=omch2, omk=omk, tau=tau, mnu=mnu,
-        As0=As0, ns0=ns0,
-        k_min=k_min, k_max=k_max, dlog=dlog, n_k=n_k,
-        a_min=a_min, a_max=a_max, n_a=n_a,
-        x_split=x_split, k_split=k_split,
-        derivative_window=window, derivative_polyorder=polyord,
-        k0=k0, decay=decay,
-        cs2_param=cs2_param, delta_phi_param=delta_phi_param,
-        tolerance_primary=tol1, tolerance_order2=tol2,
-        phi0_init=phi0_init, phi_inf=phi_inf,
-        a_char=a_char, m_phi=m_phi, m_eff_const=m_eff_const,
-        a_eq=a_eq, freeze_scale=freeze_scale, Phi0=Phi0
+        H0=H0,
+        ombh2=ombh2,
+        omch2=omch2,
+        omk=omk,
+        tau=tau,
+        mnu=mnu,
+        As0=As0,
+        ns0=ns0,
+        k_min=k_min,
+        k_max=k_max,
+        dlog=dlog,
+        n_k=n_k,
+        a_min=a_min,
+        a_max=a_max,
+        n_a=n_a,
+        x_split=x_split,
+        k_split=k_split,
+        derivative_window=window,
+        derivative_polyorder=polyord,
+        k0=k0,
+        decay=decay,
+        cs2_param=cs2_param,
+        delta_phi_param=delta_phi_param,
+        tolerance_primary=tol1,
+        tolerance_order2=tol2,
+        phi0_init=phi0_init,
+        phi_inf=phi_inf,
+        a_char=a_char,
+        m_phi=m_phi,
+        m_eff_const=m_eff_const,
+        a_eq=a_eq,
+        freeze_scale=freeze_scale,
+        Phi0=Phi0,
     )


@@ -164,15 +196,19 @@ def parse_args():
     p = argparse.ArgumentParser(
         description="Génère le scan brut c_s²(k,a) et δφ/φ(k,a) pour le Chapitre 7."
     )
-    p.add_argument('-i','--ini',      required=True, help="INI de config")
-    p.add_argument('--export-raw',    required=True, help="CSV brut unifié (sortie)")
-    p.add_argument('--export-2d',     action='store_true', help="Exporter matrices 2D")
-    p.add_argument('--n-k',           type=int, metavar="NK", help="Override # points k")
-    p.add_argument('--n-a',           type=int, metavar="NA", help="Override # points a")
-    p.add_argument('--dry-run',       action='store_true', help="Valide config et grille")
-    p.add_argument('--log-level',     default='INFO',
-                   choices=['DEBUG','INFO','WARNING','ERROR','CRITICAL'], help="Niveau log")
-    p.add_argument('--log-file',      metavar="FILE", help="Fichier log")
+    p.add_argument("-i", "--ini", required=True, help="INI de config")
+    p.add_argument("--export-raw", required=True, help="CSV brut unifié (sortie)")
+    p.add_argument("--export-2d", action="store_true", help="Exporter matrices 2D")
+    p.add_argument("--n-k", type=int, metavar="NK", help="Override # points k")
+    p.add_argument("--n-a", type=int, metavar="NA", help="Override # points a")
+    p.add_argument("--dry-run", action="store_true", help="Valide config et grille")
+    p.add_argument(
+        "--log-level",
+        default="INFO",
+        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
+        help="Niveau log",
+    )
+    p.add_argument("--log-file", metavar="FILE", help="Fichier log")
     return p.parse_args()


@@ -181,12 +217,17 @@ def main():

     # Logging
     logger = logging.getLogger()
-    fmt = logging.Formatter('[%(levelname)s] %(message)s')
-    ch = logging.StreamHandler(); ch.setFormatter(fmt); logger.addHandler(ch)
+    fmt = logging.Formatter("[%(levelname)s] %(message)s")
+    ch = logging.StreamHandler()
+    ch.setFormatter(fmt)
+    logger.addHandler(ch)
     logger.setLevel(args.log_level.upper())
     if args.log_file:
-        lf = Path(args.log_file); lf.parent.mkdir(parents=True, exist_ok=True)
-        fh = logging.FileHandler(lf); fh.setFormatter(fmt); logger.addHandler(fh)
+        lf = Path(args.log_file)
+        lf.parent.mkdir(parents=True, exist_ok=True)
+        fh = logging.FileHandler(lf)
+        fh.setFormatter(fmt)
+        logger.addHandler(fh)

     # Charger config
     ini_path = Path(args.ini)
@@ -217,31 +258,38 @@ def main():
     phi_mat = compute_delta_phi(k_grid, a_vals, p)

     # Export brut unifié
-    brut_path = Path(args.export_raw); brut_path.parent.mkdir(parents=True, exist_ok=True)
-    df_brut = pd.DataFrame({
-        'k':               k_grid.repeat(len(a_vals)),
-        'a':               np.tile(a_vals, len(k_grid)),
-        'cs2_brut':        cs2_mat.ravel(),
-        'delta_phi_brut':  phi_mat.ravel()
-    })
+    brut_path = Path(args.export_raw)
+    brut_path.parent.mkdir(parents=True, exist_ok=True)
+    df_brut = pd.DataFrame(
+        {
+            "k": k_grid.repeat(len(a_vals)),
+            "a": np.tile(a_vals, len(k_grid)),
+            "cs2_brut": cs2_mat.ravel(),
+            "delta_phi_brut": phi_mat.ravel(),
+        }
+    )
     df_brut.to_csv(brut_path, index=False)
     logger.info("Brut unifié écrit → %s", brut_path)

     # Export matrices 2D
     if args.export_2d:
         mat_dir = brut_path.parent
-        pd.DataFrame({
-            'k':           k_grid.repeat(len(a_vals)),
-            'a':           np.tile(a_vals, len(k_grid)),
-            'cs2_matrice': cs2_mat.ravel()
-        }).to_csv(mat_dir / '07_cs2_matrix.csv', index=False)
-        pd.DataFrame({
-            'k':                   k_grid.repeat(len(a_vals)),
-            'a':                   np.tile(a_vals, len(k_grid)),
-            'delta_phi_matrice':   phi_mat.ravel()
-        }).to_csv(mat_dir / '07_delta_phi_matrix.csv', index=False)
+        pd.DataFrame(
+            {
+                "k": k_grid.repeat(len(a_vals)),
+                "a": np.tile(a_vals, len(k_grid)),
+                "cs2_matrice": cs2_mat.ravel(),
+            }
+        ).to_csv(mat_dir / "07_cs2_matrix.csv", index=False)
+        pd.DataFrame(
+            {
+                "k": k_grid.repeat(len(a_vals)),
+                "a": np.tile(a_vals, len(k_grid)),
+                "delta_phi_matrice": phi_mat.ravel(),
+            }
+        ).to_csv(mat_dir / "07_delta_phi_matrix.csv", index=False)
         logger.info("Matrices 2D exportées → %s", mat_dir)


-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter07/launch_scalar_perturbations_solver.py b/zz-scripts/chapter07/launch_scalar_perturbations_solver.py
index 7914f85..93c2be0 100755
--- a/zz-scripts/chapter07/launch_scalar_perturbations_solver.py
+++ b/zz-scripts/chapter07/launch_scalar_perturbations_solver.py
@@ -13,6 +13,7 @@ Ce script :
  - (optionnel) écrit des matrices 2D CS2 / delta_phi,
  - journalise et contrôle les erreurs.
 """
+
 from __future__ import annotations

 import sys
@@ -36,8 +37,11 @@ sys.path.insert(0, str(ROOT))
 try:
     from mcgt.perturbations_scalaires import compute_cs2, compute_delta_phi
 except Exception as e:
-    raise ImportError("Impossible d'importer compute_cs2 / compute_delta_phi depuis mcgt. "
-                      "Vérifiez l'installation du package mcgt.") from e
+    raise ImportError(
+        "Impossible d'importer compute_cs2 / compute_delta_phi depuis mcgt. "
+        "Vérifiez l'installation du package mcgt."
+    ) from e
+

 # ---------------------------------------------------------------------------
 # Dataclass configuration (structure attendue dans l'INI)
@@ -93,6 +97,7 @@ class PhaseParams:
     freeze_scale: float
     Phi0: float

+
 # ---------------------------------------------------------------------------
 # Utilitaires
 # ---------------------------------------------------------------------------
@@ -106,7 +111,10 @@ def safe_git_hash(root: Path) -> str | None:
     except Exception:
         return None

-def build_log_grid(xmin: float, xmax: float, n_points: int | None = None, dlog: float | None = None) -> np.ndarray:
+
+def build_log_grid(
+    xmin: float, xmax: float, n_points: int | None = None, dlog: float | None = None
+) -> np.ndarray:
     """Construit une grille log-uniforme."""
     if xmin <= 0 or xmax <= xmin:
         raise ValueError("xmin doit être > 0 et xmax > xmin.")
@@ -117,127 +125,183 @@ def build_log_grid(xmin: float, xmax: float, n_points: int | None = None, dlog:
         return 10 ** (np.log10(xmin) + np.arange(n) * dlog)
     raise ValueError("Fournir n_points ou dlog.")

+
 def load_config(ini_path: Path) -> PhaseParams:
     """Lit un INI flexible et renvoie une PhaseParams."""
-    cfg = configparser.ConfigParser(interpolation=None, inline_comment_prefixes=('#', ';'))
-    read = cfg.read(ini_path, encoding='utf-8')
+    cfg = configparser.ConfigParser(
+        interpolation=None, inline_comment_prefixes=("#", ";")
+    )
+    read = cfg.read(ini_path, encoding="utf-8")
     if not read:
         raise FileNotFoundError(f"INI introuvable ou illisible : {ini_path}")

     # cosmologie (section obligatoire)
-    if 'cosmologie' not in cfg:
+    if "cosmologie" not in cfg:
         raise KeyError("Section [cosmologie] manquante dans l'INI.")
-    cos = cfg['cosmologie']
-    H0    = cos.getfloat('H0')
-    ombh2 = cos.getfloat('ombh2')
-    omch2 = cos.getfloat('omch2')
-    omk   = cos.getfloat('omk')
-    tau   = cos.getfloat('tau')
-    mnu   = cos.getfloat('mnu')
-    As0   = cos.getfloat('As0')
-    ns0   = cos.getfloat('ns0')
+    cos = cfg["cosmologie"]
+    H0 = cos.getfloat("H0")
+    ombh2 = cos.getfloat("ombh2")
+    omch2 = cos.getfloat("omch2")
+    omk = cos.getfloat("omk")
+    tau = cos.getfloat("tau")
+    mnu = cos.getfloat("mnu")
+    As0 = cos.getfloat("As0")
+    ns0 = cos.getfloat("ns0")

     # grille : privilégier [scan] si présent
-    if 'scan' in cfg and 'k_min' in cfg['scan']:
-        s = cfg['scan']
-        k_min = float(s['k_min']); k_max = float(s['k_max'])
-        dlog  = float(s.get('dlog', s.get('dlog_k', 0.01)))
-        n_k   = int(s.get('n_k', max(2, int((np.log10(k_max)-np.log10(k_min))/dlog)+1)))
-        a_min = float(s.get('a_min', 0.0)); a_max = float(s.get('a_max', 1.0)); n_a = int(s.get('n_a', 1))
+    if "scan" in cfg and "k_min" in cfg["scan"]:
+        s = cfg["scan"]
+        k_min = float(s["k_min"])
+        k_max = float(s["k_max"])
+        dlog = float(s.get("dlog", s.get("dlog_k", 0.01)))
+        n_k = int(
+            s.get("n_k", max(2, int((np.log10(k_max) - np.log10(k_min)) / dlog) + 1))
+        )
+        a_min = float(s.get("a_min", 0.0))
+        a_max = float(s.get("a_max", 1.0))
+        n_a = int(s.get("n_a", 1))
     else:
-        g1 = cfg['grille1D']; g2 = cfg['grille2D']
-        k_min = g1.getfloat('k_min'); k_max = g1.getfloat('k_max')
-        dlog  = g1.getfloat('dlog_k'); n_k = g1.getint('n_k')
-        a_min = g2.getfloat('a_min'); a_max = g2.getfloat('a_max'); n_a = g2.getint('n_a')
-        s = cfg['scan'] if 'scan' in cfg else {}
+        g1 = cfg["grille1D"]
+        g2 = cfg["grille2D"]
+        k_min = g1.getfloat("k_min")
+        k_max = g1.getfloat("k_max")
+        dlog = g1.getfloat("dlog_k")
+        n_k = g1.getint("n_k")
+        a_min = g2.getfloat("a_min")
+        a_max = g2.getfloat("a_max")
+        n_a = g2.getint("n_a")
+        s = cfg["scan"] if "scan" in cfg else {}

     # découpe / split
-    x_split = float(s.get('x_split', cfg.get('segmentation', {}).get('x_split', 1.0)))
+    x_split = float(s.get("x_split", cfg.get("segmentation", {}).get("x_split", 1.0)))
     k_split = x_split

     # lissage
-    if 'lissage' in cfg:
-        l = cfg['lissage']
-        window  = int(l.get('derivative_window', l.get('window', 7)))
-        polyord = int(l.get('derivative_polyorder', l.get('polyorder', 3)))
+    if "lissage" in cfg:
+        l = cfg["lissage"]
+        window = int(l.get("derivative_window", l.get("window", 7)))
+        polyord = int(l.get("derivative_polyorder", l.get("polyorder", 3)))
     else:
-        window  = int(s.get('derivative_window', 7))
-        polyord = int(s.get('derivative_polyorder', 3))
+        window = int(s.get("derivative_window", 7))
+        polyord = int(s.get("derivative_polyorder", 3))

     # tolérances
-    if 'tolerances' in cfg:
-        t = cfg['tolerances']
-        tol1 = float(t.get('primary', t.get('tolerance_primary', 0.01)))
-        tol2 = float(t.get('order2', t.get('tolerance_order2', 0.10)))
+    if "tolerances" in cfg:
+        t = cfg["tolerances"]
+        tol1 = float(t.get("primary", t.get("tolerance_primary", 0.01)))
+        tol2 = float(t.get("order2", t.get("tolerance_order2", 0.10)))
     else:
-        tol1 = float(s.get('tolerance_primary', 0.01))
-        tol2 = float(s.get('tolerance_order2', 0.10))
+        tol1 = float(s.get("tolerance_primary", 0.01))
+        tol2 = float(s.get("tolerance_order2", 0.10))

     # knobs
-    k0              = float(s.get('k0', 1.0))
-    decay           = float(s.get('decay', 1.0))
-    cs2_param       = float(s.get('cs2_param', 1.0))
-    delta_phi_param = float(s.get('delta_phi_param', 1.0))
-    phi0_init       = float(s.get('phi0_init', 1.0))
-    phi_inf         = float(s.get('phi_inf', 1.0))
-    a_char          = float(s.get('a_char', 1.0))
-    m_phi           = float(s.get('m_phi', 0.0))
-    m_eff_const     = float(s.get('m_eff_const', 0.0))
+    k0 = float(s.get("k0", 1.0))
+    decay = float(s.get("decay", 1.0))
+    cs2_param = float(s.get("cs2_param", 1.0))
+    delta_phi_param = float(s.get("delta_phi_param", 1.0))
+    phi0_init = float(s.get("phi0_init", 1.0))
+    phi_inf = float(s.get("phi_inf", 1.0))
+    a_char = float(s.get("a_char", 1.0))
+    m_phi = float(s.get("m_phi", 0.0))
+    m_eff_const = float(s.get("m_eff_const", 0.0))

     # dynamique phi optionnelle
-    if 'dynamique_phi' in cfg:
-        dyn = cfg['dynamique_phi']
-        a_eq         = float(dyn.get('a_eq', phi0_init))
-        freeze_scale = float(dyn.get('freeze_scale', 1.0))
-        Phi0         = float(dyn.get('Phi0', phi0_init))
+    if "dynamique_phi" in cfg:
+        dyn = cfg["dynamique_phi"]
+        a_eq = float(dyn.get("a_eq", phi0_init))
+        freeze_scale = float(dyn.get("freeze_scale", 1.0))
+        Phi0 = float(dyn.get("Phi0", phi0_init))
     else:
         a_eq = phi0_init
         freeze_scale = 1.0
         Phi0 = phi0_init

     return PhaseParams(
-        H0=H0, ombh2=ombh2, omch2=omch2, omk=omk, tau=tau, mnu=mnu,
-        As0=As0, ns0=ns0,
-        k_min=k_min, k_max=k_max, dlog=dlog, n_k=n_k,
-        a_min=a_min, a_max=a_max, n_a=n_a,
-        x_split=x_split, k_split=k_split,
-        derivative_window=window, derivative_polyorder=polyord,
-        k0=k0, decay=decay,
-        cs2_param=cs2_param, delta_phi_param=delta_phi_param,
-        tolerance_primary=tol1, tolerance_order2=tol2,
-        phi0_init=phi0_init, phi_inf=phi_inf,
-        a_char=a_char, m_phi=m_phi, m_eff_const=m_eff_const,
-        a_eq=a_eq, freeze_scale=freeze_scale, Phi0=Phi0
+        H0=H0,
+        ombh2=ombh2,
+        omch2=omch2,
+        omk=omk,
+        tau=tau,
+        mnu=mnu,
+        As0=As0,
+        ns0=ns0,
+        k_min=k_min,
+        k_max=k_max,
+        dlog=dlog,
+        n_k=n_k,
+        a_min=a_min,
+        a_max=a_max,
+        n_a=n_a,
+        x_split=x_split,
+        k_split=k_split,
+        derivative_window=window,
+        derivative_polyorder=polyord,
+        k0=k0,
+        decay=decay,
+        cs2_param=cs2_param,
+        delta_phi_param=delta_phi_param,
+        tolerance_primary=tol1,
+        tolerance_order2=tol2,
+        phi0_init=phi0_init,
+        phi_inf=phi_inf,
+        a_char=a_char,
+        m_phi=m_phi,
+        m_eff_const=m_eff_const,
+        a_eq=a_eq,
+        freeze_scale=freeze_scale,
+        Phi0=Phi0,
     )

+
 # ---------------------------------------------------------------------------
 # Entrée / sortie et exécution
 # ---------------------------------------------------------------------------
 def parse_args():
-    p = argparse.ArgumentParser(description="Lance le solveur de perturbations scalaires (Chapter 7).")
-    p.add_argument('-i', '--ini', required=True, help="Chemin du fichier INI de configuration")
-    p.add_argument('--export-raw', required=True, help="Chemin du CSV raw unifié (k,a,cs2_raw,delta_phi_raw)")
-    p.add_argument('--export-2d', action='store_true', help="Exporter matrices 2D (csv)")
-    p.add_argument('--n-k', type=int, help="Override du nombre de points en k")
-    p.add_argument('--n-a', type=int, help="Override du nombre de points en a")
-    p.add_argument('--dry-run', action='store_true', help="Construire les grilles et quitter")
-    p.add_argument('--log-level', default='INFO', choices=['DEBUG','INFO','WARNING','ERROR','CRITICAL'])
-    p.add_argument('--log-file', help="Fichier de log")
+    p = argparse.ArgumentParser(
+        description="Lance le solveur de perturbations scalaires (Chapter 7)."
+    )
+    p.add_argument(
+        "-i", "--ini", required=True, help="Chemin du fichier INI de configuration"
+    )
+    p.add_argument(
+        "--export-raw",
+        required=True,
+        help="Chemin du CSV raw unifié (k,a,cs2_raw,delta_phi_raw)",
+    )
+    p.add_argument(
+        "--export-2d", action="store_true", help="Exporter matrices 2D (csv)"
+    )
+    p.add_argument("--n-k", type=int, help="Override du nombre de points en k")
+    p.add_argument("--n-a", type=int, help="Override du nombre de points en a")
+    p.add_argument(
+        "--dry-run", action="store_true", help="Construire les grilles et quitter"
+    )
+    p.add_argument(
+        "--log-level",
+        default="INFO",
+        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
+    )
+    p.add_argument("--log-file", help="Fichier de log")
     return p.parse_args()

+
 def main():
     args = parse_args()

     # logger
     logger = logging.getLogger()
     logger.handlers.clear()
-    fmt = logging.Formatter('[%(levelname)s] %(message)s')
-    ch = logging.StreamHandler(); ch.setFormatter(fmt); logger.addHandler(ch)
+    fmt = logging.Formatter("[%(levelname)s] %(message)s")
+    ch = logging.StreamHandler()
+    ch.setFormatter(fmt)
+    logger.addHandler(ch)
     logger.setLevel(args.log_level.upper())
     if args.log_file:
         lf = Path(args.log_file)
         lf.parent.mkdir(parents=True, exist_ok=True)
-        fh = logging.FileHandler(lf); fh.setFormatter(fmt); logger.addHandler(fh)
+        fh = logging.FileHandler(lf)
+        fh.setFormatter(fmt)
+        logger.addHandler(fh)

     # lire INI
     ini_path = Path(args.ini)
@@ -254,15 +318,24 @@ def main():
     if args.n_k:
         params.n_k = args.n_k
         # ajuster dlog pour rester cohérent
-        params.dlog = (np.log10(params.k_max) - np.log10(params.k_min)) / max(params.n_k - 1, 1)
+        params.dlog = (np.log10(params.k_max) - np.log10(params.k_min)) / max(
+            params.n_k - 1, 1
+        )
     if args.n_a:
         params.n_a = args.n_a

     # construire grilles
     k_grid = build_log_grid(params.k_min, params.k_max, n_points=params.n_k)
     a_vals = np.linspace(params.a_min, params.a_max, params.n_a)
-    logger.info("Grilles : %d k-points entre [%g, %g], %d a-points entre [%g, %g]",
-                len(k_grid), params.k_min, params.k_max, len(a_vals), params.a_min, params.a_max)
+    logger.info(
+        "Grilles : %d k-points entre [%g, %g], %d a-points entre [%g, %g]",
+        len(k_grid),
+        params.k_min,
+        params.k_max,
+        len(a_vals),
+        params.a_min,
+        params.a_max,
+    )

     if args.dry_run:
         logger.info("Dry-run demandé. Fin.")
@@ -271,8 +344,10 @@ def main():
     # Appel du solveur fourni par mcgt
     logger.info("Appel du solveur : compute_cs2 / compute_delta_phi …")
     try:
-        cs2_mat = compute_cs2(k_grid, a_vals, params)       # attente : shape (len(k), len(a))
-        phi_mat = compute_delta_phi(k_grid, a_vals, params) # même shape
+        cs2_mat = compute_cs2(
+            k_grid, a_vals, params
+        )  # attente : shape (len(k), len(a))
+        phi_mat = compute_delta_phi(k_grid, a_vals, params)  # même shape
     except Exception as e:
         logger.exception("Erreur lors de l'exécution du solveur : %s", e)
         sys.exit(1)
@@ -286,30 +361,36 @@ def main():
     # export raw unifié
     out_raw = Path(args.export_raw)
     out_raw.parent.mkdir(parents=True, exist_ok=True)
-    df_raw = pd.DataFrame({
-        'k': k_grid.repeat(len(a_vals)),
-        'a': np.tile(a_vals, len(k_grid)),
-        'cs2_raw': cs2_mat.ravel(),
-        'delta_phi_raw': phi_mat.ravel()
-    })
+    df_raw = pd.DataFrame(
+        {
+            "k": k_grid.repeat(len(a_vals)),
+            "a": np.tile(a_vals, len(k_grid)),
+            "cs2_raw": cs2_mat.ravel(),
+            "delta_phi_raw": phi_mat.ravel(),
+        }
+    )
     df_raw.to_csv(out_raw, index=False)
     logger.info("Raw unifié écrit → %s (%d lignes)", out_raw, len(df_raw))

     # export matrices 2D si demandé (format long k,a,val)
     if args.export_2d:
         mat_dir = out_raw.parent
-        df_cs2_mat = pd.DataFrame({
-            'k': k_grid.repeat(len(a_vals)),
-            'a': np.tile(a_vals, len(k_grid)),
-            'cs2_matrix': cs2_mat.ravel()
-        })
-        df_phi_mat = pd.DataFrame({
-            'k': k_grid.repeat(len(a_vals)),
-            'a': np.tile(a_vals, len(k_grid)),
-            'delta_phi_matrix': phi_mat.ravel()
-        })
-        cs2_path = mat_dir / '07_cs2_matrix.csv'
-        phi_path = mat_dir / '07_delta_phi_matrix.csv'
+        df_cs2_mat = pd.DataFrame(
+            {
+                "k": k_grid.repeat(len(a_vals)),
+                "a": np.tile(a_vals, len(k_grid)),
+                "cs2_matrix": cs2_mat.ravel(),
+            }
+        )
+        df_phi_mat = pd.DataFrame(
+            {
+                "k": k_grid.repeat(len(a_vals)),
+                "a": np.tile(a_vals, len(k_grid)),
+                "delta_phi_matrix": phi_mat.ravel(),
+            }
+        )
+        cs2_path = mat_dir / "07_cs2_matrix.csv"
+        phi_path = mat_dir / "07_delta_phi_matrix.csv"
         df_cs2_mat.to_csv(cs2_path, index=False)
         df_phi_mat.to_csv(phi_path, index=False)
         logger.info("Matrices 2D exportées → %s , %s", cs2_path, phi_path)
@@ -317,25 +398,28 @@ def main():
     # écriture d'un méta JSON avec hash git si disponible
     data_dir = out_raw.parent
     meta = {
-        'generated_at': datetime.now(timezone.utc).isoformat(timespec='seconds').replace('+00:00','Z'),
-        'n_k': int(len(k_grid)),
-        'n_a': int(len(a_vals)),
-        'k_min': float(params.k_min),
-        'k_max': float(params.k_max),
-        'dlog': float(params.dlog),
-        'a_min': float(params.a_min),
-        'a_max': float(params.a_max),
-        'files': [str(out_raw.name)]
+        "generated_at": datetime.now(timezone.utc)
+        .isoformat(timespec="seconds")
+        .replace("+00:00", "Z"),
+        "n_k": int(len(k_grid)),
+        "n_a": int(len(a_vals)),
+        "k_min": float(params.k_min),
+        "k_max": float(params.k_max),
+        "dlog": float(params.dlog),
+        "a_min": float(params.a_min),
+        "a_max": float(params.a_max),
+        "files": [str(out_raw.name)],
     }
     if args.export_2d:
-        meta['files'] += ['07_cs2_matrix.csv', '07_delta_phi_matrix.csv']
+        meta["files"] += ["07_cs2_matrix.csv", "07_delta_phi_matrix.csv"]
     git_h = safe_git_hash(ROOT)
-    meta['git_hash'] = git_h or 'unknown'
-    meta_path = data_dir / '07_meta_perturbations.json'
-    meta_path.write_text(json.dumps(meta, indent=2), encoding='utf-8')
+    meta["git_hash"] = git_h or "unknown"
+    meta_path = data_dir / "07_meta_perturbations.json"
+    meta_path.write_text(json.dumps(meta, indent=2), encoding="utf-8")
     logger.info("Méta écrit → %s", meta_path)

     logger.info("Terminé avec succès.")

-if __name__ == '__main__':
+
+if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter07/plot_fig01_cs2_heatmap.py b/zz-scripts/chapter07/plot_fig01_cs2_heatmap.py
index c532f4b..514c936 100755
--- a/zz-scripts/chapter07/plot_fig01_cs2_heatmap.py
+++ b/zz-scripts/chapter07/plot_fig01_cs2_heatmap.py
@@ -7,7 +7,6 @@ Figure 01 – Carte de chaleur de $c_s^2(k,a)$
 pour le Chapitre 7 (Perturbations scalaires) du projet MCGT.
 """

-import sys
 import logging
 import json
 from pathlib import Path
@@ -18,7 +17,7 @@ import matplotlib.pyplot as plt
 from matplotlib.colors import LogNorm

 # --- CONFIGURATION DU LOGGING ---
-logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
+logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")

 # --- RACINE DU PROJET ---
 try:
@@ -27,9 +26,9 @@ except NameError:
     RACINE = Path.cwd()

 # --- CHEMINS (names and files in English) ---
-DONNEES_CSV   = RACINE / 'zz-data' / 'chapter07' / '07_cs2_matrix.csv'
-META_JSON     = RACINE / 'zz-data' / 'chapter07' / '07_meta_perturbations.json'
-FIGURE_SORTIE = RACINE / 'zz-figures' / 'chapter07' / 'fig_01_cs2_heatmap_k_a.png'
+DONNEES_CSV = RACINE / "zz-data" / "chapter07" / "07_cs2_matrix.csv"
+META_JSON = RACINE / "zz-data" / "chapter07" / "07_meta_perturbations.json"
+FIGURE_SORTIE = RACINE / "zz-figures" / "chapter07" / "fig_01_cs2_heatmap_k_a.png"

 logging.info("Début du tracé de la figure 01 – Carte de chaleur de c_s²(k,a)")

@@ -37,8 +36,8 @@ logging.info("Début du tracé de la figure 01 – Carte de chaleur de c_s²(k,a
 if not META_JSON.exists():
     logging.error("Méta-paramètres introuvable : %s", META_JSON)
     raise FileNotFoundError(META_JSON)
-meta = json.loads(META_JSON.read_text(encoding='utf-8'))
-k_split = float(meta.get('x_split', meta.get('k_split', 0.0)))
+meta = json.loads(META_JSON.read_text(encoding="utf-8"))
+k_split = float(meta.get("x_split", meta.get("k_split", 0.0)))
 logging.info("Lecture de k_split = %.2e [h/Mpc]", k_split)

 # --- CHARGEMENT DES DONNÉES ---
@@ -49,13 +48,13 @@ df = pd.read_csv(DONNEES_CSV)
 logging.info("Chargement terminé : %d lignes", len(df))

 try:
-    pivot = df.pivot(index='k', columns='a', values='cs2_matrice')
+    pivot = df.pivot(index="k", columns="a", values="cs2_matrice")
 except KeyError:
     logging.error("Colonnes 'k','a','cs2_matrice' manquantes dans %s", DONNEES_CSV)
     raise
 k_vals = pivot.index.to_numpy()
 a_vals = pivot.columns.to_numpy()
-mat    = pivot.to_numpy()
+mat = pivot.to_numpy()
 logging.info("Matrice brute : %d×%d (k×a)", mat.shape[0], mat.shape[1])

 # Masquage des valeurs non finies ou ≤ 0
@@ -72,40 +71,47 @@ if vmin >= vmax:
 logging.info("LogNorm vmin=%.3e vmax=%.3e", vmin, vmax)

 # --- Pas de usetex, on utilise mathtext natif ---
-plt.rc('font', family='serif')
+plt.rc("font", family="serif")

 # --- TRACÉ ---
 fig, ax = plt.subplots(figsize=(8, 5))

-cmap = plt.get_cmap('Blues')
+cmap = plt.get_cmap("Blues")

 mesh = ax.pcolormesh(
-    a_vals, k_vals, mat_masked,
+    a_vals,
+    k_vals,
+    mat_masked,
     norm=LogNorm(vmin=vmin, vmax=vmax),
-    cmap=cmap, shading='auto'
+    cmap=cmap,
+    shading="auto",
 )

-ax.set_xscale('linear')
-ax.set_yscale('log')
-ax.set_xlabel(r'$a$ (facteur d\'échelle)', fontsize='small')
-ax.set_ylabel(r'$k$ [h/Mpc]',             fontsize='small')
-ax.set_title(r'Carte de chaleur de $c_s^2(k,a)$', fontsize='small')
+ax.set_xscale("linear")
+ax.set_yscale("log")
+ax.set_xlabel(r"$a$ (facteur d\'échelle)", fontsize="small")
+ax.set_ylabel(r"$k$ [h/Mpc]", fontsize="small")
+ax.set_title(r"Carte de chaleur de $c_s^2(k,a)$", fontsize="small")

 # Ticks en taille small
 for lbl in ax.xaxis.get_ticklabels() + ax.yaxis.get_ticklabels():
-    lbl.set_fontsize('small')
+    lbl.set_fontsize("small")

 # Colorbar
 cbar = fig.colorbar(mesh, ax=ax)
-cbar.set_label(r'$c_s^2$', rotation=270, labelpad=15, fontsize='small')
-cbar.ax.yaxis.set_tick_params(labelsize='small')
+cbar.set_label(r"$c_s^2$", rotation=270, labelpad=15, fontsize="small")
+cbar.ax.yaxis.set_tick_params(labelsize="small")

 # Trace de k_split
-ax.axhline(k_split, color='white', linestyle='--', linewidth=1)
+ax.axhline(k_split, color="white", linestyle="--", linewidth=1)
 ax.text(
-    a_vals.max(), k_split * 1.1,
-    r'$k_{\rm split}$',
-    color='white', va='bottom', ha='right', fontsize='small'
+    a_vals.max(),
+    k_split * 1.1,
+    r"$k_{\rm split}$",
+    color="white",
+    va="bottom",
+    ha="right",
+    fontsize="small",
 )
 logging.info("Ajout de la ligne horizontale à k = %.2e", k_split)

diff --git a/zz-scripts/chapter07/plot_fig02_delta_phi_heatmap.py b/zz-scripts/chapter07/plot_fig02_delta_phi_heatmap.py
index 764e95d..88c7215 100755
--- a/zz-scripts/chapter07/plot_fig02_delta_phi_heatmap.py
+++ b/zz-scripts/chapter07/plot_fig02_delta_phi_heatmap.py
@@ -18,7 +18,7 @@ import matplotlib.pyplot as plt
 from matplotlib.colors import PowerNorm

 # --- CONFIGURATION DU LOGGING ---
-logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
+logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")

 # --- RACINE DU PROJET ---
 try:
@@ -28,11 +28,11 @@ except NameError:
 sys.path.insert(0, str(RACINE))

 # --- PATHS (directory and file names in English) ---
-DONNEES_DIR  = RACINE / 'zz-data'  / 'chapter07'
-FIG_DIR      = RACINE / 'zz-figures' / 'chapter07'
-CSV_MATRICE  = DONNEES_DIR  / '07_delta_phi_matrix.csv'
-JSON_META    = DONNEES_DIR  / '07_meta_perturbations.json'
-FIG_OUT      = FIG_DIR      / 'fig_02_delta_phi_heatmap_k_a.png'
+DONNEES_DIR = RACINE / "zz-data" / "chapter07"
+FIG_DIR = RACINE / "zz-figures" / "chapter07"
+CSV_MATRICE = DONNEES_DIR / "07_delta_phi_matrix.csv"
+JSON_META = DONNEES_DIR / "07_meta_perturbations.json"
+FIG_OUT = FIG_DIR / "fig_02_delta_phi_heatmap_k_a.png"

 logging.info("Début du tracé de la figure 02 – Carte de chaleur de δφ/φ")

@@ -40,8 +40,8 @@ logging.info("Début du tracé de la figure 02 – Carte de chaleur de δφ/φ")
 if not JSON_META.exists():
     logging.error("Méta-paramètres introuvable : %s", JSON_META)
     raise FileNotFoundError(JSON_META)
-meta    = json.loads(JSON_META.read_text(encoding='utf-8'))
-k_split = float(meta.get('x_split', meta.get('k_split', 0.0)))
+meta = json.loads(JSON_META.read_text(encoding="utf-8"))
+k_split = float(meta.get("x_split", meta.get("k_split", 0.0)))
 logging.info("Lecture de k_split = %.2e [h/Mpc]", k_split)

 # --- CHARGEMENT DES DONNÉES 2D ---
@@ -52,9 +52,11 @@ df = pd.read_csv(CSV_MATRICE)
 logging.info("Chargement terminé : %d lignes", len(df))

 try:
-    pivot = df.pivot(index='k', columns='a', values='delta_phi_matrice')
+    pivot = df.pivot(index="k", columns="a", values="delta_phi_matrice")
 except KeyError:
-    logging.error("Colonnes 'k','a','delta_phi_matrice' manquantes dans %s", CSV_MATRICE)
+    logging.error(
+        "Colonnes 'k','a','delta_phi_matrice' manquantes dans %s", CSV_MATRICE
+    )
     raise
 k_vals = pivot.index.to_numpy()
 a_vals = pivot.columns.to_numpy()
@@ -72,52 +74,53 @@ vmin, vmax = 1e-6, 1e-5
 logging.info("Colorbar fixed range: [%.1e, %.1e]", vmin, vmax)

 norm = PowerNorm(gamma=0.5, vmin=vmin, vmax=vmax)
-cmap = plt.get_cmap('Oranges')
-cmap.set_bad(color='lightgrey', alpha=0.8)
+cmap = plt.get_cmap("Oranges")
+cmap.set_bad(color="lightgrey", alpha=0.8)

 # --- FONTS (mathtext natif) ---
-plt.rc('font', family='serif')
+plt.rc("font", family="serif")

 # --- TRACÉ ---
 FIG_DIR.mkdir(parents=True, exist_ok=True)
 fig, ax = plt.subplots(figsize=(8, 5))

-mesh = ax.pcolormesh(
-    a_vals, k_vals, mat,
-    cmap=cmap, norm=norm,
-    shading='auto'
-)
+mesh = ax.pcolormesh(a_vals, k_vals, mat, cmap=cmap, norm=norm, shading="auto")

-ax.set_xscale('linear')
-ax.set_yscale('log')
-ax.set_xlabel(r'$a$ (facteur d’échelle)', fontsize='small')
-ax.set_ylabel(r'$k$ [h/Mpc]',            fontsize='small')
-ax.set_title(r'Carte de chaleur de $\delta\phi/\phi(k,a)$', fontsize='small')
+ax.set_xscale("linear")
+ax.set_yscale("log")
+ax.set_xlabel(r"$a$ (facteur d’échelle)", fontsize="small")
+ax.set_ylabel(r"$k$ [h/Mpc]", fontsize="small")
+ax.set_title(r"Carte de chaleur de $\delta\phi/\phi(k,a)$", fontsize="small")

 # Ticks en taille small
 for lbl in ax.xaxis.get_ticklabels() + ax.yaxis.get_ticklabels():
-    lbl.set_fontsize('small')
+    lbl.set_fontsize("small")

 # Contours guides (en blanc, semi-opaques)
 levels = np.logspace(np.log10(vmin), np.log10(vmax), 5)
-ax.contour(a_vals, k_vals, mat_raw, levels=levels,
-           colors='white', linewidths=0.5, alpha=0.7)
+ax.contour(
+    a_vals, k_vals, mat_raw, levels=levels, colors="white", linewidths=0.5, alpha=0.7
+)

 # Repère k_split en haut à droite
-ax.axhline(k_split, color='black', linestyle='--', linewidth=1)
+ax.axhline(k_split, color="black", linestyle="--", linewidth=1)
 ax.text(
-    a_vals.max(), k_split * 1.1,
-    r'$k_{\rm split}$',
-    va='bottom', ha='right', fontsize='small', color='black'
+    a_vals.max(),
+    k_split * 1.1,
+    r"$k_{\rm split}$",
+    va="bottom",
+    ha="right",
+    fontsize="small",
+    color="black",
 )

 # --- BARRE DE COULEUR ---
-cbar = fig.colorbar(mesh, ax=ax, pad=0.02, extend='both')
-cbar.set_label(r'$\delta\phi/\phi$', rotation=270, labelpad=15, fontsize='small')
+cbar = fig.colorbar(mesh, ax=ax, pad=0.02, extend="both")
+cbar.set_label(r"$\delta\phi/\phi$", rotation=270, labelpad=15, fontsize="small")
 ticks = np.logspace(np.log10(vmin), np.log10(vmax), 5)
 cbar.set_ticks(ticks)
 cbar.set_ticklabels([f"$10^{{{int(np.round(np.log10(t)))}}}$" for t in ticks])
-cbar.ax.yaxis.set_tick_params(labelsize='small')
+cbar.ax.yaxis.set_tick_params(labelsize="small")

 # --- SAUVEGARDE ---
 fig.tight_layout()
diff --git a/zz-scripts/chapter07/plot_fig03_invariant_I1.py b/zz-scripts/chapter07/plot_fig03_invariant_I1.py
index 159f213..e904c60 100755
--- a/zz-scripts/chapter07/plot_fig03_invariant_I1.py
+++ b/zz-scripts/chapter07/plot_fig03_invariant_I1.py
@@ -4,7 +4,9 @@
 Figure 03 – Invariant scalaire I₁(k)=c_s²/k (Chapitre 7, MCGT)
 """

-import json, logging, sys
+import json
+import logging
+import sys
 from pathlib import Path

 import numpy as np
@@ -16,19 +18,19 @@ ROOT = Path(__file__).resolve().parents[2]
 sys.path.insert(0, str(ROOT))

 # Paths (directory and file names in English)
-DATA_CSV  = ROOT / "zz-data" / "chapter07" / "07_scalar_invariants.csv"
+DATA_CSV = ROOT / "zz-data" / "chapter07" / "07_scalar_invariants.csv"
 JSON_META = ROOT / "zz-data" / "chapter07" / "07_meta_perturbations.json"
-FIG_OUT   = ROOT / "zz-figures" / "chapter07" / "fig_03_invariant_I1.png"
+FIG_OUT = ROOT / "zz-figures" / "chapter07" / "fig_03_invariant_I1.png"

 logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")

 # ─────────────────── Chargement
 df = pd.read_csv(DATA_CSV, comment="#")
-k  = df["k"].to_numpy()
+k = df["k"].to_numpy()
 I1 = df.iloc[:, 1].to_numpy()

 # Masque strict : valeurs >0 et finies
-m   = (I1 > 0) & np.isfinite(I1)
+m = (I1 > 0) & np.isfinite(I1)
 k, I1 = k[m], I1[m]

 # Récupération de k_split
@@ -38,23 +40,34 @@ if JSON_META.exists():
     k_split = float(meta.get("x_split", meta.get("k_split", np.nan)))

 # ─────────────────── Tracé
-fig, ax = plt.subplots(figsize=(8,5), constrained_layout=True)
+fig, ax = plt.subplots(figsize=(8, 5), constrained_layout=True)

 ax.loglog(k, I1, lw=2, color="#1f77b4", label=r"$I_1(k)=c_s^2/k$")

 # loi ∝ k⁻¹ sur une décennie après k_split
 if np.isfinite(k_split):
-    kk = np.logspace(np.log10(k_split)-1, np.log10(k_split), 2)
-    ax.loglog(kk, (I1[np.argmin(abs(k-k_split))]*k_split)/kk,
-              ls="--", color="k", label=r"$\propto k^{-1}$")
+    kk = np.logspace(np.log10(k_split) - 1, np.log10(k_split), 2)
+    ax.loglog(
+        kk,
+        (I1[np.argmin(abs(k - k_split))] * k_split) / kk,
+        ls="--",
+        color="k",
+        label=r"$\propto k^{-1}$",
+    )
     ax.axvline(k_split, ls="--", color="k")
-    ax.text(k_split, I1.min()*1.1, r"$k_{\rm split}$",
-            ha="center", va="bottom", fontsize=9)
+    ax.text(
+        k_split,
+        I1.min() * 1.1,
+        r"$k_{\rm split}$",
+        ha="center",
+        va="bottom",
+        fontsize=9,
+    )

 # Limites Y : 2 décennies sous la médiane
 y_med = np.median(I1)
-ymin  = 10**(np.floor(np.log10(y_med))-2)
-ymax  = I1.max()*1.2
+ymin = 10 ** (np.floor(np.log10(y_med)) - 2)
+ymax = I1.max() * 1.2
 ax.set_ylim(ymin, ymax)

 # Axes / grille
@@ -62,13 +75,13 @@ ax.set_xlabel(r"$k\, [h/\mathrm{Mpc}]$")
 ax.set_ylabel(r"$I_1(k)$")
 ax.set_title(r"Invariant scalaire $I_1(k)$")

-ax.xaxis.set_minor_locator(LogLocator(base=10, subs=range(2,10)))
+ax.xaxis.set_minor_locator(LogLocator(base=10, subs=range(2, 10)))
 ax.yaxis.set_major_locator(LogLocator(base=10))
-ax.yaxis.set_minor_locator(LogLocator(base=10, subs=range(2,10)))
+ax.yaxis.set_minor_locator(LogLocator(base=10, subs=range(2, 10)))
 ax.yaxis.set_major_formatter(LogFormatterSciNotation(base=10))

-ax.grid(which="major", ls=":", lw=.6, color="#888", alpha=.6)
-ax.grid(which="minor", ls=":", lw=.4, color="#ccc", alpha=.4)
+ax.grid(which="major", ls=":", lw=0.6, color="#888", alpha=0.6)
+ax.grid(which="minor", ls=":", lw=0.4, color="#ccc", alpha=0.4)

 ax.legend(frameon=False)

diff --git a/zz-scripts/chapter07/plot_fig04_dcs2_vs_k.py b/zz-scripts/chapter07/plot_fig04_dcs2_vs_k.py
index 6c34de3..918f2f2 100755
--- a/zz-scripts/chapter07/plot_fig04_dcs2_vs_k.py
+++ b/zz-scripts/chapter07/plot_fig04_dcs2_vs_k.py
@@ -23,23 +23,23 @@ logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
 plt.style.use("classic")

 # --- Définitions des chemins (noms en anglais) ---
-ROOT      = Path(__file__).resolve().parents[2]
+ROOT = Path(__file__).resolve().parents[2]
 sys.path.insert(0, str(ROOT))
-DATA_DIR  = ROOT / "zz-data" / "chapter07"
-FIG_DIR   = ROOT / "zz-figures" / "chapter07"
+DATA_DIR = ROOT / "zz-data" / "chapter07"
+FIG_DIR = ROOT / "zz-figures" / "chapter07"
 META_JSON = DATA_DIR / "07_meta_perturbations.json"
-CSV_DCS2  = DATA_DIR / "07_dcs2_dk.csv"
-FIG_OUT   = FIG_DIR  / "fig_04_dcs2_vs_k.png"
+CSV_DCS2 = DATA_DIR / "07_dcs2_dk.csv"
+FIG_OUT = FIG_DIR / "fig_04_dcs2_vs_k.png"

 # --- Lecture de k_split ---
-meta    = json.loads(META_JSON.read_text("utf-8"))
+meta = json.loads(META_JSON.read_text("utf-8"))
 k_split = float(meta.get("x_split", 0.02))
 logging.info("k_split = %.2e h/Mpc", k_split)

 # --- Chargement des données ---
-df     = pd.read_csv(CSV_DCS2, comment="#")
+df = pd.read_csv(CSV_DCS2, comment="#")
 k_vals = df["k"].to_numpy()
-dcs2   = df.iloc[:, 1].to_numpy()
+dcs2 = df.iloc[:, 1].to_numpy()
 logging.info("Loaded %d points from %s", len(df), CSV_DCS2.name)

 # --- Création de la figure ---
@@ -47,23 +47,19 @@ FIG_DIR.mkdir(parents=True, exist_ok=True)
 fig, ax = plt.subplots(figsize=(8, 5))

 # Tracé de |∂ₖ c_s²|
-ax.loglog(
-    k_vals,
-    np.abs(dcs2),
-    color="C1",
-    lw=2,
-    label=r"$|\partial_k\,c_s^2|$"
-)
+ax.loglog(k_vals, np.abs(dcs2), color="C1", lw=2, label=r"$|\partial_k\,c_s^2|$")

 # Ligne verticale k_split
 ax.axvline(k_split, color="k", ls="--", lw=1)
 ax.text(
-    k_split, 0.85, r"$k_{\rm split}$",
+    k_split,
+    0.85,
+    r"$k_{\rm split}$",
     transform=ax.get_xaxis_transform(),
     rotation=90,
     va="bottom",
     ha="right",
-    fontsize=9
+    fontsize=9,
 )

 # Labels et titre
@@ -77,9 +73,10 @@ ax.grid(which="minor", ls=":", lw=0.3, alpha=0.7)

 # Locators pour axes log
 ax.xaxis.set_major_locator(LogLocator(base=10))
-ax.xaxis.set_minor_locator(LogLocator(base=10, subs=(2,5)))
+ax.xaxis.set_minor_locator(LogLocator(base=10, subs=(2, 5)))
 ax.yaxis.set_major_locator(LogLocator(base=10))
-ax.yaxis.set_minor_locator(LogLocator(base=10, subs=(2,5)))
+ax.yaxis.set_minor_locator(LogLocator(base=10, subs=(2, 5)))
+

 # Formatter pour n'afficher que les puissances de 10
 def pow_fmt(x, pos):
@@ -87,6 +84,7 @@ def pow_fmt(x, pos):
         return ""
     return rf"$10^{{{int(np.log10(x))}}}$"

+
 ax.xaxis.set_major_formatter(FuncFormatter(pow_fmt))
 ax.yaxis.set_major_formatter(FuncFormatter(pow_fmt))

diff --git a/zz-scripts/chapter07/plot_fig05_ddelta_phi_vs_k.py b/zz-scripts/chapter07/plot_fig05_ddelta_phi_vs_k.py
index 0171850..6163579 100755
--- a/zz-scripts/chapter07/plot_fig05_ddelta_phi_vs_k.py
+++ b/zz-scripts/chapter07/plot_fig05_ddelta_phi_vs_k.py
@@ -27,16 +27,16 @@ ROOT = Path(__file__).resolve().parents[2]
 sys.path.insert(0, str(ROOT))

 # --- Paths (English names for directories and files) ---
-DATA_DIR  = ROOT / "zz-data"  / "chapter07"
-CSV_DDK   = DATA_DIR / "07_ddelta_phi_dk.csv"
+DATA_DIR = ROOT / "zz-data" / "chapter07"
+CSV_DDK = DATA_DIR / "07_ddelta_phi_dk.csv"
 JSON_META = DATA_DIR / "07_meta_perturbations.json"
-FIG_DIR   = ROOT / "zz-figures" / "chapter07"
-FIG_OUT   = FIG_DIR / "fig_05_ddelta_phi_vs_k.png"
+FIG_DIR = ROOT / "zz-figures" / "chapter07"
+FIG_OUT = FIG_DIR / "fig_05_ddelta_phi_vs_k.png"

 # --- Lecture de k_split ---
 if not JSON_META.exists():
     raise FileNotFoundError(f"Meta parameters not found: {JSON_META}")
-meta    = json.loads(JSON_META.read_text("utf-8"))
+meta = json.loads(JSON_META.read_text("utf-8"))
 k_split = float(meta.get("x_split", 0.02))
 logging.info("k_split = %.2e h/Mpc", k_split)

@@ -47,35 +47,22 @@ df = pd.read_csv(CSV_DDK, comment="#")
 logging.info("Loaded %d points from %s", len(df), CSV_DDK.name)

 k_vals = df["k"].to_numpy()
-ddphi   = df.iloc[:, 1].to_numpy()
-abs_dd  = np.abs(ddphi)
+ddphi = df.iloc[:, 1].to_numpy()
+abs_dd = np.abs(ddphi)

 # --- Tracé ---
 FIG_DIR.mkdir(parents=True, exist_ok=True)
 fig, ax = plt.subplots(figsize=(8, 5), constrained_layout=True)

 # Courbe
-ax.loglog(
-    k_vals,
-    abs_dd,
-    color="C2",
-    lw=2,
-    label=r"$|\partial_k(\delta\phi/\phi)|$"
-)
+ax.loglog(k_vals, abs_dd, color="C2", lw=2, label=r"$|\partial_k(\delta\phi/\phi)|$")

 # Repère k_split
 ax.axvline(k_split, ls="--", color="gray", lw=1)
 # Label k_split placé juste au-dessus de ymin
 ymin, ymax = 1e-50, 1e-2
-y_text = 10 ** (np.log10(ymin) + 0.05*(np.log10(ymax)-np.log10(ymin)))
-ax.text(
-    k_split*1.05,
-    y_text,
-    r"$k_{\rm split}$",
-    ha="left",
-    va="bottom",
-    fontsize=9
-)
+y_text = 10 ** (np.log10(ymin) + 0.05 * (np.log10(ymax) - np.log10(ymin)))
+ax.text(k_split * 1.05, y_text, r"$k_{\rm split}$", ha="left", va="bottom", fontsize=9)

 # Limites
 ax.set_ylim(ymin, ymax)
@@ -100,7 +87,7 @@ ax.grid(which="minor", ls=":", lw=0.3, alpha=0.7)

 # Locators X
 ax.xaxis.set_major_locator(LogLocator(base=10))
-ax.xaxis.set_minor_locator(LogLocator(base=10, subs=(2,5)))
+ax.xaxis.set_minor_locator(LogLocator(base=10, subs=(2, 5)))

 # Légende
 ax.legend(loc="upper right", frameon=False)
diff --git a/zz-scripts/chapter07/plot_fig06_comparison.py b/zz-scripts/chapter07/plot_fig06_comparison.py
index c492fa1..ed17aca 100755
--- a/zz-scripts/chapter07/plot_fig06_comparison.py
+++ b/zz-scripts/chapter07/plot_fig06_comparison.py
@@ -23,31 +23,32 @@ logger = logging.getLogger(__name__)
 ROOT = Path(__file__).resolve().parents[2]

 # --- Paths (English names for directories and files) ---
-DATA_DIR  = ROOT / 'zz-data'  / 'chapter07'
-INV_CSV   = DATA_DIR / '07_scalar_invariants.csv'
-DCS2_CSV  = DATA_DIR / '07_derivative_cs2_dk.csv'
-DDPHI_CSV = DATA_DIR / '07_derivative_ddelta_phi_dk.csv'
-META_JSON = DATA_DIR / '07_meta_perturbations.json'
-FIG_OUT   = ROOT / 'zz-figures' / 'chapter07' / 'fig_06_comparison.png'
+DATA_DIR = ROOT / "zz-data" / "chapter07"
+INV_CSV = DATA_DIR / "07_scalar_invariants.csv"
+DCS2_CSV = DATA_DIR / "07_derivative_cs2_dk.csv"
+DDPHI_CSV = DATA_DIR / "07_derivative_ddelta_phi_dk.csv"
+META_JSON = DATA_DIR / "07_meta_perturbations.json"
+FIG_OUT = ROOT / "zz-figures" / "chapter07" / "fig_06_comparison.png"

 # --- Read k_split ---
-with open(META_JSON, 'r', encoding='utf-8') as f:
+with open(META_JSON, "r", encoding="utf-8") as f:
     meta = json.load(f)
-k_split = float(meta.get('x_split', 0.02))
+k_split = float(meta.get("x_split", 0.02))
 logger.info("k_split = %.2e h/Mpc", k_split)

 # --- Load data ---
-df_inv  = pd.read_csv(INV_CSV)
+df_inv = pd.read_csv(INV_CSV)
 df_dcs2 = pd.read_csv(DCS2_CSV)
-df_ddp  = pd.read_csv(DDPHI_CSV)
+df_ddp = pd.read_csv(DDPHI_CSV)

-k1, I1   = df_inv['k'].values,  df_inv.iloc[:,1].values
-k2, dcs2 = df_dcs2['k'].values, df_dcs2.iloc[:,1].values
-k3, ddp  = df_ddp['k'].values,  df_ddp.iloc[:,1].values
+k1, I1 = df_inv["k"].values, df_inv.iloc[:, 1].values
+k2, dcs2 = df_dcs2["k"].values, df_dcs2.iloc[:, 1].values
+k3, ddp = df_ddp["k"].values, df_ddp.iloc[:, 1].values

 # Mask zeros for derivative of delta phi/phi
 ddp_mask = np.ma.masked_where(np.abs(ddp) <= 0, np.abs(ddp))

+
 # Function to annotate the plateau region
 def zoom_plateau(ax, k, y):
     sel = k < k_split
@@ -56,34 +57,39 @@ def zoom_plateau(ax, k, y):
         return
     lo, hi = ysel.min(), ysel.max()
     ax.set_ylim(lo * 0.8, hi * 1.2)
-    xm = k[sel][len(ysel)//2]
+    xm = k[sel][len(ysel) // 2]
     ym = np.sqrt(lo * hi)
     ax.text(
-        xm, ym, 'Plateau',
-        ha='center', va='center',
-        fontsize=7, bbox=dict(boxstyle='round', fc='white', alpha=0.7)
+        xm,
+        ym,
+        "Plateau",
+        ha="center",
+        va="center",
+        fontsize=7,
+        bbox=dict(boxstyle="round", fc="white", alpha=0.7),
     )

+
 # --- Create figure ---
 fig, axs = plt.subplots(3, 1, figsize=(8, 14), sharex=True)

 # 1) I₁ = c_s²/k
 ax = axs[0]
-ax.loglog(k1, I1, color='C0', label=r'$I_1 = c_s^2/k$')
-ax.axvline(k_split, ls='--', color='k', lw=1)
+ax.loglog(k1, I1, color="C0", label=r"$I_1 = c_s^2/k$")
+ax.axvline(k_split, ls="--", color="k", lw=1)
 zoom_plateau(ax, k1, I1)
-ax.set_ylabel(r'$I_1(k)$', fontsize=10)
-ax.legend(loc='upper right', fontsize=8, framealpha=0.8)
-ax.grid(True, which='both', ls=':', linewidth=0.5)
+ax.set_ylabel(r"$I_1(k)$", fontsize=10)
+ax.legend(loc="upper right", fontsize=8, framealpha=0.8)
+ax.grid(True, which="both", ls=":", linewidth=0.5)

 # 2) |∂ₖ c_s²|
 ax = axs[1]
-ax.loglog(k2, np.abs(dcs2), color='C1', label=r'$|\partial_k c_s^2|$')
-ax.axvline(k_split, ls='--', color='k', lw=1)
+ax.loglog(k2, np.abs(dcs2), color="C1", label=r"$|\partial_k c_s^2|$")
+ax.axvline(k_split, ls="--", color="k", lw=1)
 zoom_plateau(ax, k2, np.abs(dcs2))
-ax.set_ylabel(r'$|\partial_k c_s^2|$', fontsize=10)
-ax.legend(loc='upper right', fontsize=8, framealpha=0.8)
-ax.grid(True, which='both', ls=':', linewidth=0.5)
+ax.set_ylabel(r"$|\partial_k c_s^2|$", fontsize=10)
+ax.legend(loc="upper right", fontsize=8, framealpha=0.8)
+ax.grid(True, which="both", ls=":", linewidth=0.5)

 # → Adjust upper limit to emphasize the peak
 ymin, _ = ax.get_ylim()
@@ -91,22 +97,19 @@ ax.set_ylim(ymin, 1e1)

 # 3) |∂ₖ(δφ/φ)|
 ax = axs[2]
-ax.loglog(k3, ddp_mask, color='C2',
-          label=r'$|\partial_k(\delta\phi/\phi)|_{\mathrm{smooth}}$')
-ax.axvline(k_split, ls='--', color='k', lw=1)
+ax.loglog(
+    k3, ddp_mask, color="C2", label=r"$|\partial_k(\delta\phi/\phi)|_{\mathrm{smooth}}$"
+)
+ax.axvline(k_split, ls="--", color="k", lw=1)
 zoom_plateau(ax, k3, ddp_mask)
-ax.set_ylabel(r'$|\partial_k(\delta\phi/\phi)|$', fontsize=10)
-ax.set_xlabel(r'$k\,[h/\mathrm{Mpc}]$', fontsize=10)
-ax.legend(loc='upper right', fontsize=8, framealpha=0.8)
-ax.grid(True, which='both', ls=':', linewidth=0.5)
+ax.set_ylabel(r"$|\partial_k(\delta\phi/\phi)|$", fontsize=10)
+ax.set_xlabel(r"$k\,[h/\mathrm{Mpc}]$", fontsize=10)
+ax.legend(loc="upper right", fontsize=8, framealpha=0.8)
+ax.grid(True, which="both", ls=":", linewidth=0.5)

 # --- Title and layout ---
-fig.suptitle('Comparaison des invariants et dérivées', fontsize=14)
-fig.subplots_adjust(
-    top=0.92, bottom=0.07,
-    left=0.10, right=0.95,
-    hspace=0.30
-)
+fig.suptitle("Comparaison des invariants et dérivées", fontsize=14)
+fig.subplots_adjust(top=0.92, bottom=0.07, left=0.10, right=0.95, hspace=0.30)

 # --- Save ---
 FIG_OUT.parent.mkdir(parents=True, exist_ok=True)
diff --git a/zz-scripts/chapter07/plot_fig07_invariant_I2.py b/zz-scripts/chapter07/plot_fig07_invariant_I2.py
index cb86c2b..85621d9 100755
--- a/zz-scripts/chapter07/plot_fig07_invariant_I2.py
+++ b/zz-scripts/chapter07/plot_fig07_invariant_I2.py
@@ -13,37 +13,38 @@ import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt

+
 def main():
     # --- chemins ---
-    ROOT      = Path(__file__).resolve().parents[2]
-    DATA_DIR  = ROOT / 'zz-data'  / 'chapter07'
-    CSV_DATA  = DATA_DIR / '07_scalar_perturbations_results.csv'
-    JSON_META = DATA_DIR / '07_meta_perturbations.json'
-    FIG_DIR   = ROOT / 'zz-figures'  / 'chapter07'
-    FIG_OUT   = FIG_DIR / 'fig_07_invariant_I2.png'
+    ROOT = Path(__file__).resolve().parents[2]
+    DATA_DIR = ROOT / "zz-data" / "chapter07"
+    CSV_DATA = DATA_DIR / "07_scalar_perturbations_results.csv"
+    JSON_META = DATA_DIR / "07_meta_perturbations.json"
+    FIG_DIR = ROOT / "zz-figures" / "chapter07"
+    FIG_OUT = FIG_DIR / "fig_07_invariant_I2.png"
     FIG_DIR.mkdir(parents=True, exist_ok=True)

     # --- logging ---
-    logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
+    logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
     logging.info("→ génération de la figure 07 – Invariant I₂")

     # --- chargement des données ---
     df = pd.read_csv(CSV_DATA)
     if df.empty:
         raise RuntimeError(f"Aucune donnée dans {CSV_DATA}")
-    if 'delta_phi_interp' not in df.columns:
+    if "delta_phi_interp" not in df.columns:
         raise KeyError("La colonne 'delta_phi_interp' est introuvable dans le CSV")

-    k         = df['k'].to_numpy()
-    delta_phi = df['delta_phi_interp'].to_numpy()
+    k = df["k"].to_numpy()
+    delta_phi = df["delta_phi_interp"].to_numpy()

     # --- calcul de I₂ ---
     I2 = k * delta_phi

     # --- lecture de k_split ---
     if JSON_META.exists():
-        meta = json.loads(JSON_META.read_text('utf-8'))
-        k_split = float(meta.get('x_split', 0.02))
+        meta = json.loads(JSON_META.read_text("utf-8"))
+        k_split = float(meta.get("x_split", 0.02))
     else:
         logging.warning("Méta-paramètres non trouvés → k_split=0.02")
         k_split = 0.02
@@ -51,56 +52,63 @@ def main():

     # --- préparation du tracé ---
     fig, ax = plt.subplots(figsize=(8, 5))
-    ax.loglog(k, I2, color='C3', linewidth=2, label=r'$I_2(k)=k\,\frac{\delta\phi}{\phi}$')
+    ax.loglog(
+        k, I2, color="C3", linewidth=2, label=r"$I_2(k)=k\,\frac{\delta\phi}{\phi}$"
+    )

     # --- bornes Y centrées sur le plateau (k < k_split) ---
     mask_plateau = k < k_split
     if not np.any(mask_plateau):
         raise RuntimeError("Aucune valeur de k < k_split pour définir le plateau.")
     bottom = I2[mask_plateau].min() * 0.5
-    top    = I2[mask_plateau].max() * 1.2
+    top = I2[mask_plateau].max() * 1.2
     ax.set_ylim(bottom, top)

     # --- ligne verticale k_split ---
-    ax.axvline(k_split, color='k', ls='--', lw=1)
+    ax.axvline(k_split, color="k", ls="--", lw=1)
     ax.text(
-        k_split, bottom * 1.2,
-        r'$k_{\rm split}$',
-        ha='center', va='bottom',
-        fontsize=10, backgroundcolor='white'
+        k_split,
+        bottom * 1.2,
+        r"$k_{\rm split}$",
+        ha="center",
+        va="bottom",
+        fontsize=10,
+        backgroundcolor="white",
     )

     # --- annotation Plateau ---
-    x_plt = k[mask_plateau][len(k[mask_plateau])//2]
+    x_plt = k[mask_plateau][len(k[mask_plateau]) // 2]
     y_plt = I2[mask_plateau].mean()
     ax.text(
-        x_plt, y_plt,
+        x_plt,
+        y_plt,
         "Plateau",
         fontsize=9,
-        bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.7)
+        bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.7),
     )

     # --- axes, titre, labels ---
-    ax.set_xlabel(r'$k\;[h/\mathrm{Mpc}]$', fontsize=12)
-    ax.set_ylabel(r'$I_2(k)$',            fontsize=12)
-    ax.set_title('Invariant scalaire $I_2(k)$', fontsize=14)
+    ax.set_xlabel(r"$k\;[h/\mathrm{Mpc}]$", fontsize=12)
+    ax.set_ylabel(r"$I_2(k)$", fontsize=12)
+    ax.set_title("Invariant scalaire $I_2(k)$", fontsize=14)

     # --- ticks Y explicites ---
-    dmin    = int(np.floor(np.log10(bottom)))
-    dmax    = int(np.ceil (np.log10(top)))
-    decades = np.arange(dmin, dmax+1)
-    y_ticks = 10.0 ** decades
+    dmin = int(np.floor(np.log10(bottom)))
+    dmax = int(np.ceil(np.log10(top)))
+    decades = np.arange(dmin, dmax + 1)
+    y_ticks = 10.0**decades
     ax.set_yticks(y_ticks)
     ax.set_yticklabels([f"$10^{{{d}}}$" for d in decades])

     # --- grille et légende ---
-    ax.grid(which='both', ls=':', lw=0.5, color='gray', alpha=0.7)
-    ax.legend(loc='upper right', frameon=True)
+    ax.grid(which="both", ls=":", lw=0.5, color="gray", alpha=0.7)
+    ax.legend(loc="upper right", frameon=True)

     # --- sauvegarde ---
     fig.tight_layout()
     fig.savefig(FIG_OUT, dpi=300)
     logging.info("Figure enregistrée → %s", FIG_OUT)

-if __name__ == '__main__':
+
+if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter07/tests/test_chapter07.py b/zz-scripts/chapter07/tests/test_chapter07.py
index 6fa0749..143ff02 100755
--- a/zz-scripts/chapter07/tests/test_chapter07.py
+++ b/zz-scripts/chapter07/tests/test_chapter07.py
@@ -1,13 +1,17 @@
 # --- auto-inserted by migration helper ---
 from pathlib import Path
 import pytest
+
 _ROOT = Path(__file__).resolve().parents[2]
 _CANDIDATES = [
     _ROOT / "zz-data/chapter07/07_phase_run.csv",
 ]
 _DATA_07 = next((c for c in _CANDIDATES if c.exists()), None)
 if _DATA_07 is None:
-    pytest.skip("missing 07_phase_run.csv (chapter07); skipping data-dependent tests", allow_module_level=True)
+    pytest.skip(
+        "missing 07_phase_run.csv (chapter07); skipping data-dependent tests",
+        allow_module_level=True,
+    )
 # ------------------------------------------------

 # zz-scripts/chapter07/tests/test_chapter07.py
@@ -22,8 +26,10 @@ RTOL = 1e-3
 ROOT = Path(__file__).resolve().parents[3]

 DATA_DIR = ROOT / "zz-data" / "chapter07"
-RAW_CSV  = DATA_DIR / "07_phase_run.csv"
-REF_CSV  = Path(__file__).parent / "ref_phase_run.csv"  # mettre votre CSV de référence ici
+RAW_CSV = DATA_DIR / "07_phase_run.csv"
+REF_CSV = (
+    Path(__file__).parent / "ref_phase_run.csv"
+)  # mettre votre CSV de référence ici


 def test_raw_csv_exists():
@@ -38,24 +44,26 @@ def test_reference_csv_exists():

 def test_shape_matches():
     """Le raw et la référence doivent avoir la même forme."""
-    df      = pd.read_csv(RAW_CSV)
-    df_ref  = pd.read_csv(REF_CSV)
-    assert df.shape == df_ref.shape, f"Formes différentes : {df.shape} vs {df_ref.shape}"
+    df = pd.read_csv(RAW_CSV)
+    df_ref = pd.read_csv(REF_CSV)
+    assert (
+        df.shape == df_ref.shape
+    ), f"Formes différentes : {df.shape} vs {df_ref.shape}"


 def test_no_nan_inf():
     """Aucune valeur NaN ou Inf dans les deux fichiers."""
-    df     = pd.read_csv(RAW_CSV)
+    df = pd.read_csv(RAW_CSV)
     df_ref = pd.read_csv(REF_CSV)
-    assert df.replace([float('inf'), -float('inf')], pd.NA).notna().all().all()
-    assert df_ref.replace([float('inf'), -float('inf')], pd.NA).notna().all().all()
+    assert df.replace([float("inf"), -float("inf")], pd.NA).notna().all().all()
+    assert df_ref.replace([float("inf"), -float("inf")], pd.NA).notna().all().all()


 def test_columns_present():
     """Les colonnes attendues doivent être présentes."""
     expected = {"k", "a", "cs2_raw", "delta_phi_raw"}
-    df       = pd.read_csv(RAW_CSV)
-    df_ref   = pd.read_csv(REF_CSV)
+    df = pd.read_csv(RAW_CSV)
+    df_ref = pd.read_csv(REF_CSV)
     missing_raw = expected - set(df.columns)
     missing_ref = expected - set(df_ref.columns)
     assert not missing_raw, f"Colonnes manquantes dans raw : {missing_raw}"
@@ -64,10 +72,12 @@ def test_columns_present():

 def test_values_within_tolerance():
     """Les valeurs numériques correspondent à la référence à rtol=1e-3."""
-    df     = pd.read_csv(RAW_CSV)
+    df = pd.read_csv(RAW_CSV)
     df_ref = pd.read_csv(REF_CSV)

     for col in ["k", "a", "cs2_raw", "delta_phi_raw"]:
         raw_vals = df[col].to_numpy()
         ref_vals = df_ref[col].to_numpy()
-        assert raw_vals == pytest.approx(ref_vals, rel=RTOL), f"Différence trop grande dans la colonne '{col}'"
+        assert raw_vals == pytest.approx(
+            ref_vals, rel=RTOL
+        ), f"Différence trop grande dans la colonne '{col}'"
diff --git a/zz-scripts/chapter07/utils/test_kgrid.py b/zz-scripts/chapter07/utils/test_kgrid.py
index 17bfbab..df743b6 100755
--- a/zz-scripts/chapter07/utils/test_kgrid.py
+++ b/zz-scripts/chapter07/utils/test_kgrid.py
@@ -11,17 +11,19 @@ import json
 from pathlib import Path
 import numpy as np

+
 def load_params():
     root = Path(__file__).resolve().parents[3]
-    json_path = root / 'zz-data' / 'chapter07' / '07_params_perturbations.json'
-    params = json.loads(json_path.read_text(encoding='utf-8'))
+    json_path = root / "zz-data" / "chapter07" / "07_params_perturbations.json"
+    params = json.loads(json_path.read_text(encoding="utf-8"))
     return params

+
 def main():
     params = load_params()
-    kmin = params['k_min']
-    kmax = params['k_max']
-    dlog = params['dlog']
+    kmin = params["k_min"]
+    kmax = params["k_max"]
+    dlog = params["dlog"]

     # Calcul du nombre de points et création de la grille
     n_k = int((np.log10(kmax) - np.log10(kmin)) / dlog) + 1
@@ -31,5 +33,6 @@ def main():
     print(f"Grille k : de {kgrid[0]:.1e} à {kgrid[-1]:.1e} h/Mpc")
     print(f"Nombre de points : {len(kgrid)}")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter07/utils/toy_model.py b/zz-scripts/chapter07/utils/toy_model.py
index 97480b3..b57b6eb 100755
--- a/zz-scripts/chapter07/utils/toy_model.py
+++ b/zz-scripts/chapter07/utils/toy_model.py
@@ -12,35 +12,38 @@ from pathlib import Path
 import numpy as np
 import matplotlib.pyplot as plt

+
 def load_params():
     # Déterminer la racine du projet
     root = Path(__file__).resolve().parents[3]
-    json_path = root / 'zz-data' / 'chapter07' / '07_params_perturbations.json'
-    params = json.loads(json_path.read_text(encoding='utf-8'))
+    json_path = root / "zz-data" / "chapter07" / "07_params_perturbations.json"
+    params = json.loads(json_path.read_text(encoding="utf-8"))
     return params

+
 def main():
     params = load_params()
-    kmin = params['k_min']
-    kmax = params['k_max']
-    dlog = params['dlog']
+    kmin = params["k_min"]
+    kmax = params["k_max"]
+    dlog = params["dlog"]

     # Construction de la grille k log-uniforme
     n_k = int((np.log10(kmax) - np.log10(kmin)) / dlog) + 1
     kgrid = np.logspace(np.log10(kmin), np.log10(kmax), n_k)

     # Toy-model : sinus en log(k) pour voir les oscillations
-    toy = np.sin(np.log10(kgrid) * 10)**2 + 0.1
+    toy = np.sin(np.log10(kgrid) * 10) ** 2 + 0.1

     # Tracé
     plt.figure(figsize=(6, 4))
-    plt.loglog(kgrid, toy, '.', ms=4)
-    plt.xlabel('k [h/Mpc]')
-    plt.ylabel('Toy model')
+    plt.loglog(kgrid, toy, ".", ms=4)
+    plt.xlabel("k [h/Mpc]")
+    plt.ylabel("Toy model")
     plt.title("Test d'échantillonnage log–log")
-    plt.grid(True, which='both', ls=':')
+    plt.grid(True, which="both", ls=":")
     plt.tight_layout()
     plt.show()

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter08/generate_coupling_milestones.py b/zz-scripts/chapter08/generate_coupling_milestones.py
index c31ef2d..1325c9d 100755
--- a/zz-scripts/chapter08/generate_coupling_milestones.py
+++ b/zz-scripts/chapter08/generate_coupling_milestones.py
@@ -9,32 +9,33 @@ DATA_DIR = os.path.abspath(os.path.join(BASE_DIR, "../../zz-data/chapter08"))
 # 1) Load BAO from the final CSV
 # Original: 08_donnees_bao.csv -> Translated: 08_bao_data.csv
 df_bao = pd.read_csv(os.path.join(DATA_DIR, "08_bao_data.csv"))
-df_bao = df_bao.rename(columns={'DV_obs': 'obs', 'sigma_DV': 'sigma_obs'})
-df_bao['milestone'] = df_bao['z'].apply(lambda z: f"BAO_z={z:.3f}")
-df_bao['category'] = df_bao.apply(
-    lambda row: 'primary' if row.sigma_obs / row.obs <= 0.01 else 'order2',
-    axis=1
+df_bao = df_bao.rename(columns={"DV_obs": "obs", "sigma_DV": "sigma_obs"})
+df_bao["milestone"] = df_bao["z"].apply(lambda z: f"BAO_z={z:.3f}")
+df_bao["category"] = df_bao.apply(
+    lambda row: "primary" if row.sigma_obs / row.obs <= 0.01 else "order2", axis=1
 )

 # 2) Load Pantheon+ from the final CSV
 # Original: 08_donnees_pantheon.csv -> Translated: 08_pantheon_data.csv
 df_sn = pd.read_csv(os.path.join(DATA_DIR, "08_pantheon_data.csv"))
-df_sn = df_sn.rename(columns={'mu_obs': 'obs', 'sigma_mu': 'sigma_obs'})
+df_sn = df_sn.rename(columns={"mu_obs": "obs", "sigma_mu": "sigma_obs"})
 # Create labels SN0, SN1, ...
-df_sn['milestone'] = df_sn.index.map(lambda i: f"SN{i}")
-df_sn['category'] = df_sn.apply(
-    lambda row: 'primary' if row.sigma_obs / row.obs <= 0.01 else 'order2',
-    axis=1
+df_sn["milestone"] = df_sn.index.map(lambda i: f"SN{i}")
+df_sn["category"] = df_sn.apply(
+    lambda row: "primary" if row.sigma_obs / row.obs <= 0.01 else "order2", axis=1
 )

 # 3) Concatenate and keep columns in a consistent order
-df_all = pd.concat([
-    df_bao[['milestone', 'z', 'obs', 'sigma_obs', 'category']],
-    df_sn[['milestone', 'z', 'obs', 'sigma_obs', 'category']]
-], ignore_index=True)
+df_all = pd.concat(
+    [
+        df_bao[["milestone", "z", "obs", "sigma_obs", "category"]],
+        df_sn[["milestone", "z", "obs", "sigma_obs", "category"]],
+    ],
+    ignore_index=True,
+)

 # 4) Export the final CSV (translated name)
 # Original: 08_jalons_couplage.csv -> Translated: 08_coupling_milestones.csv
 out_csv = os.path.join(DATA_DIR, "08_coupling_milestones.csv")
-df_all.to_csv(out_csv, index=False, encoding='utf-8')
+df_all.to_csv(out_csv, index=False, encoding="utf-8")
 print(f"✅ 08_coupling_milestones.csv generated: {out_csv}")
diff --git a/zz-scripts/chapter08/generate_data_chapter08.py b/zz-scripts/chapter08/generate_data_chapter08.py
index 933e1df..2a49a8b 100755
--- a/zz-scripts/chapter08/generate_data_chapter08.py
+++ b/zz-scripts/chapter08/generate_data_chapter08.py
@@ -14,104 +14,137 @@ from pathlib import Path
 from scipy.signal import savgol_filter

 # --- Permet d’importer cosmo.py depuis utils ---
-ROOT  = Path(__file__).resolve().parents[2]
+ROOT = Path(__file__).resolve().parents[2]
 UTILS = ROOT / "zz-scripts" / "chapter08" / "utils"
 sys.path.insert(0, str(UTILS))
 from cosmo import DV, distance_modulus, Omega_m0, Omega_lambda0

+
 def parse_args():
     p = argparse.ArgumentParser(
-        description="Génère les données du Chapitre 8 (Dark coupling)")
-    p.add_argument("--q0star_min", type=float, required=True,
-                   help="Minimum value of q0⋆")
-    p.add_argument("--q0star_max", type=float, required=True,
-                   help="Maximum value of q0⋆")
-    p.add_argument("--n_points",   type=int,   required=True,
-                   help="Number of points in the q0⋆ grid")
-    p.add_argument("--export_derivative", "--export-derivative",
-                   dest="export_derivative", action="store_true",
-                   help="Export the smoothed derivative dχ²/dq0⋆")
-    p.add_argument("--export_heatmap", "--export-heatmap",
-                   dest="export_heatmap", action="store_true",
-                   help="Export the 2D χ² scan")
-    p.add_argument("--param2_min", type=float,
-                   help="Minimum value for the 2nd parameter (with --export-heatmap)")
-    p.add_argument("--param2_max", type=float,
-                   help="Maximum value for the 2nd parameter (with --export-heatmap)")
-    p.add_argument("--n_param2",   type=int, default=50,
-                   help="Number of points for the 2nd parameter")
+        description="Génère les données du Chapitre 8 (Dark coupling)"
+    )
+    p.add_argument(
+        "--q0star_min", type=float, required=True, help="Minimum value of q0⋆"
+    )
+    p.add_argument(
+        "--q0star_max", type=float, required=True, help="Maximum value of q0⋆"
+    )
+    p.add_argument(
+        "--n_points", type=int, required=True, help="Number of points in the q0⋆ grid"
+    )
+    p.add_argument(
+        "--export_derivative",
+        "--export-derivative",
+        dest="export_derivative",
+        action="store_true",
+        help="Export the smoothed derivative dχ²/dq0⋆",
+    )
+    p.add_argument(
+        "--export_heatmap",
+        "--export-heatmap",
+        dest="export_heatmap",
+        action="store_true",
+        help="Export the 2D χ² scan",
+    )
+    p.add_argument(
+        "--param2_min",
+        type=float,
+        help="Minimum value for the 2nd parameter (with --export-heatmap)",
+    )
+    p.add_argument(
+        "--param2_max",
+        type=float,
+        help="Maximum value for the 2nd parameter (with --export-heatmap)",
+    )
+    p.add_argument(
+        "--n_param2",
+        type=int,
+        default=50,
+        help="Number of points for the 2nd parameter",
+    )
     return p.parse_args()

+
 def load_or_init_params(path: Path, args):
     if path.exists():
         params = json.loads(path.read_text(encoding="utf-8"))
     else:
         params = {
-            "thresholds": {"primary":0.01, "order2":0.10},
+            "thresholds": {"primary": 0.01, "order2": 0.10},
             "max_epsilon_primary": None,
-            "max_epsilon_order2": None
+            "max_epsilon_order2": None,
         }
-    params.update({
-        "q0star_min": args.q0star_min,
-        "q0star_max": args.q0star_max,
-        "n_points":   args.n_points
-    })
+    params.update(
+        {
+            "q0star_min": args.q0star_min,
+            "q0star_max": args.q0star_max,
+            "n_points": args.n_points,
+        }
+    )
     if args.export_heatmap:
         if args.param2_min is None or args.param2_max is None:
             sys.exit("❌ --param2_min & --param2_max required with --export-heatmap")
-        params.update({
-            "param2_min": args.param2_min,
-            "param2_max": args.param2_max,
-            "n_param2":   args.n_param2
-        })
+        params.update(
+            {
+                "param2_min": args.param2_min,
+                "param2_max": args.param2_max,
+                "n_param2": args.n_param2,
+            }
+        )
     return params

+
 def save_params(path: Path, params: dict):
     out = {
-        "thresholds":          params["thresholds"],
+        "thresholds": params["thresholds"],
         "max_epsilon_primary": params["max_epsilon_primary"],
-        "max_epsilon_order2":  params["max_epsilon_order2"]
+        "max_epsilon_order2": params["max_epsilon_order2"],
     }
     if "param2_min" in params:
-        out.update({
-            "param2_min": params["param2_min"],
-            "param2_max": params["param2_max"],
-            "n_param2":   params["n_param2"]
-        })
+        out.update(
+            {
+                "param2_min": params["param2_min"],
+                "param2_max": params["param2_max"],
+                "n_param2": params["n_param2"],
+            }
+        )
     path.write_text(json.dumps(out, indent=2), encoding="utf-8")

+
 def build_grid(xmin, xmax, n):
     return np.linspace(xmin, xmax, num=n)

+
 def main():
     # Prepare directories (translated names)
     DATA_DIR = ROOT / "zz-data" / "chapter08"
-    FIG_DIR  = ROOT / "zz-figures" / "chapter08"
+    FIG_DIR = ROOT / "zz-figures" / "chapter08"
     DATA_DIR.mkdir(parents=True, exist_ok=True)
     FIG_DIR.mkdir(parents=True, exist_ok=True)

-    args        = parse_args()
+    args = parse_args()
     params_file = DATA_DIR / "08_params_coupling.json"
-    params      = load_or_init_params(params_file, args)
+    params = load_or_init_params(params_file, args)

     # Load observed data (filenames translated)
-    bao    = pd.read_csv(DATA_DIR / "08_bao_data.csv",    encoding="utf-8")
-    pant   = pd.read_csv(DATA_DIR / "08_pantheon_data.csv",encoding="utf-8")
-    jalons = pd.read_csv(DATA_DIR / "08_coupling_milestones.csv",encoding="utf-8")
+    bao = pd.read_csv(DATA_DIR / "08_bao_data.csv", encoding="utf-8")
+    pant = pd.read_csv(DATA_DIR / "08_pantheon_data.csv", encoding="utf-8")
+    jalons = pd.read_csv(DATA_DIR / "08_coupling_milestones.csv", encoding="utf-8")

     # Cleaning
-    bao  = bao[bao.z>0].drop_duplicates("z").sort_values("z")
-    pant = pant[pant.z>0].drop_duplicates("z").sort_values("z")
+    bao = bao[bao.z > 0].drop_duplicates("z").sort_values("z")
+    pant = pant[pant.z > 0].drop_duplicates("z").sort_values("z")

     # Physical filter on q0⋆
-    zs    = np.unique(np.concatenate([bao.z.values, pant.z.values]))
-    bound = - (Omega_m0*(1+zs)**3 + Omega_lambda0) / (1+zs)**2
+    zs = np.unique(np.concatenate([bao.z.values, pant.z.values]))
+    bound = -(Omega_m0 * (1 + zs) ** 3 + Omega_lambda0) / (1 + zs) ** 2
     q_phys_min = bound.max()
     print(f">>> Physical domain : q0⋆ ≥ {q_phys_min:.4f}")

     # q0⋆ grid
     q0 = build_grid(params["q0star_min"], params["q0star_max"], params["n_points"])
-    q0 = q0[q0>=q_phys_min]
+    q0 = q0[q0 >= q_phys_min]
     print(f">>> q0_grid filtered : {q0[0]:.3f} → {q0[-1]:.3f} ({len(q0)} pts)")

     # 1D χ² scan
@@ -119,45 +152,46 @@ def main():
     for q in q0:
         dv = np.array([DV(z, q) for z in bao.z])
         mu = np.array([distance_modulus(z, q) for z in pant.z])
-        cb = ((dv - bao.DV_obs)/bao.sigma_DV)**2
-        cs = ((mu - pant.mu_obs)/pant.sigma_mu)**2
+        cb = ((dv - bao.DV_obs) / bao.sigma_DV) ** 2
+        cs = ((mu - pant.mu_obs) / pant.sigma_mu) ** 2
         chi2.append(cb.sum() + cs.sum())
     chi2 = np.array(chi2)

     # optimal q0⋆
-    iopt  = np.argmin(chi2)
+    iopt = np.argmin(chi2)
     qbest = q0[iopt]
     print(f">>> optimal q0⋆ = {qbest:.4f}")

     # Export DV_th(z) (translated filename)
     zbao = bao.z.values
-    dvb  = np.array([DV(z, qbest) for z in zbao])
-    pd.DataFrame({"z": zbao, "DV_calc": dvb})\
-      .to_csv(DATA_DIR / "08_dv_theory_z.csv", index=False)
+    dvb = np.array([DV(z, qbest) for z in zbao])
+    pd.DataFrame({"z": zbao, "DV_calc": dvb}).to_csv(
+        DATA_DIR / "08_dv_theory_z.csv", index=False
+    )
     print(">>> Exported 08_dv_theory_z.csv")

     # Export mu_th(z) (translated filename)
-    zsn  = pant.z.values
-    mub  = np.array([distance_modulus(z, qbest) for z in zsn])
-    pd.DataFrame({"z": zsn, "mu_calc": mub})\
-      .to_csv(DATA_DIR / "08_mu_theory_z.csv", index=False)
+    zsn = pant.z.values
+    mub = np.array([distance_modulus(z, qbest) for z in zsn])
+    pd.DataFrame({"z": zsn, "mu_calc": mub}).to_csv(
+        DATA_DIR / "08_mu_theory_z.csv", index=False
+    )
     print(">>> Exported 08_mu_theory_z.csv")

     # Export 1D scan
-    pd.DataFrame({
-        "q0star":     q0,
-        "chi2_total": chi2,
-        "chi2_err":   0.10*chi2
-    }).to_csv(DATA_DIR / "08_chi2_total_vs_q0.csv", index=False)
+    pd.DataFrame({"q0star": q0, "chi2_total": chi2, "chi2_err": 0.10 * chi2}).to_csv(
+        DATA_DIR / "08_chi2_total_vs_q0.csv", index=False
+    )
     print(">>> Exported 08_chi2_total_vs_q0.csv")

     # Smoothed derivative
     if args.export_derivative:
         d1 = np.gradient(chi2, q0)
-        w  = min(7, 2*(len(d1)//2)+1)
+        w = min(7, 2 * (len(d1) // 2) + 1)
         ds = savgol_filter(d1, w, polyorder=3, mode="interp")
-        pd.DataFrame({"q0star": q0, "dchi2_smooth": ds})\
-          .to_csv(DATA_DIR / "08_dchi2_dq0.csv", index=False)
+        pd.DataFrame({"q0star": q0, "dchi2_smooth": ds}).to_csv(
+            DATA_DIR / "08_dchi2_dq0.csv", index=False
+        )
         print(">>> Exported 08_dchi2_dq0.csv")

     # 2D χ² scan (heatmap)
@@ -169,11 +203,12 @@ def main():
                 # val is not used in DV/μ
                 dv = np.array([DV(z, q) for z in bao.z])
                 mu = np.array([distance_modulus(z, q) for z in pant.z])
-                cb = ((dv - bao.DV_obs)/bao.sigma_DV)**2
-                cs = ((mu - pant.mu_obs)/pant.sigma_mu)**2
-                rows.append({"q0star": q, "param2": val, "chi2": float(cb.sum() + cs.sum())})
-        pd.DataFrame(rows)\
-          .to_csv(DATA_DIR / "08_chi2_scan2D.csv", index=False)
+                cb = ((dv - bao.DV_obs) / bao.sigma_DV) ** 2
+                cs = ((mu - pant.mu_obs) / pant.sigma_mu) ** 2
+                rows.append(
+                    {"q0star": q, "param2": val, "chi2": float(cb.sum() + cs.sum())}
+                )
+        pd.DataFrame(rows).to_csv(DATA_DIR / "08_chi2_scan2D.csv", index=False)
         print(">>> Exported 08_chi2_scan2D.csv")

     # Epsilons ε
@@ -182,11 +217,16 @@ def main():
         pred = DV(r.z, 0.0) if r.jalon.startswith("BAO") else distance_modulus(r.z, 0.0)
         eps.append(abs(pred - r.obs) / r.obs)
     jalons["epsilon"] = eps
-    params["max_epsilon_primary"] = float(jalons.query("classe=='primaire'")["epsilon"].max())
-    params["max_epsilon_order2"]  = float(jalons.query("classe=='ordre2'")["epsilon"].max())
+    params["max_epsilon_primary"] = float(
+        jalons.query("classe=='primaire'")["epsilon"].max()
+    )
+    params["max_epsilon_order2"] = float(
+        jalons.query("classe=='ordre2'")["epsilon"].max()
+    )
     save_params(params_file, params)

     print("✅ Chapter 8 data generated successfully")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter08/plot_fig01_chi2_total_vs_q0.py b/zz-scripts/chapter08/plot_fig01_chi2_total_vs_q0.py
index a9ce7ab..10ad9f5 100755
--- a/zz-scripts/chapter08/plot_fig01_chi2_total_vs_q0.py
+++ b/zz-scripts/chapter08/plot_fig01_chi2_total_vs_q0.py
@@ -10,16 +10,17 @@ from pathlib import Path
 from matplotlib.ticker import MaxNLocator
 from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset

+
 def main():
     # --- Répertoires ---
-    ROOT     = Path(__file__).resolve().parents[2]
-    DATA_DIR = ROOT / "zz-data"  / "chapter08"
-    FIG_DIR  = ROOT / "zz-figures"  / "chapter08"
+    ROOT = Path(__file__).resolve().parents[2]
+    DATA_DIR = ROOT / "zz-data" / "chapter08"
+    FIG_DIR = ROOT / "zz-figures" / "chapter08"
     FIG_DIR.mkdir(parents=True, exist_ok=True)

     # --- Chargement des données ---
-    df   = pd.read_csv(DATA_DIR / "08_chi2_total_vs_q0.csv", encoding="utf-8")
-    q0   = df["q0star"].to_numpy()
+    df = pd.read_csv(DATA_DIR / "08_chi2_total_vs_q0.csv", encoding="utf-8")
+    q0 = df["q0star"].to_numpy()
     chi2 = df["chi2_total"].to_numpy()
     if "dchi2_smooth" in df:
         dchi2 = df["dchi2_smooth"].to_numpy()
@@ -27,7 +28,7 @@ def main():
         dchi2 = np.gradient(chi2, q0)

     # --- Figure principale ---
-    fig, ax = plt.subplots(figsize=(8,5), dpi=100)
+    fig, ax = plt.subplots(figsize=(8, 5), dpi=100)
     ax.plot(q0, chi2, color="C0", lw=2, label=r"$\chi^2$")
     ax.set_xlabel(r"$q_0^\star$", fontsize=14)
     ax.set_ylabel(r"$\chi^2$", fontsize=14, color="C0")
@@ -53,31 +54,40 @@ def main():
     ax.set_ylim(cmin - ypad, cmax + ypad)

     # --- Minimum et annotation ---
-    idx_min    = np.argmin(chi2)
-    q0_min     = q0[idx_min]
-    chi2_min   = chi2[idx_min]
+    idx_min = np.argmin(chi2)
+    q0_min = q0[idx_min]
+    chi2_min = chi2[idx_min]
     ax.plot(q0_min, chi2_min, "o", color="k", markersize=6)
     ax.annotate(
         f"Min χ² = {chi2_min:.1f}\n$q_0^* = {q0_min:.3f}$",
         xy=(q0_min, chi2_min),
-        xytext=(0, 30), textcoords="offset points",
-        ha="center", va="bottom",
+        xytext=(0, 30),
+        textcoords="offset points",
+        ha="center",
+        va="bottom",
         fontsize=12,
-        arrowprops=dict(arrowstyle="->", lw=1.0, color="k",
-                        shrinkA=0, shrinkB=2, connectionstyle="angle3")
+        arrowprops=dict(
+            arrowstyle="->",
+            lw=1.0,
+            color="k",
+            shrinkA=0,
+            shrinkB=2,
+            connectionstyle="angle3",
+        ),
     )

     # --- Inset légèrement vers le haut ---
     # width="50%", height="5%" du parent, bbox_to_anchor au milieu-haut
     axins = inset_axes(
         ax,
-        "80%", "80%",
+        "80%",
+        "80%",
         loc="upper left",
         bbox_to_anchor=(0.5, 0.6, 0.3, 0.3),
-        bbox_transform=ax.transAxes
+        bbox_transform=ax.transAxes,
     )
     lo, hi = q0_min - 0.1, q0_min + 0.1
-    mask   = (q0 >= lo) & (q0 <= hi)
+    mask = (q0 >= lo) & (q0 <= hi)
     axins.plot(q0[mask], chi2[mask], color="C0", lw=1.5)
     c2min, c2max = chi2[mask].min(), chi2[mask].max()
     cpad = 0.1 * (c2max - c2min)
@@ -92,13 +102,13 @@ def main():
     # --- Légende en haut à droite ---
     h1, l1 = ax.get_legend_handles_labels()
     h2, l2 = ax2.get_legend_handles_labels()
-    ax.legend(h1 + h2, l1 + l2,
-              loc="upper right", fontsize=12, frameon=False)
+    ax.legend(h1 + h2, l1 + l2, loc="upper right", fontsize=12, frameon=False)

     plt.tight_layout()
     outpath = FIG_DIR / "fig_01_chi2_total_vs_q0.png"
     plt.savefig(outpath, dpi=300)
     print(f"✅ Figure enregistrée → {outpath}")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter08/plot_fig02_dv_vs_z.py b/zz-scripts/chapter08/plot_fig02_dv_vs_z.py
index e53a272..a8a4457 100755
--- a/zz-scripts/chapter08/plot_fig02_dv_vs_z.py
+++ b/zz-scripts/chapter08/plot_fig02_dv_vs_z.py
@@ -12,21 +12,22 @@ from pathlib import Path
 import pandas as pd
 import matplotlib.pyplot as plt

+
 def main():
     # --- Directories (translated to English names) ---
-    ROOT     = Path(__file__).resolve().parents[2]
+    ROOT = Path(__file__).resolve().parents[2]
     DATA_DIR = ROOT / "zz-data" / "chapter08"
-    FIG_DIR  = ROOT / "zz-figures" / "chapter08"
+    FIG_DIR = ROOT / "zz-figures" / "chapter08"
     FIG_DIR.mkdir(parents=True, exist_ok=True)

     # --- Load BAO observations, theoretical curve and χ² scan ---
     # Filenames translated to English:
-    # 08_bao_data.csv
-    # 08_dv_theory_z.csv
-    # 08_chi2_total_vs_q0.csv
-    bao    = pd.read_csv(DATA_DIR / "08_bao_data.csv",    encoding="utf-8")
-    theo   = pd.read_csv(DATA_DIR / "08_dv_theory_z.csv",  encoding="utf-8")
-    chi2   = pd.read_csv(DATA_DIR / "08_chi2_total_vs_q0.csv", encoding="utf-8")
+    # 08_bao_data.csv
+    # 08_dv_theory_z.csv
+    # 08_chi2_total_vs_q0.csv
+    bao = pd.read_csv(DATA_DIR / "08_bao_data.csv", encoding="utf-8")
+    theo = pd.read_csv(DATA_DIR / "08_dv_theory_z.csv", encoding="utf-8")
+    chi2 = pd.read_csv(DATA_DIR / "08_chi2_total_vs_q0.csv", encoding="utf-8")

     # --- Extract optimal q0* ---
     params_path = DATA_DIR / "08_params_coupling.json"  # (was 08_params_couplage.json)
@@ -36,24 +37,31 @@ def main():
         q0star = params.get("q0star")
     if q0star is None:
         idx_best = chi2["chi2_total"].idxmin()
-        q0star   = float(chi2.loc[idx_best, "q0star"])
+        q0star = float(chi2.loc[idx_best, "q0star"])

     # --- Plot ---
     fig, ax = plt.subplots(figsize=(8, 5))

     # 1) BAO observations with error bars
     ax.errorbar(
-        bao["z"], bao["DV_obs"],
+        bao["z"],
+        bao["DV_obs"],
         yerr=bao["sigma_DV"],
-        fmt="o", capsize=4, mec="k", mfc="C0", ms=6,
-        label="BAO observations"
+        fmt="o",
+        capsize=4,
+        mec="k",
+        mfc="C0",
+        ms=6,
+        label="BAO observations",
     )

     # 2) Theoretical curve for optimal q0*
     ax.plot(
-        theo["z"], theo["DV_calc"],
-        linewidth=2.0, color="C1",
-        label=rf"$D_V^{{\rm th}}(z;\,q_0^*)\,,\;q_0^*={q0star:.3f}$"
+        theo["z"],
+        theo["DV_calc"],
+        linewidth=2.0,
+        color="C1",
+        label=rf"$D_V^{{\rm th}}(z;\,q_0^*)\,,\;q_0^*={q0star:.3f}$",
     )

     # --- Formatting ---
@@ -64,10 +72,7 @@ def main():
     ax.grid(which="both", linestyle="--", linewidth=0.5, alpha=0.7)

     # Legend bottom-right inside the plot
-    ax.legend(
-        loc="lower right",
-        frameon=False
-    )
+    ax.legend(loc="lower right", frameon=False)

     plt.tight_layout()

@@ -76,5 +81,6 @@ def main():
     plt.savefig(out_file, dpi=300)
     print(f"✅ Figure saved : {out_file}")

+
 if __name__ == "__main__":
-    main()
\ No newline at end of file
+    main()
diff --git a/zz-scripts/chapter08/plot_fig03_mu_vs_z.py b/zz-scripts/chapter08/plot_fig03_mu_vs_z.py
index 827ccee..4bf0ee8 100755
--- a/zz-scripts/chapter08/plot_fig03_mu_vs_z.py
+++ b/zz-scripts/chapter08/plot_fig03_mu_vs_z.py
@@ -11,20 +11,20 @@ import pandas as pd
 import matplotlib.pyplot as plt

 # -- Chemins
-ROOT     = Path(__file__).resolve().parents[2]
-DATA_DIR = ROOT / "zz-data"   / "chapter08"
-FIG_DIR  = ROOT / "zz-figures"   / "chapter08"
+ROOT = Path(__file__).resolve().parents[2]
+DATA_DIR = ROOT / "zz-data" / "chapter08"
+FIG_DIR = ROOT / "zz-figures" / "chapter08"
 FIG_DIR.mkdir(parents=True, exist_ok=True)

 # -- Chargement des données
 pantheon = pd.read_csv(DATA_DIR / "08_pantheon_data.csv", encoding="utf-8")
-theory   = pd.read_csv(DATA_DIR / "08_mu_theory_z.csv",    encoding="utf-8")
-params   = json.loads((DATA_DIR / "08_coupling_params.json").read_text(encoding="utf-8"))
-q0star   = params.get("q0star_optimal", None)  # ou autre clé selon ton JSON
+theory = pd.read_csv(DATA_DIR / "08_mu_theory_z.csv", encoding="utf-8")
+params = json.loads((DATA_DIR / "08_coupling_params.json").read_text(encoding="utf-8"))
+q0star = params.get("q0star_optimal", None)  # ou autre clé selon ton JSON

 # -- Tri par redshift
 pantheon = pantheon.sort_values("z")
-theory   = theory.sort_values("z")
+theory = theory.sort_values("z")

 # -- Configuration du tracé
 plt.rcParams.update({"font.size": 11})
@@ -35,18 +35,19 @@ ax.errorbar(
     pantheon["z"],
     pantheon["mu_obs"],
     yerr=pantheon["sigma_mu"],
-    fmt="o", markersize=5, capsize=3, label="Pantheon + obs"
+    fmt="o",
+    markersize=5,
+    capsize=3,
+    label="Pantheon + obs",
 )

 # -- Courbe théorique
-label_th = r"$\mu^{\rm th}(z; q_0^*={:.3f})$".format(q0star) \
-    if q0star is not None else r"$\mu^{\rm th}(z)$"
-ax.semilogx(
-    theory["z"],
-    theory["mu_calc"],
-    "-", lw=2,
-    label=label_th
+label_th = (
+    r"$\mu^{\rm th}(z; q_0^*={:.3f})$".format(q0star)
+    if q0star is not None
+    else r"$\mu^{\rm th}(z)$"
 )
+ax.semilogx(theory["z"], theory["mu_calc"], "-", lw=2, label=label_th)

 # -- Labels & titre
 ax.set_xlabel("Redshift $z$")
diff --git a/zz-scripts/chapter08/plot_fig04_chi2_heatmap.py b/zz-scripts/chapter08/plot_fig04_chi2_heatmap.py
index 1ba7ea6..20d4689 100755
--- a/zz-scripts/chapter08/plot_fig04_chi2_heatmap.py
+++ b/zz-scripts/chapter08/plot_fig04_chi2_heatmap.py
@@ -12,9 +12,9 @@ from matplotlib.colors import LogNorm
 from pathlib import Path

 # --- chemins ---
-ROOT     = Path(__file__).resolve().parents[2]
-DATA_DIR = ROOT / "zz-data"  / "chapter08"
-FIG_DIR  = ROOT / "zz-figures"  / "chapter08"
+ROOT = Path(__file__).resolve().parents[2]
+DATA_DIR = ROOT / "zz-data" / "chapter08"
+FIG_DIR = ROOT / "zz-figures" / "chapter08"
 FIG_DIR.mkdir(parents=True, exist_ok=True)

 # --- importer le scan 2D ---
@@ -28,24 +28,19 @@ p1 = np.sort(df["q0star"].unique())
 p2 = np.sort(df["param2"].unique())

 # pivoter en matrice
-M = (
-    df
-    .pivot(index="param2", columns="q0star", values="chi2")
-    .loc[p2, p1]
-    .values
-)
+M = df.pivot(index="param2", columns="q0star", values="chi2").loc[p2, p1].values

 # calculer les bords pour pcolormesh
 dp1 = np.diff(p1).mean()
 dp2 = np.diff(p2).mean()
-x_edges = np.concatenate([p1 - dp1/2, [p1[-1] + dp1/2]])
-y_edges = np.concatenate([p2 - dp2/2, [p2[-1] + dp2/2]])
+x_edges = np.concatenate([p1 - dp1 / 2, [p1[-1] + dp1 / 2]])
+y_edges = np.concatenate([p2 - dp2 / 2, [p2[-1] + dp2 / 2]])

 # trouver le minimum global
 i_min, j_min = np.unravel_index(np.argmin(M), M.shape)
-q0_min       = p1[j_min]
-p2_min       = p2[i_min]
-chi2_min     = M[i_min, j_min]
+q0_min = p1[j_min]
+p2_min = p2[i_min]
+chi2_min = M[i_min, j_min]

 # tracer
 plt.rcParams.update({"font.size": 12})
@@ -53,38 +48,36 @@ fig, ax = plt.subplots(figsize=(7, 5))

 # heatmap en lognorm pour renforcer le contraste
 pcm = ax.pcolormesh(
-    x_edges, y_edges, M,
+    x_edges,
+    y_edges,
+    M,
     norm=LogNorm(vmin=M.min(), vmax=M.max()),
     cmap="viridis",
-    shading="auto"
+    shading="auto",
 )

 # contours de confiance Δχ² = 2.30, 6.17, 11.8 (68%, 95%, 99.7% pour 2 paramètres)
 levels = chi2_min + np.array([2.30, 6.17, 11.8])
 cont = ax.contour(
-    p1, p2, M,
+    p1,
+    p2,
+    M,
     levels=levels,
     colors="white",
-    linestyles=["-","--",":"],
-    linewidths=1.2
+    linestyles=["-", "--", ":"],
+    linewidths=1.2,
+)
+ax.clabel(
+    cont, fmt={lvl: f"{int(lvl-chi2_min)}" for lvl in levels}, inline=True, fontsize=10
 )
-ax.clabel(cont, fmt={lvl:f"{int(lvl-chi2_min)}" for lvl in levels}, inline=True, fontsize=10)

 # point du minimum
 ax.plot(q0_min, p2_min, "o", color="black", ms=6)

 # annotation du minimum
 bbox = dict(boxstyle="round,pad=0.4", fc="white", ec="gray", alpha=0.8)
-txt = (
-    f"min χ² = {chi2_min:.1f}\n"
-    f"q₀⋆ = {q0_min:.3f}, p₂ = {p2_min:.3f}"
-)
-ax.text(
-    0.98, 0.95, txt,
-    transform=ax.transAxes,
-    va="top", ha="right",
-    bbox=bbox
-)
+txt = f"min χ² = {chi2_min:.1f}\n" f"q₀⋆ = {q0_min:.3f}, p₂ = {p2_min:.3f}"
+ax.text(0.98, 0.95, txt, transform=ax.transAxes, va="top", ha="right", bbox=bbox)

 # axes et titre
 ax.set_xlabel(r"$q_0^\star$")
diff --git a/zz-scripts/chapter08/plot_fig05_residuals.py b/zz-scripts/chapter08/plot_fig05_residuals.py
index 3baa878..ab421ac 100755
--- a/zz-scripts/chapter08/plot_fig05_residuals.py
+++ b/zz-scripts/chapter08/plot_fig05_residuals.py
@@ -3,7 +3,7 @@
 """
 zz-scripts/chapter08/plot_fig05_residuals.py

-Trace les résidus BAO et Pantheon+ :
+Trace les résidus BAO et Pantheon+ :
   (a) ΔD_V = D_V^obs - D_V^th  avec barres d'erreur σ_DV
   (b) Δμ   = μ^obs   - μ^th    avec barres d'erreur σ_μ

@@ -11,30 +11,30 @@ Trace les résidus BAO et Pantheon+ :
 """

 import pandas as pd
-import numpy as np
 import matplotlib.pyplot as plt
 from pathlib import Path

 # --- Répertoires ---
-ROOT     = Path(__file__).resolve().parents[2]
-DATA_DIR = ROOT / "zz-data"  / "chapter08"
-FIG_DIR  = ROOT / "zz-figures" / "chapter08"
+ROOT = Path(__file__).resolve().parents[2]
+DATA_DIR = ROOT / "zz-data" / "chapter08"
+FIG_DIR = ROOT / "zz-figures" / "chapter08"
 FIG_DIR.mkdir(parents=True, exist_ok=True)

+
 def main():
     # --- Chargement des données BAO + théorique ---
-    bao    = pd.read_csv(DATA_DIR / "08_bao_data.csv",    encoding="utf-8")
-    dv_th  = pd.read_csv(DATA_DIR / "08_dv_theory_z.csv",   encoding="utf-8")
+    bao = pd.read_csv(DATA_DIR / "08_bao_data.csv", encoding="utf-8")
+    dv_th = pd.read_csv(DATA_DIR / "08_dv_theory_z.csv", encoding="utf-8")
     df_bao = pd.merge(bao, dv_th, on="z", how="inner")
     df_bao["dv_resid"] = df_bao["DV_obs"] - df_bao["DV_calc"]
-    df_bao["dv_err"]   = df_bao["sigma_DV"]
+    df_bao["dv_err"] = df_bao["sigma_DV"]

     # --- Chargement des données Pantheon+ + théorique ---
-    pant   = pd.read_csv(DATA_DIR / "08_pantheon_data.csv", encoding="utf-8")
-    mu_th  = pd.read_csv(DATA_DIR / "08_mu_theory_z.csv",      encoding="utf-8")
+    pant = pd.read_csv(DATA_DIR / "08_pantheon_data.csv", encoding="utf-8")
+    mu_th = pd.read_csv(DATA_DIR / "08_mu_theory_z.csv", encoding="utf-8")
     df_pant = pd.merge(pant, mu_th, on="z", how="inner")
     df_pant["mu_resid"] = df_pant["mu_obs"] - df_pant["mu_calc"]
-    df_pant["mu_err"]   = df_pant["sigma_mu"]
+    df_pant["mu_err"] = df_pant["sigma_mu"]

     # --- Calcul des dispersions σ ---
     dv_std = df_bao["dv_resid"].std()
@@ -46,40 +46,44 @@ def main():

     # (a) BAO
     ax1.errorbar(
-        df_bao["z"], df_bao["dv_resid"], yerr=df_bao["dv_err"],
-        fmt="o", ms=5, alpha=0.8, capsize=3,
-        label=r"$D_V^{\rm obs}-D_V^{\rm th}$"
+        df_bao["z"],
+        df_bao["dv_resid"],
+        yerr=df_bao["dv_err"],
+        fmt="o",
+        ms=5,
+        alpha=0.8,
+        capsize=3,
+        label=r"$D_V^{\rm obs}-D_V^{\rm th}$",
     )
     ax1.set_xscale("log")
     ax1.set_ylabel(r"$\Delta D_V\ \mathrm{[Mpc]}$")
     ax1.set_ylim(-50, 400)
-    ax1.axhline(0,          ls="--", color="black", lw=1)
-    ax1.axhline(dv_std,     ls=":",  color="gray",  lw=1, label=r"$\pm1\sigma$")
-    ax1.axhline(-dv_std,    ls=":",  color="gray",  lw=1)
-    ax1.text(
-        0.02, 0.90, "(a) BAO",
-        transform=ax1.transAxes, weight="bold"
-    )
+    ax1.axhline(0, ls="--", color="black", lw=1)
+    ax1.axhline(dv_std, ls=":", color="gray", lw=1, label=r"$\pm1\sigma$")
+    ax1.axhline(-dv_std, ls=":", color="gray", lw=1)
+    ax1.text(0.02, 0.90, "(a) BAO", transform=ax1.transAxes, weight="bold")
     ax1.legend(loc="upper right", framealpha=0.5)
     ax1.grid(which="both", ls=":", lw=0.5, alpha=0.6)

     # (b) Supernovae Pantheon+
     ax2.errorbar(
-        df_pant["z"], df_pant["mu_resid"], yerr=df_pant["mu_err"],
-        fmt="o", ms=4, alpha=0.4, capsize=2,
-        label=r"$\mu^{\rm obs}-\mu^{\rm th}$"
+        df_pant["z"],
+        df_pant["mu_resid"],
+        yerr=df_pant["mu_err"],
+        fmt="o",
+        ms=4,
+        alpha=0.4,
+        capsize=2,
+        label=r"$\mu^{\rm obs}-\mu^{\rm th}$",
     )
     ax2.set_xscale("log")
     ax2.set_ylabel(r"$\Delta \mu\ \mathrm{[mag]}$")
     ax2.set_xlabel("Redshift $z$")
     ax2.set_ylim(-1.0, 1.0)
-    ax2.axhline(0,          ls="--", color="black", lw=1)
-    ax2.axhline(mu_std,     ls=":",  color="gray",  lw=1)
-    ax2.axhline(-mu_std,    ls=":",  color="gray",  lw=1)
-    ax2.text(
-        0.02, 0.90, "(b) Supernovae",
-        transform=ax2.transAxes, weight="bold"
-    )
+    ax2.axhline(0, ls="--", color="black", lw=1)
+    ax2.axhline(mu_std, ls=":", color="gray", lw=1)
+    ax2.axhline(-mu_std, ls=":", color="gray", lw=1)
+    ax2.text(0.02, 0.90, "(b) Supernovae", transform=ax2.transAxes, weight="bold")
     ax2.legend(loc="upper right", framealpha=0.5)
     ax2.grid(which="both", ls=":", lw=0.5, alpha=0.6)

@@ -91,5 +95,6 @@ def main():
     fig.savefig(outpath, dpi=300)
     print(f"✅ {outpath.name} générée")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter08/plot_fig06_normalized_residuals_distribution.py b/zz-scripts/chapter08/plot_fig06_normalized_residuals_distribution.py
index 0bf22e0..c9248de 100755
--- a/zz-scripts/chapter08/plot_fig06_normalized_residuals_distribution.py
+++ b/zz-scripts/chapter08/plot_fig06_normalized_residuals_distribution.py
@@ -6,6 +6,7 @@ zz-scripts/chapter08/plot_fig06_normalized_residuals_distribution.py
 Distribution des pulls (résidus normalisés) pour BAO et Supernovae.
 Rug‐plot + KDE pour BAO, histogramme pour Supernovae
 """
+
 import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt
@@ -14,42 +15,43 @@ from pathlib import Path
 import sys

 # --- pour importer cosmo.py depuis utils ---
-ROOT  = Path(__file__).resolve().parents[2]
+ROOT = Path(__file__).resolve().parents[2]
 UTILS = ROOT / "zz-scripts" / "chapter08" / "utils"
 sys.path.insert(0, str(UTILS))
 from cosmo import DV, distance_modulus

+
 def main():
     # Répertoires
     DATA_DIR = ROOT / "zz-data" / "chapter08"
-    FIG_DIR  = ROOT / "zz-figures" / "chapter08"
+    FIG_DIR = ROOT / "zz-figures" / "chapter08"
     FIG_DIR.mkdir(parents=True, exist_ok=True)

     # Lecture des données
-    bao  = pd.read_csv(DATA_DIR / "08_bao_data.csv",    encoding="utf-8")
-    pant = pd.read_csv(DATA_DIR / "08_pantheon_data.csv",encoding="utf-8")
-    df1d = pd.read_csv(DATA_DIR / "08_chi2_total_vs_q0.csv",encoding="utf-8")
+    bao = pd.read_csv(DATA_DIR / "08_bao_data.csv", encoding="utf-8")
+    pant = pd.read_csv(DATA_DIR / "08_pantheon_data.csv", encoding="utf-8")
+    df1d = pd.read_csv(DATA_DIR / "08_chi2_total_vs_q0.csv", encoding="utf-8")

     # q0* optimal
     q0_star = df1d.loc[df1d["chi2_total"].idxmin(), "q0star"]

     # Calcul des pulls BAO
-    z_bao    = bao["z"].values
-    dv_obs   = bao["DV_obs"].values
-    dv_sig   = bao["sigma_DV"].values
-    dv_th    = np.array([DV(z, q0_star) for z in z_bao])
+    z_bao = bao["z"].values
+    dv_obs = bao["DV_obs"].values
+    dv_sig = bao["sigma_DV"].values
+    dv_th = np.array([DV(z, q0_star) for z in z_bao])
     pulls_bao = (dv_obs - dv_th) / dv_sig

     # Calcul des pulls Supernovae
-    z_sn     = pant["z"].values
-    mu_obs   = pant["mu_obs"].values
-    mu_sig   = pant["sigma_mu"].values
-    mu_th    = np.array([distance_modulus(z, q0_star) for z in z_sn])
+    z_sn = pant["z"].values
+    mu_obs = pant["mu_obs"].values
+    mu_sig = pant["sigma_mu"].values
+    mu_th = np.array([distance_modulus(z, q0_star) for z in z_sn])
     pulls_sn = (mu_obs - mu_th) / mu_sig

     # Statistiques
     mu_bao, sigma_bao, N_bao = pulls_bao.mean(), pulls_bao.std(ddof=1), len(pulls_bao)
-    mu_sn,  sigma_sn,  N_sn  = pulls_sn.mean(),  pulls_sn.std(ddof=1),  len(pulls_sn)
+    mu_sn, sigma_sn, N_sn = pulls_sn.mean(), pulls_sn.std(ddof=1), len(pulls_sn)

     # Plot
     plt.rcParams.update({"font.size": 11})
@@ -57,16 +59,21 @@ def main():

     # (a) BAO – rug + KDE
     ax = axes[0]
-    ax.plot(pulls_bao, np.zeros_like(pulls_bao), '|', ms=20, mew=2,
-            label="BAO pulls")
+    ax.plot(pulls_bao, np.zeros_like(pulls_bao), "|", ms=20, mew=2, label="BAO pulls")
     kde = gaussian_kde(pulls_bao)
     xk = np.linspace(pulls_bao.min() - 1, pulls_bao.max() + 1, 300)
-    ax.plot(xk, kde(xk), '-', lw=2, label="KDE")
+    ax.plot(xk, kde(xk), "-", lw=2, label="KDE")
     # Annotation μ,σ,N en haut-gauche
     txt_bao = rf"$\mu={mu_bao:.2f},\ \sigma={sigma_bao:.2f},\ N={N_bao}$"
-    ax.text(0.02, 0.95, txt_bao, transform=ax.transAxes,
-            va="top", ha="left",
-            bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="0.5"))
+    ax.text(
+        0.02,
+        0.95,
+        txt_bao,
+        transform=ax.transAxes,
+        va="top",
+        ha="left",
+        bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="0.5"),
+    )
     ax.set_title("(a) BAO")
     ax.set_xlabel("Pull")
     ax.set_ylabel("Densité")
@@ -77,15 +84,27 @@ def main():
     # (b) Supernovae – histogramme
     ax = axes[1]
     bins = np.linspace(-5, 5, 50)
-    ax.hist(pulls_sn, bins=bins, density=True,
-            histtype="stepfilled", alpha=0.8,
-            color="#FF8C00", label="SNe pulls")
+    ax.hist(
+        pulls_sn,
+        bins=bins,
+        density=True,
+        histtype="stepfilled",
+        alpha=0.8,
+        color="#FF8C00",
+        label="SNe pulls",
+    )
     x = np.linspace(-5, 5, 400)
-    ax.plot(x, norm.pdf(x, 0, 1), 'k--', lw=2, label=r"$\mathcal{N}(0,1)$")
+    ax.plot(x, norm.pdf(x, 0, 1), "k--", lw=2, label=r"$\mathcal{N}(0,1)$")
     txt_sn = rf"$\mu={mu_sn:.2f},\ \sigma={sigma_sn:.2f},\ N={N_sn}$"
-    ax.text(0.02, 0.95, txt_sn, transform=ax.transAxes,
-            va="top", ha="left",
-            bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="0.5"))
+    ax.text(
+        0.02,
+        0.95,
+        txt_sn,
+        transform=ax.transAxes,
+        va="top",
+        ha="left",
+        bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="0.5"),
+    )
     ax.set_title("(b) Supernovae")
     ax.set_xlabel("Pull")
     ax.set_ylim(0, 0.9)
@@ -99,5 +118,6 @@ def main():
     fig.savefig(out_path, dpi=300, bbox_inches="tight")
     print(f"✅ {out_path.name} générée")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter08/plot_fig07_chi2_profile.py b/zz-scripts/chapter08/plot_fig07_chi2_profile.py
index 81a7624..2efd133 100755
--- a/zz-scripts/chapter08/plot_fig07_chi2_profile.py
+++ b/zz-scripts/chapter08/plot_fig07_chi2_profile.py
@@ -3,7 +3,7 @@
 """
 zz-scripts/chapter08/plot_fig07_chi2_profile.py

-Trace le profil Δχ² en fonction de q₀⋆ autour du minimum,
+Trace le profil Δχ² en fonction de q₀⋆ autour du minimum,
 avec annotations des niveaux 1σ, 2σ, 3σ (1 degré de liberté).
 """

@@ -11,11 +11,12 @@ import pandas as pd
 import matplotlib.pyplot as plt
 from pathlib import Path

+
 def main():
     # Répertoires
-    ROOT     = Path(__file__).resolve().parents[2]
+    ROOT = Path(__file__).resolve().parents[2]
     DATA_DIR = ROOT / "zz-data" / "chapter08"
-    FIG_DIR  = ROOT / "zz-figures" / "chapter08"
+    FIG_DIR = ROOT / "zz-figures" / "chapter08"
     FIG_DIR.mkdir(parents=True, exist_ok=True)

     # Chargement du scan 1D χ²
@@ -34,24 +35,34 @@ def main():
     fig, ax = plt.subplots(figsize=(6.5, 4.5))

     # Profil Δχ²
-    ax.plot(q0, delta_chi2, color="C0", lw=2,
-            label=r"$\Delta\chi^2(q_0^\star)$")
+    ax.plot(q0, delta_chi2, color="C0", lw=2, label=r"$\Delta\chi^2(q_0^\star)$")

     # Niveaux de confiance (1 dof)
     sigmas = [1.0, 4.0, 9.0]
     styles = ["--", "-.", ":"]
-    colors = ["C1"]*3
+    colors = ["C1"] * 3
     for lvl, ls in zip(sigmas, styles):
         ax.axhline(lvl, color="C1", linestyle=ls, lw=1.5)
         # annotation sur la ligne
-        ax.text(q0_best + 0.02, lvl + 0.2,
-                rf"${int(lvl**0.5)}\sigma$",
-                color="C1", va="bottom")
+        ax.text(
+            q0_best + 0.02,
+            lvl + 0.2,
+            rf"${int(lvl**0.5)}\sigma$",
+            color="C1",
+            va="bottom",
+        )

     # Best-fit point
-    ax.plot(q0_best, 0.0, "o",
-            mfc="white", mec="C0", mew=2, ms=8,
-            label=rf"$q_0^* = {q0_best:.3f}$")
+    ax.plot(
+        q0_best,
+        0.0,
+        "o",
+        mfc="white",
+        mec="C0",
+        mew=2,
+        ms=8,
+        label=rf"$q_0^* = {q0_best:.3f}$",
+    )

     # Zoom autour du minimum
     dx = 0.2
@@ -73,5 +84,6 @@ def main():
     fig.savefig(out, dpi=300)
     print(f"✅ {out.name} générée")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter08/utils/cosmo.py b/zz-scripts/chapter08/utils/cosmo.py
index 4c61ca6..7a8a6d9 100755
--- a/zz-scripts/chapter08/utils/cosmo.py
+++ b/zz-scripts/chapter08/utils/cosmo.py
@@ -5,56 +5,60 @@ import numpy as np
 from scipy.integrate import quad

 # Constantes cosmologiques de référence
-H0             = 67.66       # km/s/Mpc
-c_kms          = 299792.458  # km/s
-Omega_m0       = 0.3111
-Omega_lambda0  = 0.6889
+H0 = 67.66  # km/s/Mpc
+c_kms = 299792.458  # km/s
+Omega_m0 = 0.3111
+Omega_lambda0 = 0.6889

 # Valeurs de tolérance
-_EPS         = 1e-8
-_MAX_CHI2    = 1e8
-_INT_EPSABS  = 1e-8
-_INT_EPSREL  = 1e-8
+_EPS = 1e-8
+_MAX_CHI2 = 1e8
+_INT_EPSABS = 1e-8
+_INT_EPSREL = 1e-8
+

 def Hubble(z, q0star=0.0):
     """
     H(z) = H0 * sqrt( Omega_m*(1+z)^3 + q0*(1+z)^2 + Omega_lambda )
     Avec clamp pour garantir positivité.
     """
-    inside = Omega_m0*(1+z)**3 + q0star*(1+z)**2 + Omega_lambda0
+    inside = Omega_m0 * (1 + z) ** 3 + q0star * (1 + z) ** 2 + Omega_lambda0
     # Éviter underflow/overflow et négatif sous la racine
     inside = np.maximum(inside, _EPS)
     return H0 * np.sqrt(inside)

+
 def comoving_distance(z, q0star=0.0):
     """
     ∫_0^z [c / H(z')] dz' en Mpc, avec gestion d'erreur.
     """
     if z <= 0:
         return 0.0
+
     def integrand(zp):
         return c_kms / Hubble(zp, q0star)
+
     try:
         dc, err = quad(
-            integrand, 0.0, z,
-            epsabs=_INT_EPSABS,
-            epsrel=_INT_EPSREL,
-            limit=200
+            integrand, 0.0, z, epsabs=_INT_EPSABS, epsrel=_INT_EPSREL, limit=200
         )
         return dc
     except Exception:
         return _MAX_CHI2

+
 def lum_distance(z, q0star=0.0):
     dc = comoving_distance(z, q0star)
     return dc * (1 + z)

+
 def distance_modulus(z, q0star=0.0):
     dl = lum_distance(z, q0star)
     if not np.isfinite(dl) or dl <= 0:
-        return 5*np.log10(_MAX_CHI2) + 25
+        return 5 * np.log10(_MAX_CHI2) + 25
     return 5.0 * np.log10(dl) + 25.0

+
 def DV(z, q0star=0.0):
     """
     DV ≡ [ (1+z)^2 D_A^2 c z / H(z) ]^(1/3), avec D_A = DC/(1+z).
@@ -66,9 +70,10 @@ def DV(z, q0star=0.0):
     if not np.isfinite(dc) or not np.isfinite(Hz) or Hz <= 0:
         return _MAX_CHI2
     DA = dc / (1 + z)
-    factor = (1+z)**2 * DA**2 * (c_kms * z / Hz)
+    factor = (1 + z) ** 2 * DA**2 * (c_kms * z / Hz)
     factor = np.maximum(factor, _EPS)
-    return factor**(1/3)
+    return factor ** (1 / 3)
+

 # Self-test rapide
 if __name__ == "__main__":
diff --git a/zz-scripts/chapter08/utils/coupling_example_model.py b/zz-scripts/chapter08/utils/coupling_example_model.py
index b56ebbb..1d1d44f 100755
--- a/zz-scripts/chapter08/utils/coupling_example_model.py
+++ b/zz-scripts/chapter08/utils/coupling_example_model.py
@@ -3,33 +3,32 @@
 # Génère un toy-model pour tester l’interpolation PCHIP en log-log

 import numpy as np
-import pandas as pd
 import matplotlib.pyplot as plt
 from scipy.interpolate import PchipInterpolator
 from pathlib import Path
 import os

 # 1. Points de référence (coarse grid)
-z_ref = np.logspace(-2, 0, 6)    # de 0.01 à 1.0
-y_ref = z_ref**1.5               # toy-fonction y = z^1.5
+z_ref = np.logspace(-2, 0, 6)  # de 0.01 à 1.0
+y_ref = z_ref**1.5  # toy-fonction y = z^1.5

 # 2. Grille fine pour interpolation
 z_fine = np.logspace(np.log10(z_ref.min()), np.log10(z_ref.max()), 200)

 # 3. Constructeur PCHIP log-log
 interp = PchipInterpolator(np.log10(z_ref), np.log10(y_ref), extrapolate=True)
-y_interp = 10**interp(np.log10(z_fine))
+y_interp = 10 ** interp(np.log10(z_fine))

 # 4. Préparation du dossier de sortie
-ROOT    = Path(__file__).resolve().parents[2]
+ROOT = Path(__file__).resolve().parents[2]
 FIG_DIR = ROOT / "zz-figures" / "chapter08"
 os.makedirs(FIG_DIR, exist_ok=True)
 out_png = FIG_DIR / "fig_00_toy_model_coupling.png"

 # 5. Tracé
 plt.figure(figsize=(6.5, 4.5))
-plt.loglog(z_ref, y_ref, 'o', label="Points de référence")
-plt.loglog(z_fine, y_interp,  '-', label="Interpolation PCHIP")
+plt.loglog(z_ref, y_ref, "o", label="Points de référence")
+plt.loglog(z_fine, y_interp, "-", label="Interpolation PCHIP")
 plt.xlabel("z")
 plt.ylabel("y = z^1.5")
 plt.title("Toy-model : test interpolation log–log")
diff --git a/zz-scripts/chapter08/utils/extract_bao_data.py b/zz-scripts/chapter08/utils/extract_bao_data.py
index 072662f..721a478 100755
--- a/zz-scripts/chapter08/utils/extract_bao_data.py
+++ b/zz-scripts/chapter08/utils/extract_bao_data.py
@@ -9,41 +9,36 @@ import pandas as pd

 # 1. Définition des chemins
 SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
-DATA_DIR   = os.path.abspath(os.path.join(SCRIPT_DIR, "../../../zz-data/chapter08"))
+DATA_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, "../../../zz-data/chapter08"))
 os.makedirs(DATA_DIR, exist_ok=True)

-input_file  = os.path.join(DATA_DIR, "bao_distances_DR12v5.dat")
+input_file = os.path.join(DATA_DIR, "bao_distances_DR12v5.dat")
 output_file = os.path.join(DATA_DIR, "08_bao_data.csv")

 # 2. Lecture du fichier brut BAO
-df = pd.read_csv(
-    input_file,
-    delim_whitespace=True,
-    comment="#"
-)
+df = pd.read_csv(input_file, delim_whitespace=True, comment="#")

 # 3. Sélection et renommage des colonnes
 #    - 'z'        : redshift
 #    - 'DV'       : distance de diffusion baryonique
 #    - 'sigma_DV' : incertitude absolue de DV
-df_out = df[['z', 'DV', 'sigma_DV']].rename(
-    columns={
-        'DV': 'DV_obs',
-        'sigma_DV': 'sigma_DV'
-    }
+df_out = df[["z", "DV", "sigma_DV"]].rename(
+    columns={"DV": "DV_obs", "sigma_DV": "sigma_DV"}
 )

+
 # 4. Classification des jalons (primaire / ordre2)
 def classify(row):
-    frac = row['sigma_DV'] / row['DV_obs'] if row['DV_obs'] != 0 else float('inf')
-    return 'primaire' if frac <= 0.01 else 'ordre2'
+    frac = row["sigma_DV"] / row["DV_obs"] if row["DV_obs"] != 0 else float("inf")
+    return "primaire" if frac <= 0.01 else "ordre2"
+

-df_out['classe'] = df_out.apply(classify, axis=1)
+df_out["classe"] = df_out.apply(classify, axis=1)

 # 5. (Optionnel) Filtrer la plage de redshift
 # df_out = df_out[(df_out['z'] >= 0.1) & (df_out['z'] <= 2.5)]

 # 6. Sauvegarde au format CSV UTF-8
-df_out.to_csv(output_file, index=False, encoding='utf-8')
+df_out.to_csv(output_file, index=False, encoding="utf-8")

 print(f"✅ 08_bao_data.csv généré dans : {output_file}")
diff --git a/zz-scripts/chapter08/utils/extract_pantheon_plus_data.py b/zz-scripts/chapter08/utils/extract_pantheon_plus_data.py
index bbd3a70..ff52949 100755
--- a/zz-scripts/chapter08/utils/extract_pantheon_plus_data.py
+++ b/zz-scripts/chapter08/utils/extract_pantheon_plus_data.py
@@ -8,33 +8,27 @@ import os
 import pandas as pd

 # Chemins relatifs depuis ce script
-DATA_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../zz-data/chapter08"))
+DATA_DIR = os.path.abspath(
+    os.path.join(os.path.dirname(__file__), "../../../zz-data/chapter08")
+)

 # Fichier brut Pantheon+SH0ES (sans caractères spéciaux dans le nom)
-input_file  = os.path.join(DATA_DIR, "pantheon_plus_sh0es.dat")
+input_file = os.path.join(DATA_DIR, "pantheon_plus_sh0es.dat")
 # Fichier CSV de sortie
 output_file = os.path.join(DATA_DIR, "08_pantheon_data.csv")

 # 1. Lecture du fichier brut
-df = pd.read_csv(
-    input_file,
-    delim_whitespace=True,
-    comment='#'
-)
+df = pd.read_csv(input_file, delim_whitespace=True, comment="#")

 # 2. Sélection et renommage des colonnes
-df_out = df[['zHD', 'MU_SH0ES', 'MU_SH0ES_ERR_DIAG']].rename(
-    columns={
-        'zHD':                'z',
-        'MU_SH0ES':           'mu_obs',
-        'MU_SH0ES_ERR_DIAG':  'sigma_mu'
-    }
+df_out = df[["zHD", "MU_SH0ES", "MU_SH0ES_ERR_DIAG"]].rename(
+    columns={"zHD": "z", "MU_SH0ES": "mu_obs", "MU_SH0ES_ERR_DIAG": "sigma_mu"}
 )

 # 3. Filtrer la plage 0 ≤ z ≤ 2.3
-df_out = df_out[(df_out['z'] >= 0) & (df_out['z'] <= 2.3)]
+df_out = df_out[(df_out["z"] >= 0) & (df_out["z"] <= 2.3)]

 # 4. Sauvegarde au format CSV UTF-8
-df_out.to_csv(output_file, index=False, encoding='utf-8')
+df_out.to_csv(output_file, index=False, encoding="utf-8")

 print(f"✅ 08_pantheon_data.csv généré dans : {output_file}")
diff --git a/zz-scripts/chapter08/utils/generate_coupling_milestones.py b/zz-scripts/chapter08/utils/generate_coupling_milestones.py
index 1cca8cf..cad1e01 100755
--- a/zz-scripts/chapter08/utils/generate_coupling_milestones.py
+++ b/zz-scripts/chapter08/utils/generate_coupling_milestones.py
@@ -8,30 +8,31 @@ DATA_DIR = os.path.abspath(os.path.join(BASE_DIR, "../../zz-data/chapter08"))

 # 1) Charger BAO depuis le CSV final
 df_bao = pd.read_csv(os.path.join(DATA_DIR, "08_bao_data.csv"))
-df_bao = df_bao.rename(columns={'DV_obs':'obs', 'sigma_DV':'sigma_obs'})
-df_bao['jalon']  = df_bao['z'].apply(lambda z: f"BAO_z={z:.3f}")
-df_bao['classe'] = df_bao.apply(
-    lambda row: 'primaire' if row.sigma_obs/row.obs <= 0.01 else 'ordre2',
-    axis=1
+df_bao = df_bao.rename(columns={"DV_obs": "obs", "sigma_DV": "sigma_obs"})
+df_bao["jalon"] = df_bao["z"].apply(lambda z: f"BAO_z={z:.3f}")
+df_bao["classe"] = df_bao.apply(
+    lambda row: "primaire" if row.sigma_obs / row.obs <= 0.01 else "ordre2", axis=1
 )

 # 2) Charger Pantheon+ depuis le CSV final
 df_sn = pd.read_csv(os.path.join(DATA_DIR, "08_pantheon_data.csv"))
-df_sn = df_sn.rename(columns={'mu_obs':'obs', 'sigma_mu':'sigma_obs'})
+df_sn = df_sn.rename(columns={"mu_obs": "obs", "sigma_mu": "sigma_obs"})
 # Création du libellé SN0, SN1, …
-df_sn['jalon']  = df_sn.index.map(lambda i: f"SN{i}")
-df_sn['classe'] = df_sn.apply(
-    lambda row: 'primaire' if row.sigma_obs/row.obs <= 0.01 else 'ordre2',
-    axis=1
+df_sn["jalon"] = df_sn.index.map(lambda i: f"SN{i}")
+df_sn["classe"] = df_sn.apply(
+    lambda row: "primaire" if row.sigma_obs / row.obs <= 0.01 else "ordre2", axis=1
 )

 # 3) Concaténer et ordonner les colonnes
-df_all = pd.concat([
-    df_bao[['jalon','z','obs','sigma_obs','classe']],
-    df_sn[['jalon','z','obs','sigma_obs','classe']]
-], ignore_index=True)
+df_all = pd.concat(
+    [
+        df_bao[["jalon", "z", "obs", "sigma_obs", "classe"]],
+        df_sn[["jalon", "z", "obs", "sigma_obs", "classe"]],
+    ],
+    ignore_index=True,
+)

 # 4) Exporter le CSV final
 out_csv = os.path.join(DATA_DIR, "08_coupling_milestones.csv")
-df_all.to_csv(out_csv, index=False, encoding='utf-8')
+df_all.to_csv(out_csv, index=False, encoding="utf-8")
 print(f"✅ 08_coupling_milestones.csv généré : {out_csv}")
diff --git a/zz-scripts/chapter09/apply_poly_unwrap_rebranch.py b/zz-scripts/chapter09/apply_poly_unwrap_rebranch.py
index 70f7f77..83c9729 100755
--- a/zz-scripts/chapter09/apply_poly_unwrap_rebranch.py
+++ b/zz-scripts/chapter09/apply_poly_unwrap_rebranch.py
@@ -43,7 +43,10 @@ python zz-scripts/chapter09/apply_poly_unwrap_rebranch.py \\

 from __future__ import annotations

-import argparse, json, logging, shutil
+import argparse
+import json
+import logging
+import shutil
 from pathlib import Path
 from typing import Tuple

@@ -54,34 +57,80 @@ import pandas as pd
 def setup_logger(level: str = "INFO") -> logging.Logger:
     logging.basicConfig(
         level=getattr(logging, level.upper(), logging.INFO),
-        format='[%(asctime)s] [%(levelname)s] %(message)s',
-        datefmt='%Y-%m-%d %H:%M:%S')
+        format="[%(asctime)s] [%(levelname)s] %(message)s",
+        datefmt="%Y-%m-%d %H:%M:%S",
+    )
     return logging.getLogger("apply_poly_unwrap_rebranch")


 def parse_args() -> argparse.Namespace:
-    ap = argparse.ArgumentParser(description="Correction polynomiale unwrap+rebranch de Δφ.")
+    ap = argparse.ArgumentParser(
+        description="Correction polynomiale unwrap+rebranch de Δφ."
+    )
     ap.add_argument("--csv", type=Path, required=True, help="CSV phases (écrit/écrasé)")
-    ap.add_argument("--meta", type=Path, default=Path("zz-data/chapter09/09_metrics_phase.json"),
-                    help="JSON méta (mis à jour ou créé)")
-    ap.add_argument("--degree", type=int, default=4, help="Degré du polynôme (défaut: 4)")
-    ap.add_argument("--fit-window", nargs=2, type=float, default=[30.0, 250.0], metavar=("F_LO","F_HI"))
-    ap.add_argument("--metrics-window", nargs=2, type=float, default=[20.0, 300.0], metavar=("M_LO","M_HI"))
-    ap.add_argument("--basis", choices=["log10","hz"], default="log10", help="Variable de fit: log10(f) ou f.")
-    ap.add_argument("--from-column", default="phi_mcgt_cal", help="Colonne source pour repartir (défaut: phi_mcgt_cal)")
-    ap.add_argument("--backup", action="store_true", help="Sauvegarde <csv> -> <csv>.backup avant écriture")
-    ap.add_argument("--dry-run", action="store_true", help="N’écrit pas, affiche seulement métriques")
-    ap.add_argument("--log-level", choices=["DEBUG","INFO","WARNING","ERROR"], default="INFO")
+    ap.add_argument(
+        "--meta",
+        type=Path,
+        default=Path("zz-data/chapter09/09_metrics_phase.json"),
+        help="JSON méta (mis à jour ou créé)",
+    )
+    ap.add_argument(
+        "--degree", type=int, default=4, help="Degré du polynôme (défaut: 4)"
+    )
+    ap.add_argument(
+        "--fit-window",
+        nargs=2,
+        type=float,
+        default=[30.0, 250.0],
+        metavar=("F_LO", "F_HI"),
+    )
+    ap.add_argument(
+        "--metrics-window",
+        nargs=2,
+        type=float,
+        default=[20.0, 300.0],
+        metavar=("M_LO", "M_HI"),
+    )
+    ap.add_argument(
+        "--basis",
+        choices=["log10", "hz"],
+        default="log10",
+        help="Variable de fit: log10(f) ou f.",
+    )
+    ap.add_argument(
+        "--from-column",
+        default="phi_mcgt_cal",
+        help="Colonne source pour repartir (défaut: phi_mcgt_cal)",
+    )
+    ap.add_argument(
+        "--backup",
+        action="store_true",
+        help="Sauvegarde <csv> -> <csv>.backup avant écriture",
+    )
+    ap.add_argument(
+        "--dry-run",
+        action="store_true",
+        help="N’écrit pas, affiche seulement métriques",
+    )
+    ap.add_argument(
+        "--log-level", choices=["DEBUG", "INFO", "WARNING", "ERROR"], default="INFO"
+    )
     return ap.parse_args()


-def compute_metrics(f: np.ndarray, phi_mcgt: np.ndarray, phi_ref: np.ndarray,
-                    lo: float, hi: float) -> Tuple[float,float,float,int]:
-    mask = (f>=lo) & (f<=hi) & np.isfinite(phi_mcgt) & np.isfinite(phi_ref)
+def compute_metrics(
+    f: np.ndarray, phi_mcgt: np.ndarray, phi_ref: np.ndarray, lo: float, hi: float
+) -> Tuple[float, float, float, int]:
+    mask = (f >= lo) & (f <= hi) & np.isfinite(phi_mcgt) & np.isfinite(phi_ref)
     if not np.any(mask):
         return float("nan"), float("nan"), float("nan"), 0
     d = np.abs(np.unwrap(phi_mcgt[mask] - phi_ref[mask]))
-    return float(np.nanmean(d)), float(np.nanpercentile(d,95)), float(np.nanmax(d)), int(np.sum(mask))
+    return (
+        float(np.nanmean(d)),
+        float(np.nanpercentile(d, 95)),
+        float(np.nanmax(d)),
+        int(np.sum(mask)),
+    )


 def main():
@@ -94,7 +143,9 @@ def main():
     df = pd.read_csv(args.csv)
     need = {"f_Hz", "phi_ref", args.from_column}
     if not need.issubset(df.columns):
-        raise SystemExit(f"Colonnes manquantes dans {args.csv} (requis: {sorted(need)})")
+        raise SystemExit(
+            f"Colonnes manquantes dans {args.csv} (requis: {sorted(need)})"
+        )

     # Repartir d’une base saine (pas d’empilement)
     df["phi_mcgt"] = df[args.from_column].to_numpy(float)
@@ -105,32 +156,51 @@ def main():
     ru = np.unwrap(r0)

     flo, fhi = map(float, args.fit_window)
-    mfit = (f>=flo) & (f<=fhi) & np.isfinite(x) & np.isfinite(ru)
+    mfit = (f >= flo) & (f <= fhi) & np.isfinite(x) & np.isfinite(ru)
     nfit = int(np.sum(mfit))
-    if nfit < max(8, args.degree+2):
-        raise SystemExit(f"Trop peu de points valides dans la fenêtre de fit {flo}-{fhi} Hz (n={nfit}).")
+    if nfit < max(8, args.degree + 2):
+        raise SystemExit(
+            f"Trop peu de points valides dans la fenêtre de fit {flo}-{fhi} Hz (n={nfit})."
+        )

     # Fit polynôme sur ru(x) dans la fenêtre
     c_desc = np.polyfit(x[mfit], ru[mfit], args.degree)
-    trend  = np.polyval(c_desc, x)
+    trend = np.polyval(c_desc, x)

     # Rebranchage sur la branche du résidu brut r0 dans la fenêtre de fit
-    two_pi = 2*np.pi
+    two_pi = 2 * np.pi
     k = int(np.round(np.median((trend[mfit] - r0[mfit]) / two_pi)))
-    trend_adj = trend - k*two_pi
+    trend_adj = trend - k * two_pi

     # Appliquer la correction (soustraction)
     phi_corr = df["phi_mcgt"].to_numpy(float) - trend_adj

     # Métriques @ metrics-window sur unwrap(Δφ)
     mlo, mhi = map(float, args.metrics_window)
-    mean_abs, p95_abs, max_abs, n = compute_metrics(f, phi_corr, df["phi_ref"].to_numpy(float), mlo, mhi)
-
-    log.info("Fit: basis=%s, degree=%d, window=[%.1f, %.1f] Hz, points=%d",
-             args.basis, args.degree, flo, fhi, nfit)
-    log.info("Rebranch: k=%d cycles (soustraction de %.6f rad à la tendance).", k, k*two_pi)
-    log.info("Metrics %g–%g Hz: mean=%.3f  p95=%.3f  max=%.3f  (n=%d)",
-             mlo, mhi, mean_abs, p95_abs, max_abs, n)
+    mean_abs, p95_abs, max_abs, n = compute_metrics(
+        f, phi_corr, df["phi_ref"].to_numpy(float), mlo, mhi
+    )
+
+    log.info(
+        "Fit: basis=%s, degree=%d, window=[%.1f, %.1f] Hz, points=%d",
+        args.basis,
+        args.degree,
+        flo,
+        fhi,
+        nfit,
+    )
+    log.info(
+        "Rebranch: k=%d cycles (soustraction de %.6f rad à la tendance).", k, k * two_pi
+    )
+    log.info(
+        "Metrics %g–%g Hz: mean=%.3f  p95=%.3f  max=%.3f  (n=%d)",
+        mlo,
+        mhi,
+        mean_abs,
+        p95_abs,
+        max_abs,
+        n,
+    )

     if args.dry_run:
         return
@@ -158,10 +228,16 @@ def main():
             meta = {}

     meta["metrics_active"] = {
-        "mean_abs_20_300": mean_abs if (mlo, mhi)==(20.0, 300.0) else meta.get("metrics_active",{}).get("mean_abs_20_300", mean_abs),
-        "p95_abs_20_300":  p95_abs  if (mlo, mhi)==(20.0, 300.0) else meta.get("metrics_active",{}).get("p95_abs_20_300", p95_abs),
-        "max_abs_20_300":  max_abs  if (mlo, mhi)==(20.0, 300.0) else meta.get("metrics_active",{}).get("max_abs_20_300", max_abs),
-        "variant": f"calibrated+poly_deg{args.degree}_{args.basis}_fit{int(flo)}-{int(fhi)}_unwrap_rebranch"
+        "mean_abs_20_300": mean_abs
+        if (mlo, mhi) == (20.0, 300.0)
+        else meta.get("metrics_active", {}).get("mean_abs_20_300", mean_abs),
+        "p95_abs_20_300": p95_abs
+        if (mlo, mhi) == (20.0, 300.0)
+        else meta.get("metrics_active", {}).get("p95_abs_20_300", p95_abs),
+        "max_abs_20_300": max_abs
+        if (mlo, mhi) == (20.0, 300.0)
+        else meta.get("metrics_active", {}).get("max_abs_20_300", max_abs),
+        "variant": f"calibrated+poly_deg{args.degree}_{args.basis}_fit{int(flo)}-{int(fhi)}_unwrap_rebranch",
     }

     meta["poly_correction"] = {
@@ -172,7 +248,7 @@ def main():
         "fit_window_Hz": [flo, fhi],
         "metrics_window_Hz": [mlo, mhi],
         "coeff_desc": [float(x) for x in c_desc],
-        "k_cycles": int(k)
+        "k_cycles": int(k),
     }

     args.meta.write_text(json.dumps(meta, indent=2))
diff --git a/zz-scripts/chapter09/check_p95_methods.py b/zz-scripts/chapter09/check_p95_methods.py
index 3d9081f..7666dcf 100755
--- a/zz-scripts/chapter09/check_p95_methods.py
+++ b/zz-scripts/chapter09/check_p95_methods.py
@@ -13,48 +13,83 @@ python zz-scripts/chapter09/check_p95_methods.py \
   --bins 30 50 80 \
   --plot --out-dir zz-figures/chapter09/p95_methods --xscale log
 """
+
 import argparse
 from pathlib import Path
 import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt
-import math
-import textwrap
+

 def parse_args():
-    p = argparse.ArgumentParser(description="Compare p95 raw / unwrap / rebranch (k cycles).")
-    p.add_argument("--csv", type=Path, required=True, help="CSV phases (09_phases_mcgt.csv)")
-    p.add_argument("--window", nargs=2, type=float, default=[20.0,300.0], help="Fenêtre [Fmin Fmax] (Hz)")
-    p.add_argument("--bins", nargs="+", type=int, default=[30,50,80], help="Bins pour histogrammes (optionnel)")
-    p.add_argument("--plot", action="store_true", help="Enregistrer histogrammes pour chaque méthode et bins")
-    p.add_argument("--out-dir", type=Path, default=Path("zz-figures/chapter09/p95_methods"), help="Répertoire de sortie si --plot")
-    p.add_argument("--xscale", choices=["linear","log"], default="log", help="Echelle x pour histogrammes")
+    p = argparse.ArgumentParser(
+        description="Compare p95 raw / unwrap / rebranch (k cycles)."
+    )
+    p.add_argument(
+        "--csv", type=Path, required=True, help="CSV phases (09_phases_mcgt.csv)"
+    )
+    p.add_argument(
+        "--window",
+        nargs=2,
+        type=float,
+        default=[20.0, 300.0],
+        help="Fenêtre [Fmin Fmax] (Hz)",
+    )
+    p.add_argument(
+        "--bins",
+        nargs="+",
+        type=int,
+        default=[30, 50, 80],
+        help="Bins pour histogrammes (optionnel)",
+    )
+    p.add_argument(
+        "--plot",
+        action="store_true",
+        help="Enregistrer histogrammes pour chaque méthode et bins",
+    )
+    p.add_argument(
+        "--out-dir",
+        type=Path,
+        default=Path("zz-figures/chapter09/p95_methods"),
+        help="Répertoire de sortie si --plot",
+    )
+    p.add_argument(
+        "--xscale",
+        choices=["linear", "log"],
+        default="log",
+        help="Echelle x pour histogrammes",
+    )
     p.add_argument("--dpi", type=int, default=150)
     return p.parse_args()

+
 def compute_stats(arr):
     arrf = arr[np.isfinite(arr)]
     if arrf.size == 0:
-        return {"n":0,"mean":np.nan,"median":np.nan,"p95":np.nan,"max":np.nan}
-    return {"n":int(arrf.size),
-            "mean":float(np.nanmean(arrf)),
-            "median":float(np.nanmedian(arrf)),
-            "p95":float(np.nanpercentile(arrf,95)),
-            "max":float(np.nanmax(arrf))}
+        return {"n": 0, "mean": np.nan, "median": np.nan, "p95": np.nan, "max": np.nan}
+    return {
+        "n": int(arrf.size),
+        "mean": float(np.nanmean(arrf)),
+        "median": float(np.nanmedian(arrf)),
+        "p95": float(np.nanpercentile(arrf, 95)),
+        "max": float(np.nanmax(arrf)),
+    }
+

 def make_log_bins(vals, nbins):
     """Create log-spaced bins for positive vals. If no positive, fallback to linear bins."""
     pos = vals[vals > 0]
     if pos.size == 0:
-        return np.linspace(0.0, 1.0, nbins+1)
+        return np.linspace(0.0, 1.0, nbins + 1)
     lo = max(pos.min(), 1e-12)
     hi = pos.max()
     if hi <= lo:
         hi = lo * 10.0
-    return np.logspace(np.log10(lo), np.log10(hi), nbins+1)
+    return np.logspace(np.log10(lo), np.log10(hi), nbins + 1)
+

 def plot_hist(vals, bins, outpath, xscale="log", dpi=150, title=None):
-    plt.figure(figsize=(8,4.5))
+    plt.figure(figsize=(8, 4.5))
     if xscale == "log":
         bins_arr = make_log_bins(vals, bins)
         plt.hist(vals, bins=bins_arr, alpha=0.7, edgecolor="k", linewidth=0.3)
@@ -71,13 +106,14 @@ def plot_hist(vals, bins, outpath, xscale="log", dpi=150, title=None):
     plt.savefig(outpath, dpi=dpi)
     plt.close()

+
 def main():
     args = parse_args()
     csv = args.csv
     if not csv.exists():
         raise SystemExit(f"CSV not found: {csv}")
     df = pd.read_csv(csv)
-    need = {"f_Hz","phi_ref","phi_mcgt"}
+    need = {"f_Hz", "phi_ref", "phi_mcgt"}
     if not need.issubset(df.columns):
         raise SystemExit(f"{csv} must contain columns {need}")

@@ -114,8 +150,8 @@ def main():
     reb = np.abs(phi_mcgt_rebranch - phi_ref_win)

     # diagnostics: differences
-    max_raw_unw = float(np.nanmax(np.abs(raw - unw))) if raw.size>0 else np.nan
-    max_unw_reb = float(np.nanmax(np.abs(unw - reb))) if unw.size>0 else np.nan
+    max_raw_unw = float(np.nanmax(np.abs(raw - unw))) if raw.size > 0 else np.nan
+    max_unw_reb = float(np.nanmax(np.abs(unw - reb))) if unw.size > 0 else np.nan

     print("\n=== Diagnostics ===")
     print(f"CSV: {csv}")
@@ -123,25 +159,33 @@ def main():
     print(f"K (median cycles) = {k_med}")
     print(f"max |raw - unwrap| = {max_raw_unw:.6g}")
     print(f"max |unwrap - rebranch| = {max_unw_reb:.6g}")
-    print("-"*72)
+    print("-" * 72)

     methods = [("raw", raw), ("unwrap", unw), ("rebranch_k", reb)]
     # print table header
-    print("{:>12s} {:>6s} {:>8s} {:>8s} {:>8s} {:>8s}".format("method","n","mean","median","p95","max"))
-    print("-"*72)
+    print(
+        "{:>12s} {:>6s} {:>8s} {:>8s} {:>8s} {:>8s}".format(
+            "method", "n", "mean", "median", "p95", "max"
+        )
+    )
+    print("-" * 72)
     for name, arr in methods:
         s = compute_stats(arr)
-        print("{:>12s} {:6d} {:8.3f} {:8.3f} {:8.3f} {:8.3f}".format(
-            name, s["n"], s["mean"], s["median"], s["p95"], s["max"]
-        ))
-    print("-"*72)
+        print(
+            "{:>12s} {:6d} {:8.3f} {:8.3f} {:8.3f} {:8.3f}".format(
+                name, s["n"], s["mean"], s["median"], s["p95"], s["max"]
+            )
+        )
+    print("-" * 72)

     # show a few sample numbers for inspection
     sample_idx = np.arange(0, min(10, raw.size))
     print("\nFirst sample (index, f_Hz, raw, unwrap, rebranch):")
     for i in sample_idx:
         fi = np.where(sel)[0][i]  # original index for frequency
-        print(f" {i:2d}  f={f[fi]:7.3f} Hz  raw={raw[i]:10.6g}  unw={unw[i]:10.6g}  reb={reb[i]:10.6g}")
+        print(
+            f" {i:2d}  f={f[fi]:7.3f} Hz  raw={raw[i]:10.6g}  unw={unw[i]:10.6g}  reb={reb[i]:10.6g}"
+        )

     # plotting (optional) : one PNG per method and per bin
     if args.plot:
@@ -149,16 +193,30 @@ def main():
             for name, arr in methods:
                 out = args.out_dir / f"fig_03_{name}_bins{bins}.png"
                 title = f"{name} — bins={bins} — window={int(fmin)}-{int(fmax)} Hz (k={k_med})"
-                plot_hist(arr, bins=bins, outpath=out, xscale=args.xscale, dpi=args.dpi, title=title)
+                plot_hist(
+                    arr,
+                    bins=bins,
+                    outpath=out,
+                    xscale=args.xscale,
+                    dpi=args.dpi,
+                    title=title,
+                )
         print(f"\nPlots saved in: {args.out_dir}")

     # final recommendation line
     print("\nRecommendation:")
     if np.nanmean(unw) < np.nanmean(raw):
-        print(" - unwrap reduces large offsets vs raw: use unwrap (or rebranch) for statistics in publication.")
+        print(
+            " - unwrap reduces large offsets vs raw: use unwrap (or rebranch) for statistics in publication."
+        )
     if np.nanmean(reb) < np.nanmean(unw):
-        print(" - rebranch (k cycles) reduces differences further: consider using rebranch output.")
-    print(" - If raw and unwrap identical => likely every point shifted by same integer*2π (unwrap won't change). Use 'rebranch' to align.\n")
+        print(
+            " - rebranch (k cycles) reduces differences further: consider using rebranch output."
+        )
+    print(
+        " - If raw and unwrap identical => likely every point shifted by same integer*2π (unwrap won't change). Use 'rebranch' to align.\n"
+    )
+

 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter09/extract_phenom_phase.py b/zz-scripts/chapter09/extract_phenom_phase.py
index b09d4ad..f48146f 100755
--- a/zz-scripts/chapter09/extract_phenom_phase.py
+++ b/zz-scripts/chapter09/extract_phenom_phase.py
@@ -12,21 +12,27 @@ import numpy as np
 import pandas as pd
 from pycbc.waveform import get_fd_waveform

+
 def parse_args():
     p = argparse.ArgumentParser(
         description="Extraire la phase de référence IMRPhenomD (PyCBC)"
     )
-    p.add_argument("--fmin",   type=float, required=True, help="Fréquence minimale (Hz)")
-    p.add_argument("--fmax",   type=float, required=True, help="Fréquence maximale (Hz)")
-    p.add_argument("--dlogf",  type=float, required=True, help="Pas Δlog10(f)")
-    p.add_argument("--m1",     type=float, required=True, help="Masse primaire (M☉)")
-    p.add_argument("--m2",     type=float, required=True, help="Masse secondaire (M☉)")
-    p.add_argument("--phi0",   type=float, default=0.0, help="Phase initiale φ0 (rad)")
-    p.add_argument("--dist",   type=float, required=True, help="Distance (Mpc)")
-    p.add_argument("--outcsv", type=str, default="09_phases_imrphenom.csv",
-                   help="Nom du fichier CSV de sortie")
+    p.add_argument("--fmin", type=float, required=True, help="Fréquence minimale (Hz)")
+    p.add_argument("--fmax", type=float, required=True, help="Fréquence maximale (Hz)")
+    p.add_argument("--dlogf", type=float, required=True, help="Pas Δlog10(f)")
+    p.add_argument("--m1", type=float, required=True, help="Masse primaire (M☉)")
+    p.add_argument("--m2", type=float, required=True, help="Masse secondaire (M☉)")
+    p.add_argument("--phi0", type=float, default=0.0, help="Phase initiale φ0 (rad)")
+    p.add_argument("--dist", type=float, required=True, help="Distance (Mpc)")
+    p.add_argument(
+        "--outcsv",
+        type=str,
+        default="09_phases_imrphenom.csv",
+        help="Nom du fichier CSV de sortie",
+    )
     return p.parse_args()

+
 def main():
     args = parse_args()

@@ -38,13 +44,16 @@ def main():
     # Générer le waveform fréquentiel
     hp, hc = get_fd_waveform(
         approximant="IMRPhenomD",
-        mass1=m1, mass2=m2,
-        spin1z=0.0, spin2z=0.0,
-        delta_f=10**(np.log10(args.fmin + args.dlogf) - np.log10(args.fmin)) * args.fmin,
+        mass1=m1,
+        mass2=m2,
+        spin1z=0.0,
+        spin2z=0.0,
+        delta_f=10 ** (np.log10(args.fmin + args.dlogf) - np.log10(args.fmin))
+        * args.fmin,
         f_lower=args.fmin,
         f_final=args.fmax,
         distance=distance,
-        phi0=args.phi0
+        phi0=args.phi0,
     )

     # Récupérer fréquences et phases
@@ -53,7 +62,7 @@ def main():

     # Filtrer la grille log-lin manuellement si nécessaire
     # Ici on suppose que PyCBC renvoie une grille lin-équidistante en delta_f
-    # Pour une grille log-lin, on reconstruirait la grille :
+    # Pour une grille log-lin, on reconstruirait la grille :
     # freqs = 10**np.arange(np.log10(args.fmin), np.log10(args.fmax)+1e-12, args.dlogf)
     # phase = np.interp(freqs, hp.sample_frequencies, np.unwrap(np.angle(hp)))

@@ -62,5 +71,6 @@ def main():
     df.to_csv(args.outcsv, index=False, float_format="%.8e", encoding="utf-8")
     print(f"Écrit : {args.outcsv}")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter09/fetch_gwtc3_confident.py b/zz-scripts/chapter09/fetch_gwtc3_confident.py
index 50f5ffa..c90d160 100755
--- a/zz-scripts/chapter09/fetch_gwtc3_confident.py
+++ b/zz-scripts/chapter09/fetch_gwtc3_confident.py
@@ -10,18 +10,19 @@ dans zz-data/chapter09/.
 Usage:
     python zz-scripts/chapter09/fetch_gwtc3_confident.py
 """
+
 from __future__ import annotations
 import json
 import os
 import sys
 import hashlib
 import datetime
-import shutil

 OUT_CFG = "zz-configuration/GWTC-3-confident-events.json"
 OUT_DATA = "zz-data/chapter09/gwtc3_confident_parameters.json"
 URL_BASE = "https://gwosc.org/api/v2/catalogs/GWTC-3-confident/events"

+
 def sha256_of_file(path):
     h = hashlib.sha256()
     with open(path, "rb") as f:
@@ -29,36 +30,38 @@ def sha256_of_file(path):
             h.update(b)
     return h.hexdigest(), os.path.getsize(path)

+
 def fetch_with_requests(url):
     import requests
+
     headers = {"Accept": "application/json", "User-Agent": "mcgt-fetch/1.0"}
     r = requests.get(url, headers=headers, timeout=25)
     r.raise_for_status()
     return r.json()

+
 def fetch_with_urllib(url):
     from urllib.request import Request, urlopen
-    req = Request(url, headers={"Accept": "application/json", "User-Agent": "mcgt-fetch/1.0"})
+
+    req = Request(
+        url, headers={"Accept": "application/json", "User-Agent": "mcgt-fetch/1.0"}
+    )
     with urlopen(req, timeout=25) as resp:
         data = resp.read()
     return json.loads(data.decode("utf-8", "replace"))

+
 def try_fetch():
     """
     Essaie plusieurs variantes d'URL/format et retourne (obj_json, url_used).
     Lève RuntimeError si tout échoue.
     """
-    candidates = [
-        URL_BASE,
-        URL_BASE + "?format=json",
-        URL_BASE + "?format=api"
-    ]
+    candidates = [URL_BASE, URL_BASE + "?format=json", URL_BASE + "?format=api"]
     errs = []
     for url in candidates:
         try:
             # prefer requests if available
             try:
-                import requests  # type: ignore
                 obj = fetch_with_requests(url)
             except Exception:
                 obj = fetch_with_urllib(url)
@@ -68,6 +71,7 @@ def try_fetch():
             print(f"[WARN] fetch failed for {url}: {e}", file=sys.stderr)
     raise RuntimeError(f"All fetch attempts failed: {errs}")

+
 def normalize_events(json_obj):
     """
     Normalise structure retournée par l'API:
@@ -89,6 +93,7 @@ def normalize_events(json_obj):
     # unknown form -> wrap
     return [json_obj]

+
 def build_minimal_and_rich(events_list, url_used):
     event_ids = []
     rich_events = []
@@ -98,7 +103,13 @@ def build_minimal_and_rich(events_list, url_used):
             name = str(e)
             raw = e
         else:
-            name = e.get("name") or e.get("event_id") or e.get("id") or e.get("event") or e.get("common_name")
+            name = (
+                e.get("name")
+                or e.get("event_id")
+                or e.get("id")
+                or e.get("event")
+                or e.get("common_name")
+            )
             raw = e
         if name is None:
             # generate canonical name if missing
@@ -115,7 +126,7 @@ def build_minimal_and_rich(events_list, url_used):
         entry = {
             "name": name,
             "source_url": raw.get("url") if isinstance(raw, dict) else None,
-            "raw": raw
+            "raw": raw,
         }
         if f_peak is not None:
             entry["f_peak_Hz"] = f_peak
@@ -129,16 +140,17 @@ def build_minimal_and_rich(events_list, url_used):
         "source_url": url_used,
         "fetched_at": now,
         "n_events": len(event_ids),
-        "event_ids": event_ids
+        "event_ids": event_ids,
     }
     rich = {
         "generated_at": now,
         "source_url": url_used,
         "n_events": len(rich_events),
-        "events": rich_events
+        "events": rich_events,
     }
     return minimal, rich

+
 def safe_write(path, obj):
     tmp = path + ".tmp"
     d = os.path.dirname(path)
@@ -150,12 +162,15 @@ def safe_write(path, obj):
     os.replace(tmp, path)
     return sha256_of_file(path)

+
 def main():
     try:
         api_json, url_used = try_fetch()
     except Exception as e:
         print("[ERROR] fetch failed:", e, file=sys.stderr)
-        print("→ Si l'API bloque, télécharger manuellement et placer le fichier dans zz-data/chapter09/")
+        print(
+            "→ Si l'API bloque, télécharger manuellement et placer le fichier dans zz-data/chapter09/"
+        )
         sys.exit(2)

     events_list = normalize_events(api_json)
@@ -167,7 +182,10 @@ def main():
     print(f"[WROTE] {OUT_DATA}  sha256={h_data}  bytes={sz_data}")

     print(f"Fetched {minimal['n_events']} events. Minimal & rich files written.")
-    print(f"Validate with: python -m json.tool {OUT_CFG} && python -m json.tool {OUT_DATA}")
+    print(
+        f"Validate with: python -m json.tool {OUT_CFG} && python -m json.tool {OUT_DATA}"
+    )
+

 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter09/flag_jalons.py b/zz-scripts/chapter09/flag_jalons.py
index 566f621..14dcede 100755
--- a/zz-scripts/chapter09/flag_jalons.py
+++ b/zz-scripts/chapter09/flag_jalons.py
@@ -7,14 +7,19 @@ Usage:
     --meta zz-data/chapter09/09_comparison_milestones.meta.json \
     --out zz-data/chapter09/09_comparison_milestones.flagged.csv
 """
-import argparse, json, datetime
+
+import argparse
+import json
+import datetime
 import numpy as np
 import pandas as pd
 from pathlib import Path

+
 def principal_wrap(d):
     # ramène en (-pi, pi]
-    return ((d + np.pi) % (2*np.pi)) - np.pi
+    return ((d + np.pi) % (2 * np.pi)) - np.pi
+

 def safe_float(x):
     try:
@@ -22,12 +27,23 @@ def safe_float(x):
     except Exception:
         return np.nan

+
 def main():
     p = argparse.ArgumentParser()
     p.add_argument("--csv", required=True, help="CSV milestones input")
     p.add_argument("--meta", required=True, help="Meta JSON to update")
-    p.add_argument("--out", help="Output flagged CSV (defaults to input with .flagged.csv)", default=None)
-    p.add_argument("--metrics-window", nargs=2, type=float, default=[20.0,300.0], help="metrics window (Hz)")
+    p.add_argument(
+        "--out",
+        help="Output flagged CSV (defaults to input with .flagged.csv)",
+        default=None,
+    )
+    p.add_argument(
+        "--metrics-window",
+        nargs=2,
+        type=float,
+        default=[20.0, 300.0],
+        help="metrics window (Hz)",
+    )
     p.add_argument("--sigma-warn", type=float, default=3.0, help="z-score for WARN")
     p.add_argument("--sigma-fail", type=float, default=5.0, help="z-score for FAIL")
     args = p.parse_args()
@@ -37,9 +53,13 @@ def main():
     out_path = Path(args.out) if args.out else csv_path.with_suffix(".flagged.csv")

     df = pd.read_csv(csv_path)
-    required_cols = ["event","f_Hz","phi_mcgt_at_fpeak","obs_phase","sigma_phase"]
+    required_cols = ["event", "f_Hz", "phi_mcgt_at_fpeak", "obs_phase", "sigma_phase"]
     # Accept also phi_mcgt_at_fpeak_cal or phi_mcgt_at_fpeak_raw if main absent
-    alt_phi_cols = ["phi_mcgt_at_fpeak", "phi_mcgt_at_fpeak_cal", "phi_mcgt_at_fpeak_raw"]
+    alt_phi_cols = [
+        "phi_mcgt_at_fpeak",
+        "phi_mcgt_at_fpeak_cal",
+        "phi_mcgt_at_fpeak_raw",
+    ]

     # find usable phi_mcgt column
     phi_col = None
@@ -51,7 +71,7 @@ def main():
         raise SystemExit("Aucune colonne phi_mcgt_at_fpeak trouvée (ni .cal/.raw).")

     # ensure columns exist (add missing as NaN)
-    for c in ["event","f_Hz","obs_phase","sigma_phase","classe"]:
+    for c in ["event", "f_Hz", "obs_phase", "sigma_phase", "classe"]:
         if c not in df.columns:
             df[c] = np.nan

@@ -74,19 +94,28 @@ def main():
         if not np.isfinite(f) or f <= 0:
             flags.append("FAIL")
             reason_list.append("f_Hz missing/invalid")
-            abs_diffs.append(np.nan); z_scores.append(np.nan); reasons.append("; ".join(reason_list)); continue
+            abs_diffs.append(np.nan)
+            z_scores.append(np.nan)
+            reasons.append("; ".join(reason_list))
+            continue

         if not np.isfinite(phi_mcgt):
             flags.append("FAIL")
             reason_list.append(f"{phi_col} missing/invalid")
-            abs_diffs.append(np.nan); z_scores.append(np.nan); reasons.append("; ".join(reason_list)); continue
+            abs_diffs.append(np.nan)
+            z_scores.append(np.nan)
+            reasons.append("; ".join(reason_list))
+            continue

         if not np.isfinite(obs):
             # if obs missing: warn
             reason_list.append("obs_phase missing")
             # we'll compute abs diff vs phi_ref if present? For now mark warn.
             flags.append("WARN")
-            abs_diffs.append(np.nan); z_scores.append(np.nan); reasons.append("; ".join(reason_list)); continue
+            abs_diffs.append(np.nan)
+            z_scores.append(np.nan)
+            reasons.append("; ".join(reason_list))
+            continue

         # compute abs diff (wrapped)
         d = principal_wrap(phi_mcgt - obs)
@@ -98,25 +127,32 @@ def main():
             z_scores.append(np.nan)
             # warn: no sigma
             if f < fmin or f > fmax:
-                flags.append("WARN"); reason_list.append("sigma missing; f_peak hors fenêtre metrics")
+                flags.append("WARN")
+                reason_list.append("sigma missing; f_peak hors fenêtre metrics")
             else:
-                flags.append("WARN"); reason_list.append("sigma missing")
+                flags.append("WARN")
+                reason_list.append("sigma missing")
             z_scores.append(np.nan)
             reasons.append("; ".join(reason_list))
             continue

         if sigma == 0:
-            flags.append("FAIL"); reason_list.append("sigma_phase == 0")
-            z_scores.append(np.nan); reasons.append("; ".join(reason_list)); continue
+            flags.append("FAIL")
+            reason_list.append("sigma_phase == 0")
+            z_scores.append(np.nan)
+            reasons.append("; ".join(reason_list))
+            continue

         z = ad / sigma
         z_scores.append(float(z))

         # thresholds
         if z > args.sigma_fail:
-            flags.append("FAIL"); reason_list.append(f"z={z:.2f}>{args.sigma_fail}")
+            flags.append("FAIL")
+            reason_list.append(f"z={z:.2f}>{args.sigma_fail}")
         elif z > args.sigma_warn:
-            flags.append("WARN"); reason_list.append(f"z={z:.2f}>{args.sigma_warn}")
+            flags.append("WARN")
+            reason_list.append(f"z={z:.2f}>{args.sigma_warn}")
         else:
             flags.append("OK")

@@ -144,27 +180,30 @@ def main():
     totals = int(len(df))

     summary = {
-        "generated_at": datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z",
+        "generated_at": datetime.datetime.utcnow().replace(microsecond=0).isoformat()
+        + "Z",
         "n_rows_checked": totals,
         "n_ok": n_ok,
         "n_warn": n_warn,
         "n_fail": n_fail,
         "examples_fail": [],
-        "examples_warn": []
+        "examples_warn": [],
     }

     # collect up to 5 examples each
     for label, key in [("FAIL", "examples_fail"), ("WARN", "examples_warn")]:
         sub = df[df["flag"] == label].head(5)
         for _, r in sub.iterrows():
-            summary[key].append({
-                "event": r.get("event", ""),
-                "f_Hz": r.get("f_Hz", None),
-                "flag_reason": r.get("flag_reason", ""),
-                "abs_phase_diff": r.get("_abs_phase_diff_rad", None),
-                "sigma_phase": r.get("sigma_phase", None),
-                "z_score": r.get("_z_score", None)
-            })
+            summary[key].append(
+                {
+                    "event": r.get("event", ""),
+                    "f_Hz": r.get("f_Hz", None),
+                    "flag_reason": r.get("flag_reason", ""),
+                    "abs_phase_diff": r.get("_abs_phase_diff_rad", None),
+                    "sigma_phase": r.get("sigma_phase", None),
+                    "z_score": r.get("_z_score", None),
+                }
+            )

     # update meta json
     try:
@@ -182,5 +221,6 @@ def main():
     print("Flagged csv written to:", out_path)
     print("Meta updated at:", meta_path)

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter09/generate_data_chapter09.py b/zz-scripts/chapter09/generate_data_chapter09.py
index 3bb4f1a..17d91f9 100755
--- a/zz-scripts/chapter09/generate_data_chapter09.py
+++ b/zz-scripts/chapter09/generate_data_chapter09.py
@@ -20,6 +20,7 @@ Conventions:
 - La "variante active" reflète la politique de calage (calibrated si --calibrate≠off, sinon raw).
 - Les métriques par défaut sont calculées sur 20–300 Hz.
 """
+
 from __future__ import annotations

 import argparse
@@ -53,21 +54,26 @@ REF_CSV = OUT_DIR / "09_phases_imrphenom.csv"
 REF_META = OUT_DIR / "09_phases_imrphenom.meta.json"
 JALONS_CSV = OUT_DIR / "09_comparison_milestones.csv"

+
 # -----------------------
 # Utilities
 # -----------------------
 def setup_logger(level: str = "INFO") -> logging.Logger:
     logging.basicConfig(
         level=getattr(logging, level.upper(), logging.INFO),
-        format='[%(asctime)s] [%(levelname)s] %(message)s',
-        datefmt='%Y-%m-%d %H:%M:%S',
+        format="[%(asctime)s] [%(levelname)s] %(message)s",
+        datefmt="%Y-%m-%d %H:%M:%S",
     )
     return logging.getLogger("chapitre9.pipeline")


 def git_hash() -> Optional[str]:
     try:
-        return subprocess.check_output(["git", "rev-parse", "HEAD"], cwd=PROJECT_ROOT).decode().strip()
+        return (
+            subprocess.check_output(["git", "rev-parse", "HEAD"], cwd=PROJECT_ROOT)
+            .decode()
+            .strip()
+        )
     except Exception:
         return None

@@ -80,26 +86,30 @@ def ensure_dirs() -> None:
 def load_ini(path: Path | None) -> dict:
     cfg = {}
     if path and path.exists():
-        parser = configparser.ConfigParser(inline_comment_prefixes=('#', ';'), interpolation=None)
+        parser = configparser.ConfigParser(
+            inline_comment_prefixes=("#", ";"), interpolation=None
+        )
         parser.read(path)
         if "scan" in parser:
             s = parser["scan"]
+
             def fget(key: str, default: float) -> float:
                 try:
                     return s.getfloat(key)
                 except Exception:
                     return default
+
             cfg = {
-                "fmin":   fget("fmin", 10.0),
-                "fmax":   fget("fmax", 2048.0),
-                "dlog":   fget("dlog", 0.01),
-                "m1":     fget("m1", 30.0),
-                "m2":     fget("m2", 30.0),
+                "fmin": fget("fmin", 10.0),
+                "fmax": fget("fmax", 2048.0),
+                "dlog": fget("dlog", 0.01),
+                "m1": fget("m1", 30.0),
+                "m2": fget("m2", 30.0),
                 "q0star": fget("q0star", 0.20),
-                "alpha":  fget("alpha", 1.00),
-                "phi0":   fget("phi0", 0.0),
-                "tc":     fget("tc", 0.0),
-                "tol":    fget("tol", 1e-8),
+                "alpha": fget("alpha", 1.00),
+                "phi0": fget("phi0", 0.0),
+                "tc": fget("tc", 0.0),
+                "tol": fget("tol", 1e-8),
             }
     return cfg

@@ -128,7 +138,9 @@ def clamp_min_frequencies(f: np.ndarray, fmin: float) -> np.ndarray:
 # -----------------------
 # Reference generation via LALSuite or fallback
 # -----------------------
-def _try_lalsuite_phi_ref(freqs: np.ndarray, logger: logging.Logger) -> Optional[np.ndarray]:
+def _try_lalsuite_phi_ref(
+    freqs: np.ndarray, logger: logging.Logger
+) -> Optional[np.ndarray]:
     try:
         import lal  # type: ignore
         import lalsimulation as lalsim  # type: ignore
@@ -146,7 +158,11 @@ def _try_lalsuite_phi_ref(freqs: np.ndarray, logger: logging.Logger) -> Optional
         fmin = float(np.nanmin(f))
         fmax = float(np.nanmax(f))
         df_candidates = np.diff(np.unique(f[np.isfinite(f)]))
-        df = float(np.nanmin(df_candidates)) if df_candidates.size else max(1.0, fmin * 1e-3)
+        df = (
+            float(np.nanmin(df_candidates))
+            if df_candidates.size
+            else max(1.0, fmin * 1e-3)
+        )
         if not np.isfinite(df) or df <= 0:
             df = max(1.0, fmin * 1e-3)

@@ -161,13 +177,26 @@ def _try_lalsuite_phi_ref(freqs: np.ndarray, logger: logging.Logger) -> Optional
         dct = lal.CreateDict()

         hp, _ = lalsim.SimInspiralChooseFDWaveform(
-            m1_kg, m2_kg,
-            zero, zero, zero,
-            zero, zero, zero,
-            dist, incl, phi0,
-            zero, zero, zero,
-            df, fmin, fmax, f_ref,
-            dct, lalsim.IMRPhenomD
+            m1_kg,
+            m2_kg,
+            zero,
+            zero,
+            zero,
+            zero,
+            zero,
+            zero,
+            dist,
+            incl,
+            phi0,
+            zero,
+            zero,
+            zero,
+            df,
+            fmin,
+            fmax,
+            f_ref,
+            dct,
+            lalsim.IMRPhenomD,
         )

         n = int(hp.data.length)
@@ -179,27 +208,42 @@ def _try_lalsuite_phi_ref(freqs: np.ndarray, logger: logging.Logger) -> Optional
         return None


-def _try_external_script_for_ref(freqs: np.ndarray, logger: logging.Logger) -> Optional[np.ndarray]:
+def _try_external_script_for_ref(
+    freqs: np.ndarray, logger: logging.Logger
+) -> Optional[np.ndarray]:
     script = PROJECT_ROOT / "zz-scripts" / "chapter09" / "extract_phenom_phase.py"
     if not script.exists():
         logger.debug("Fallback script introuvable: %s", script)
         return None
     try:
-        proc = subprocess.run([sys.executable, str(script)], cwd=PROJECT_ROOT,
-                              stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False)
+        proc = subprocess.run(
+            [sys.executable, str(script)],
+            cwd=PROJECT_ROOT,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+            check=False,
+        )
         if REF_CSV.exists():
             df = pd.read_csv(REF_CSV)
             if {"f_Hz", "phi_ref"}.issubset(df.columns):
                 f = np.asarray(freqs, float)
-                return np.interp(f, df["f_Hz"].to_numpy(float), df["phi_ref"].to_numpy(float))
-        logger.debug("extract_phenom_phase stdout: %s", proc.stdout.decode(errors="ignore")[:400])
-        logger.debug("extract_phenom_phase stderr: %s", proc.stderr.decode(errors="ignore")[:400])
+                return np.interp(
+                    f, df["f_Hz"].to_numpy(float), df["phi_ref"].to_numpy(float)
+                )
+        logger.debug(
+            "extract_phenom_phase stdout: %s", proc.stdout.decode(errors="ignore")[:400]
+        )
+        logger.debug(
+            "extract_phenom_phase stderr: %s", proc.stderr.decode(errors="ignore")[:400]
+        )
     except Exception as e:
         logger.warning("Fallback script a échoué: %s", e)
     return None


-def load_or_build_reference(freqs_cfg: np.ndarray, logger: logging.Logger, refresh: bool) -> Tuple[np.ndarray, np.ndarray, str]:
+def load_or_build_reference(
+    freqs_cfg: np.ndarray, logger: logging.Logger, refresh: bool
+) -> Tuple[np.ndarray, np.ndarray, str]:
     """
     Charge ou (re)génère la référence. Retourne (f_ref, phi_ref, tag_source).
     """
@@ -231,19 +275,33 @@ def load_or_build_reference(freqs_cfg: np.ndarray, logger: logging.Logger, refre
         phi_ref = _try_external_script_for_ref(f, logger)

     if phi_ref is None:
-        raise RuntimeError("Impossible de générer φ_ref : installez LALSuite ou fournissez " + str(REF_CSV))
+        raise RuntimeError(
+            "Impossible de générer φ_ref : installez LALSuite ou fournissez "
+            + str(REF_CSV)
+        )

     # Save reference
     try:
-        pd.DataFrame({"f_Hz": f, "phi_ref": np.asarray(phi_ref, float)}).to_csv(REF_CSV, index=False)
+        pd.DataFrame({"f_Hz": f, "phi_ref": np.asarray(phi_ref, float)}).to_csv(
+            REF_CSV, index=False
+        )
         meta = {
             "source": tag,
             "n_points": int(len(f)),
-            "grid": {"fmin_Hz": float(np.nanmin(f)), "fmax_Hz": float(np.nanmax(f)), "monotone": bool(np.all(np.diff(f) > 0))},
-            "repro": {"git_hash": git_hash(), "numpy": np.__version__, "pandas": pd.__version__}
+            "grid": {
+                "fmin_Hz": float(np.nanmin(f)),
+                "fmax_Hz": float(np.nanmax(f)),
+                "monotone": bool(np.all(np.diff(f) > 0)),
+            },
+            "repro": {
+                "git_hash": git_hash(),
+                "numpy": np.__version__,
+                "pandas": pd.__version__,
+            },
         }
         try:
             import lalsimulation as _ls  # type: ignore
+
             meta["repro"]["lalsuite"] = getattr(_ls, "__version__", "present")
         except Exception:
             meta["repro"]["lalsuite"] = None
@@ -266,14 +324,16 @@ def fit_alignment_phi0_tc(
     fmax: float,
     model: str,
     weight: str,
-    logger: logging.Logger
+    logger: logging.Logger,
 ) -> Tuple[float, float, int]:
     f = np.asarray(f, float)
     y = np.asarray(phi_ref, float) - np.asarray(phi_mcgt, float)
     mask = np.isfinite(f) & np.isfinite(y) & (f >= fmin) & (f <= fmax)
     n = int(np.sum(mask))
     if n < 2:
-        logger.warning("Calage: trop peu de points dans [%.1f, %.1f] Hz (n=%d).", fmin, fmax, n)
+        logger.warning(
+            "Calage: trop peu de points dans [%.1f, %.1f] Hz (n=%d).", fmin, fmax, n
+        )
         return 0.0, 0.0, n

     ff = f[mask]
@@ -301,8 +361,16 @@ def fit_alignment_phi0_tc(
     phi0_hat = float(beta[0])
     tc_hat = float(beta[1]) if (model == "phi0_tc" and len(beta) > 1) else 0.0

-    logger.info("Calage %s (poids=%s): φ0=%.6e rad, t_c=%.6e s (n=%d, window=[%.1f, %.1f])",
-                model, weight, phi0_hat, tc_hat, n, fmin, fmax)
+    logger.info(
+        "Calage %s (poids=%s): φ0=%.6e rad, t_c=%.6e s (n=%d, window=[%.1f, %.1f])",
+        model,
+        weight,
+        phi0_hat,
+        tc_hat,
+        n,
+        fmin,
+        fmax,
+    )
     return phi0_hat, tc_hat, n


@@ -310,34 +378,102 @@ def fit_alignment_phi0_tc(
 # CLI
 # -----------------------
 def parse_args() -> argparse.Namespace:
-    ap = argparse.ArgumentParser(description="Chapitre 9 — Pipeline MCGT (accord homogène 20–300 Hz)")
-    ap.add_argument("-i", "--ini", type=Path, default=Path("zz-configuration/gw_phase.ini"),
-                    help="Fichier INI (section [scan]).")
-    ap.add_argument("--log-level", choices=["DEBUG", "INFO", "WARNING", "ERROR"], default="INFO")
-    ap.add_argument("--overwrite", action="store_true", help="Écraser les fichiers de sortie s'ils existent.")
-    ap.add_argument("--refresh-ref", action="store_true", help="Forcer la régénération de φ_ref (IMRPhenom).")
-
-    ap.add_argument("--metrics-window", nargs=2, type=float, default=[20.0, 300.0],
-                    metavar=("FMIN", "FMAX"), help="Fenêtre [Hz] pour les métriques.")
-    ap.add_argument("--calibrate", choices=["off", "phi0", "phi0,tc"], default="phi0,tc",
-                    help="Modèle de calage global (φ0 / φ0+t_c).")
-    ap.add_argument("--calib-window", nargs=2, type=float, default=[20.0, 300.0],
-                    metavar=("FMIN", "FMAX"), help="Fenêtre [Hz] pour le fit φ0(/t_c).")
-    ap.add_argument("--calib-weight", choices=["flat", "1/f", "1/f2"], default="1/f2",
-                    help="Pondération WLS dans la fenêtre de calage.")
-
-    ap.add_argument("--auto-tighten", dest="auto_tighten", action="store_true", default=True,
-                    help="Activer le resserrage automatique (par défaut ON).")
-    ap.add_argument("--no-auto-tighten", dest="auto_tighten", action="store_false",
-                    help="Désactiver le resserrage automatique.")
-    ap.add_argument("--tighten-window", nargs=2, type=float, default=[30.0, 250.0],
-                    metavar=("FMIN", "FMAX"), help="Fenêtre [Hz] utilisée si resserrage.")
-    ap.add_argument("--tighten-threshold-p95", type=float, default=5.0,
-                    help="Seuil p95(|Δφ|) 20–300 (rad) déclenchant le resserrage.")
-
-    ap.add_argument("--export-diff", action="store_true", help="Écrire zz-data/chapter09/09_phase_diff.csv.")
-    ap.add_argument("--export-anomalies", action="store_true", help="Écrire comparaison jalons si présents.")
-    ap.add_argument("--export-heatmap", action="store_true", help="Écrire 09_fisher_scan2D.csv (approx locale).")
+    ap = argparse.ArgumentParser(
+        description="Chapitre 9 — Pipeline MCGT (accord homogène 20–300 Hz)"
+    )
+    ap.add_argument(
+        "-i",
+        "--ini",
+        type=Path,
+        default=Path("zz-configuration/gw_phase.ini"),
+        help="Fichier INI (section [scan]).",
+    )
+    ap.add_argument(
+        "--log-level", choices=["DEBUG", "INFO", "WARNING", "ERROR"], default="INFO"
+    )
+    ap.add_argument(
+        "--overwrite",
+        action="store_true",
+        help="Écraser les fichiers de sortie s'ils existent.",
+    )
+    ap.add_argument(
+        "--refresh-ref",
+        action="store_true",
+        help="Forcer la régénération de φ_ref (IMRPhenom).",
+    )
+
+    ap.add_argument(
+        "--metrics-window",
+        nargs=2,
+        type=float,
+        default=[20.0, 300.0],
+        metavar=("FMIN", "FMAX"),
+        help="Fenêtre [Hz] pour les métriques.",
+    )
+    ap.add_argument(
+        "--calibrate",
+        choices=["off", "phi0", "phi0,tc"],
+        default="phi0,tc",
+        help="Modèle de calage global (φ0 / φ0+t_c).",
+    )
+    ap.add_argument(
+        "--calib-window",
+        nargs=2,
+        type=float,
+        default=[20.0, 300.0],
+        metavar=("FMIN", "FMAX"),
+        help="Fenêtre [Hz] pour le fit φ0(/t_c).",
+    )
+    ap.add_argument(
+        "--calib-weight",
+        choices=["flat", "1/f", "1/f2"],
+        default="1/f2",
+        help="Pondération WLS dans la fenêtre de calage.",
+    )
+
+    ap.add_argument(
+        "--auto-tighten",
+        dest="auto_tighten",
+        action="store_true",
+        default=True,
+        help="Activer le resserrage automatique (par défaut ON).",
+    )
+    ap.add_argument(
+        "--no-auto-tighten",
+        dest="auto_tighten",
+        action="store_false",
+        help="Désactiver le resserrage automatique.",
+    )
+    ap.add_argument(
+        "--tighten-window",
+        nargs=2,
+        type=float,
+        default=[30.0, 250.0],
+        metavar=("FMIN", "FMAX"),
+        help="Fenêtre [Hz] utilisée si resserrage.",
+    )
+    ap.add_argument(
+        "--tighten-threshold-p95",
+        type=float,
+        default=5.0,
+        help="Seuil p95(|Δφ|) 20–300 (rad) déclenchant le resserrage.",
+    )
+
+    ap.add_argument(
+        "--export-diff",
+        action="store_true",
+        help="Écrire zz-data/chapter09/09_phase_diff.csv.",
+    )
+    ap.add_argument(
+        "--export-anomalies",
+        action="store_true",
+        help="Écrire comparaison jalons si présents.",
+    )
+    ap.add_argument(
+        "--export-heatmap",
+        action="store_true",
+        help="Écrire 09_fisher_scan2D.csv (approx locale).",
+    )

     ap.add_argument("--fmin", type=float, default=None)
     ap.add_argument("--fmax", type=float, default=None)
@@ -363,22 +499,45 @@ def main() -> None:

     # Defaults + INI + overrides
     cfg = {
-        "fmin": 10.0, "fmax": 2048.0, "dlog": 0.01,
-        "m1": 30.0, "m2": 30.0, "q0star": 0.20, "alpha": 1.00,
-        "phi0": 0.0, "tc": 0.0, "tol": 1e-8,
+        "fmin": 10.0,
+        "fmax": 2048.0,
+        "dlog": 0.01,
+        "m1": 30.0,
+        "m2": 30.0,
+        "q0star": 0.20,
+        "alpha": 1.00,
+        "phi0": 0.0,
+        "tc": 0.0,
+        "tol": 1e-8,
     }
     cfg.update(load_ini(args.ini))
-    for k in ("fmin", "fmax", "dlog", "m1", "m2", "q0star", "alpha", "phi0", "tc", "tol"):
+    for k in (
+        "fmin",
+        "fmax",
+        "dlog",
+        "m1",
+        "m2",
+        "q0star",
+        "alpha",
+        "phi0",
+        "tc",
+        "tol",
+    ):
         v = getattr(args, k, None)
         if v is not None:
             cfg[k] = v

-    fmin = float(cfg["fmin"]); fmax = float(cfg["fmax"]); dlog = float(cfg["dlog"])
+    fmin = float(cfg["fmin"])
+    fmax = float(cfg["fmax"])
+    dlog = float(cfg["dlog"])
     params = PhaseParams(
-        m1=float(cfg["m1"]), m2=float(cfg["m2"]),
-        q0star=float(cfg["q0star"]), alpha=float(cfg["alpha"]),
-        phi0=float(cfg["phi0"]), tc=float(cfg["tc"]),
-        tol=float(cfg["tol"])
+        m1=float(cfg["m1"]),
+        m2=float(cfg["m2"]),
+        q0star=float(cfg["q0star"]),
+        alpha=float(cfg["alpha"]),
+        phi0=float(cfg["phi0"]),
+        tc=float(cfg["tc"]),
+        tol=float(cfg["tol"]),
     )
     logger.info("Paramètres MCGT: %s", params)

@@ -386,7 +545,9 @@ def main() -> None:
     freqs_cfg = build_loglin_grid(fmin, fmax, dlog)

     # Load or build reference
-    f_ref, phi_ref, ref_tag = load_or_build_reference(freqs_cfg, logger, refresh=args.refresh_ref)
+    f_ref, phi_ref, ref_tag = load_or_build_reference(
+        freqs_cfg, logger, refresh=args.refresh_ref
+    )

     # Solve MCGT on reference grid (clamp f < fmin to fmin)
     f = clamp_min_frequencies(f_ref, fmin)
@@ -395,17 +556,25 @@ def main() -> None:
     # Filter finite points
     mask_ok = np.isfinite(f) & np.isfinite(phi_ref) & np.isfinite(phi_mcgt_raw)
     if not np.any(mask_ok):
-        raise RuntimeError("Aucune donnée finie après génération de la référence et solveur MCGT.")
+        raise RuntimeError(
+            "Aucune donnée finie après génération de la référence et solveur MCGT."
+        )
     if np.sum(~mask_ok) > 0:
-        logger.warning("Points non finis supprimés: %d / %d", int(np.sum(~mask_ok)), int(len(f)))
+        logger.warning(
+            "Points non finis supprimés: %d / %d", int(np.sum(~mask_ok)), int(len(f))
+        )

     f = f[mask_ok]
     phi_ref = phi_ref[mask_ok]
     phi_mcgt_raw = phi_mcgt_raw[mask_ok]

     # Calibration WLS
-    calib_enabled = (args.calibrate != "off")
-    calib_model = "phi0_tc" if args.calibrate == "phi0,tc" else ("phi0" if args.calibrate == "phi0" else None)
+    calib_enabled = args.calibrate != "off"
+    calib_model = (
+        "phi0_tc"
+        if args.calibrate == "phi0,tc"
+        else ("phi0" if args.calibrate == "phi0" else None)
+    )
     f_cal_lo, f_cal_hi = map(float, args.calib_window)

     phi0_hat = 0.0
@@ -419,7 +588,14 @@ def main() -> None:

     if calib_enabled and calib_model is not None:
         phi0_hat, tc_hat, n_cal = fit_alignment_phi0_tc(
-            f, phi_ref, phi_mcgt_raw, f_cal_lo, f_cal_hi, model=calib_model, weight=args.calib_weight, logger=logger
+            f,
+            phi_ref,
+            phi_mcgt_raw,
+            f_cal_lo,
+            f_cal_hi,
+            model=calib_model,
+            weight=args.calib_weight,
+            logger=logger,
         )
         phi_mcgt_cal = phi_mcgt_raw + phi0_hat + (2.0 * np.pi * f * tc_hat)

@@ -427,20 +603,39 @@ def main() -> None:
         band_lo, band_hi = map(float, args.metrics_window)
         band_mask = (f >= band_lo) & (f <= band_hi)
         p95_check_before = p95(np.abs(phi_mcgt_cal[band_mask] - phi_ref[band_mask]))
-        logger.info("Contrôle p95 avant resserrage: p95(|Δφ|)@[%.1f-%.1f]=%.6f rad (seuil=%.3f)",
-                    band_lo, band_hi, p95_check_before, float(args.tighten_threshold_p95))
+        logger.info(
+            "Contrôle p95 avant resserrage: p95(|Δφ|)@[%.1f-%.1f]=%.6f rad (seuil=%.3f)",
+            band_lo,
+            band_hi,
+            p95_check_before,
+            float(args.tighten_threshold_p95),
+        )

         if args.auto_tighten and (p95_check_before > float(args.tighten_threshold_p95)):
             tlo, thi = map(float, args.tighten_window)
-            logger.info("Resserrement automatique: refit sur [%.1f, %.1f] Hz.", tlo, thi)
+            logger.info(
+                "Resserrement automatique: refit sur [%.1f, %.1f] Hz.", tlo, thi
+            )
             phi0_hat, tc_hat, n_cal = fit_alignment_phi0_tc(
-                f, phi_ref, phi_mcgt_raw, tlo, thi, model=calib_model, weight=args.calib_weight, logger=logger
+                f,
+                phi_ref,
+                phi_mcgt_raw,
+                tlo,
+                thi,
+                model=calib_model,
+                weight=args.calib_weight,
+                logger=logger,
             )
             phi_mcgt_cal = phi_mcgt_raw + phi0_hat + (2.0 * np.pi * f * tc_hat)
             used_window = [tlo, thi]
             tightened = True
             p95_check_after = p95(np.abs(phi_mcgt_cal[band_mask] - phi_ref[band_mask]))
-            logger.info("Après resserrage: p95(|Δφ|)@[%.1f-%.1f]=%.6f rad", band_lo, band_hi, p95_check_after)
+            logger.info(
+                "Après resserrage: p95(|Δφ|)@[%.1f-%.1f]=%.6f rad",
+                band_lo,
+                band_hi,
+                p95_check_after,
+            )
         else:
             p95_check_after = p95_check_before

@@ -454,15 +649,20 @@ def main() -> None:
     # Exports phases (respect overwrite)
     out_mcgt = OUT_DIR / "09_phases_mcgt.csv"
     if out_mcgt.exists() and not args.overwrite:
-        logger.info("Conserver fichier existant (utilisez --overwrite pour écraser): %s", out_mcgt)
+        logger.info(
+            "Conserver fichier existant (utilisez --overwrite pour écraser): %s",
+            out_mcgt,
+        )
     else:
-        pd.DataFrame({
-            "f_Hz": f,
-            "phi_ref": phi_ref,
-            "phi_mcgt": phi_mcgt_active,
-            "phi_mcgt_raw": phi_mcgt_raw,
-            "phi_mcgt_cal": phi_mcgt_cal
-        }).to_csv(out_mcgt, index=False)
+        pd.DataFrame(
+            {
+                "f_Hz": f,
+                "phi_ref": phi_ref,
+                "phi_mcgt": phi_mcgt_active,
+                "phi_mcgt_raw": phi_mcgt_raw,
+                "phi_mcgt_cal": phi_mcgt_cal,
+            }
+        ).to_csv(out_mcgt, index=False)
         logger.info("Écrit → %s", out_mcgt)

     # Δφ and metrics (metrics window)
@@ -497,20 +697,24 @@ def main() -> None:
     out_diff = OUT_DIR / "09_phase_diff.csv"
     if args.export_diff:
         if out_diff.exists() and not args.overwrite:
-            logger.info("Conserver diff existant (use --overwrite to replace): %s", out_diff)
+            logger.info(
+                "Conserver diff existant (use --overwrite to replace): %s", out_diff
+            )
         else:
-            df_diff = pd.DataFrame({
-                "f_Hz": f,
-                "dphi": dphi,
-                "abs_dphi": abs_dphi,
-                "mean_abs_20_300": np.full_like(f, float(mean_abs), dtype=float),
-                "max_abs_20_300":  np.full_like(f, float(max_abs),  dtype=float),
-                "p95_abs_20_300":  np.full_like(f, float(p95_abs),  dtype=float),
-                "dphi_raw": dphi_raw,
-                "abs_dphi_raw": abs_raw,
-                "dphi_cal": dphi_cal,
-                "abs_dphi_cal": abs_cal,
-            })
+            df_diff = pd.DataFrame(
+                {
+                    "f_Hz": f,
+                    "dphi": dphi,
+                    "abs_dphi": abs_dphi,
+                    "mean_abs_20_300": np.full_like(f, float(mean_abs), dtype=float),
+                    "max_abs_20_300": np.full_like(f, float(max_abs), dtype=float),
+                    "p95_abs_20_300": np.full_like(f, float(p95_abs), dtype=float),
+                    "dphi_raw": dphi_raw,
+                    "abs_dphi_raw": abs_raw,
+                    "dphi_cal": dphi_cal,
+                    "abs_dphi_cal": abs_cal,
+                }
+            )
             df_diff.to_csv(out_diff, index=False)
             logger.info("Écrit → %s", out_diff)

@@ -521,12 +725,16 @@ def main() -> None:
             jal = pd.read_csv(JALONS_CSV)
             need = {"event", "f_Hz", "obs_phase", "sigma_phase"}
             if not need.issubset(jal.columns):
-                logger.warning("Jalons: colonnes manquantes (attendues: %s).", sorted(need))
+                logger.warning(
+                    "Jalons: colonnes manquantes (attendues: %s).", sorted(need)
+                )
             else:
                 fpk = np.asarray(jal["f_Hz"].to_numpy(float), float)
                 phi_ref_at = np.interp(fpk, f, phi_ref)
                 phi_raw_at = np.interp(fpk, f, phi_mcgt_raw)
-                phi_cal_at = np.interp(fpk, f, phi_mcgt_raw + phi0_hat + 2.0*np.pi*f*tc_hat)
+                phi_cal_at = np.interp(
+                    fpk, f, phi_mcgt_raw + phi0_hat + 2.0 * np.pi * f * tc_hat
+                )
                 phi_active_at = phi_cal_at if calib_enabled else phi_raw_at

                 obs = jal["obs_phase"].to_numpy(float)
@@ -534,22 +742,26 @@ def main() -> None:
                 epsilon_rel = (phi_active_at - obs) / denom

                 out_jalons_cmp = OUT_DIR / "09_comparison_milestones.csv"
-                pd.DataFrame({
-                    "event": jal["event"],
-                    "f_Hz": fpk,
-                    "phi_ref_at_fpeak": phi_ref_at,
-                    "phi_mcgt_at_fpeak": phi_active_at,
-                    "phi_mcgt_at_fpeak_raw": phi_raw_at,
-                    "phi_mcgt_at_fpeak_cal": phi_cal_at,
-                    "obs_phase": jal["obs_phase"],
-                    "sigma_phase": jal["sigma_phase"],
-                    "epsilon_rel": epsilon_rel,
-                    "classe": jal.get("classe", pd.Series([""] * len(jal))),
-                    "variant": "calibrated" if calib_enabled else "raw",
-                }).to_csv(out_jalons_cmp, index=False)
+                pd.DataFrame(
+                    {
+                        "event": jal["event"],
+                        "f_Hz": fpk,
+                        "phi_ref_at_fpeak": phi_ref_at,
+                        "phi_mcgt_at_fpeak": phi_active_at,
+                        "phi_mcgt_at_fpeak_raw": phi_raw_at,
+                        "phi_mcgt_at_fpeak_cal": phi_cal_at,
+                        "obs_phase": jal["obs_phase"],
+                        "sigma_phase": jal["sigma_phase"],
+                        "epsilon_rel": epsilon_rel,
+                        "classe": jal.get("classe", pd.Series([""] * len(jal))),
+                        "variant": "calibrated" if calib_enabled else "raw",
+                    }
+                ).to_csv(out_jalons_cmp, index=False)
                 logger.info("Écrit → %s", out_jalons_cmp)
         else:
-            logger.warning("Aucun jalon à comparer (fichier introuvable): %s", JALONS_CSV)
+            logger.warning(
+                "Aucun jalon à comparer (fichier introuvable): %s", JALONS_CSV
+            )

     # Optional local Fisher-like heatmap (approx)
     out_fisher = None
@@ -560,19 +772,47 @@ def main() -> None:
                 sigma_phase = float(np.median(sj.values)) if len(sj) else 0.1
             else:
                 sigma_phase = 0.1
-            param2_vals = np.linspace(max(0.5, params.alpha - 0.5), min(2.0, params.alpha + 0.5), 51)
+            param2_vals = np.linspace(
+                max(0.5, params.alpha - 0.5), min(2.0, params.alpha + 0.5), 51
+            )
             eps = 1e-5
             rows = []
             for a in param2_vals:
-                p_lo = PhaseParams(m1=params.m1, m2=params.m2, q0star=params.q0star - eps, alpha=a,
-                                   phi0=params.phi0, tc=params.tc, tol=params.tol)
-                p_hi = PhaseParams(m1=params.m1, m2=params.m2, q0star=params.q0star + eps, alpha=a,
-                                   phi0=params.phi0, tc=params.tc, tol=params.tol)
+                p_lo = PhaseParams(
+                    m1=params.m1,
+                    m2=params.m2,
+                    q0star=params.q0star - eps,
+                    alpha=a,
+                    phi0=params.phi0,
+                    tc=params.tc,
+                    tol=params.tol,
+                )
+                p_hi = PhaseParams(
+                    m1=params.m1,
+                    m2=params.m2,
+                    q0star=params.q0star + eps,
+                    alpha=a,
+                    phi0=params.phi0,
+                    tc=params.tc,
+                    tol=params.tol,
+                )
                 phi_lo = solve_mcgt(f, p_lo)
                 phi_hi = solve_mcgt(f, p_hi)
                 dphi_dq = (phi_hi - phi_lo) / (2 * eps)
-                fisher = (dphi_dq ** 2) / (sigma_phase ** 2) if sigma_phase > 0 else np.full_like(dphi_dq, np.nan)
-                rows.append(pd.DataFrame({"f_Hz": f, "param2": np.full_like(f, a), "fisher_value": fisher}))
+                fisher = (
+                    (dphi_dq**2) / (sigma_phase**2)
+                    if sigma_phase > 0
+                    else np.full_like(dphi_dq, np.nan)
+                )
+                rows.append(
+                    pd.DataFrame(
+                        {
+                            "f_Hz": f,
+                            "param2": np.full_like(f, a),
+                            "fisher_value": fisher,
+                        }
+                    )
+                )
             fisher_df = pd.concat(rows, ignore_index=True)
             out_fisher = OUT_DIR / "09_fisher_scan2D.csv"
             fisher_df.to_csv(out_fisher, index=False)
@@ -584,17 +824,34 @@ def main() -> None:
     meta = {
         "ini": str(args.ini),
         "params": asdict(params),
-        "reference": {"csv": str(REF_CSV), "meta": str(REF_META), "source_tag": ref_tag},
-        "grid_used": {"fmin_Hz": float(np.nanmin(f)), "fmax_Hz": float(np.nanmax(f)), "dlog10": float(dlog), "n_points_used": int(len(f))},
+        "reference": {
+            "csv": str(REF_CSV),
+            "meta": str(REF_META),
+            "source_tag": ref_tag,
+        },
+        "grid_used": {
+            "fmin_Hz": float(np.nanmin(f)),
+            "fmax_Hz": float(np.nanmax(f)),
+            "dlog10": float(dlog),
+            "n_points_used": int(len(f)),
+        },
         "metrics_window_Hz": [float(mw_lo), float(mw_hi)],
         "metrics_active": {
             "mean_abs_20_300": float(mean_abs),
-            "max_abs_20_300":  float(max_abs),
-            "p95_abs_20_300":  float(p95_abs),
+            "max_abs_20_300": float(max_abs),
+            "p95_abs_20_300": float(p95_abs),
             "variant": active_variant,
         },
-        "metrics_raw": {"mean_abs_20_300": float(mean_raw), "max_abs_20_300": float(max_raw), "p95_abs_20_300": float(p95_raw)},
-        "metrics_cal": {"mean_abs_20_300": float(mean_cal), "max_abs_20_300": float(max_cal), "p95_abs_20_300": float(p95_cal)},
+        "metrics_raw": {
+            "mean_abs_20_300": float(mean_raw),
+            "max_abs_20_300": float(max_raw),
+            "p95_abs_20_300": float(p95_raw),
+        },
+        "metrics_cal": {
+            "mean_abs_20_300": float(mean_cal),
+            "max_abs_20_300": float(max_cal),
+            "p95_abs_20_300": float(p95_cal),
+        },
         "calibration": {
             "enabled": bool(calib_enabled),
             "mode": args.calibrate,
@@ -612,21 +869,30 @@ def main() -> None:
         },
         "outputs": {
             "phases_mcgt_csv": str(out_mcgt) if out_mcgt.exists() else None,
-            "diff_phase_csv": str(out_diff) if args.export_diff and out_diff.exists() else None,
-            "comparison_milestones_csv": str(out_jalons_cmp) if out_jalons_cmp else None,
+            "diff_phase_csv": str(out_diff)
+            if args.export_diff and out_diff.exists()
+            else None,
+            "comparison_milestones_csv": str(out_jalons_cmp)
+            if out_jalons_cmp
+            else None,
             "fisher_scan2D_csv": str(out_fisher) if out_fisher else None,
         },
         "repro": {
             "git_hash": git_hash(),
             "python": "{}.{}.{}".format(*sys.version_info[:3]),
-            "libs": {"numpy": np.__version__, "pandas": pd.__version__}
-        }
+            "libs": {"numpy": np.__version__, "pandas": pd.__version__},
+        },
     }
     (OUT_DIR / "09_metrics_phase.json").write_text(json.dumps(meta, indent=2))
     logger.info("Écrit → %s", OUT_DIR / "09_metrics_phase.json")

-    logger.info("Terminé. Variante ACTIVE: %s | p95(|Δφ|)@%g–%g = %.6f rad",
-                active_variant, mw_lo, mw_hi, p95_abs)
+    logger.info(
+        "Terminé. Variante ACTIVE: %s | p95(|Δφ|)@%g–%g = %.6f rad",
+        active_variant,
+        mw_lo,
+        mw_hi,
+        p95_abs,
+    )


 if __name__ == "__main__":
diff --git a/zz-scripts/chapter09/generate_mcgt_raw_phase.py b/zz-scripts/chapter09/generate_mcgt_raw_phase.py
index 0135e06..8fa5429 100755
--- a/zz-scripts/chapter09/generate_mcgt_raw_phase.py
+++ b/zz-scripts/chapter09/generate_mcgt_raw_phase.py
@@ -17,16 +17,18 @@ from pathlib import Path
 import numpy as np
 from dataclasses import dataclass

+
 # --- Dataclass de paramètres ------------------------------------------------
 @dataclass
 class PhaseParams:
-    m1:      float
-    m2:      float
-    q0star:  float
-    alpha:   float
-    phi0:    float = 0.0
-    tc:      float = 0.0
-    tol:     float = 1e-8
+    m1: float
+    m2: float
+    q0star: float
+    alpha: float
+    phi0: float = 0.0
+    tc: float = 0.0
+    tol: float = 1e-8
+

 # --- Fonctions utilitaires ---------------------------------------------------
 def build_loglin_grid(fmin: float, fmax: float, dlog: float) -> np.ndarray:
@@ -35,43 +37,55 @@ def build_loglin_grid(fmin: float, fmax: float, dlog: float) -> np.ndarray:
     N = int(np.floor((logf_max - logf_min) / dlog)) + 1
     return 10 ** (logf_min + np.arange(N) * dlog)

+
 def check_log_spacing(grid: np.ndarray, atol: float = 1e-12) -> bool:
-    logg  = np.log10(grid)
+    logg = np.log10(grid)
     diffs = np.diff(logg)
     return np.allclose(diffs, diffs[0], atol=atol, rtol=0.0)

+
 # --- Coefficients PN jusqu’à 3.5PN (simplifiés) --------------------------------
 _CPN = {
-    0:  1,
-    2:  (3715/756 + 55/9),
-    3:  -16 * np.pi,
-    4:  (15293365/508032 + 27145/504 + 3085/72),
-    5:  np.pi * (38645/756 - 65/9) * (1 + 3 * np.log(np.pi)),
-    6:  (11583231236531/4694215680 - 640/3 * np.pi**2 - 6848/21 * np.log(4*np.pi)),
-    7:  np.pi * (77096675/254016 + 378515/1512),
+    0: 1,
+    2: (3715 / 756 + 55 / 9),
+    3: -16 * np.pi,
+    4: (15293365 / 508032 + 27145 / 504 + 3085 / 72),
+    5: np.pi * (38645 / 756 - 65 / 9) * (1 + 3 * np.log(np.pi)),
+    6: (
+        11583231236531 / 4694215680 - 640 / 3 * np.pi**2 - 6848 / 21 * np.log(4 * np.pi)
+    ),
+    7: np.pi * (77096675 / 254016 + 378515 / 1512),
 }

+
 def _symmetric_eta(m1: float, m2: float) -> float:
-    return (m1 * m2) / (m1 + m2)**2
+    return (m1 * m2) / (m1 + m2) ** 2
+

 # --- Phase GR (SPA) ----------------------------------------------------------
 def phi_gr(freqs: np.ndarray, p: PhaseParams) -> np.ndarray:
     """Phase fréquentielle GR via SPA jusqu’à 3.5PN."""
     M_s = (p.m1 + p.m2) * 4.925490947e-6  # conversion M☉ → s
     eta = _symmetric_eta(p.m1, p.m2)
-    v   = (np.pi * M_s * freqs) ** (1/3)
+    v = (np.pi * M_s * freqs) ** (1 / 3)
     series = np.zeros_like(freqs)
     for k, c_k in _CPN.items():
         series += c_k * v**k
-    prefac = 3 / (128 * eta) * v**(-5)
-    return 2*np.pi*freqs*p.tc - p.phi0 - np.pi/4 + prefac * series
+    prefac = 3 / (128 * eta) * v ** (-5)
+    return 2 * np.pi * freqs * p.tc - p.phi0 - np.pi / 4 + prefac * series
+

 # --- Correcteur analytique ---------------------------------------------------
-def corr_phase(freqs: np.ndarray, fmin: float, q0star: float, alpha: float) -> np.ndarray:
+def corr_phase(
+    freqs: np.ndarray, fmin: float, q0star: float, alpha: float
+) -> np.ndarray:
     """Correction analytique pour δt = q0star * f^(−alpha)."""
     if np.isclose(alpha, 1.0):
         return 2 * np.pi * q0star * np.log(freqs / fmin)
-    return (2 * np.pi * q0star / (1 - alpha)) * (freqs**(1 - alpha) - fmin**(1 - alpha))
+    return (2 * np.pi * q0star / (1 - alpha)) * (
+        freqs ** (1 - alpha) - fmin ** (1 - alpha)
+    )
+

 # --- Solveur MCGT ------------------------------------------------------------
 def solve_mcgt(freqs: np.ndarray, p: PhaseParams, fmin: float = None) -> np.ndarray:
@@ -81,43 +95,52 @@ def solve_mcgt(freqs: np.ndarray, p: PhaseParams, fmin: float = None) -> np.ndar
     if not np.all(freqs[1:] > freqs[:-1]):
         raise ValueError("La grille de fréquences doit être strictement croissante.")
     phi_gr_vals = phi_gr(freqs, p)
-    delta_phi   = corr_phase(freqs, f0, p.q0star, p.alpha)
+    delta_phi = corr_phase(freqs, f0, p.q0star, p.alpha)
     return phi_gr_vals - delta_phi

+
 # --- CLI & logging -----------------------------------------------------------
 def parse_args():
     parser = argparse.ArgumentParser(
         description="Génère les phases brutes MCGT (09_phase_run_*.dat)"
     )
-    parser.add_argument('-i', '--ini',
-        type=Path, required=True,
-        help="Chemin vers gw_phase.ini")
-    parser.add_argument('--dry-run', action='store_true',
-        help="Affiche un aperçu et n'écrit pas les fichiers")
-    parser.add_argument('--export-raw', action='store_true',
-        help="Exporter le CSV et le meta-JSON")
-    parser.add_argument('--npts', type=int,
-        help="Override du nombre de points de la grille")
-    parser.add_argument('--log-level',
-        choices=['DEBUG','INFO','WARNING','ERROR'],
-        default='INFO',
-        help="Niveau de verbosité")
-    parser.add_argument('--log-file', type=Path,
-        help="Chemin vers un fichier de log")
+    parser.add_argument(
+        "-i", "--ini", type=Path, required=True, help="Chemin vers gw_phase.ini"
+    )
+    parser.add_argument(
+        "--dry-run",
+        action="store_true",
+        help="Affiche un aperçu et n'écrit pas les fichiers",
+    )
+    parser.add_argument(
+        "--export-raw", action="store_true", help="Exporter le CSV et le meta-JSON"
+    )
+    parser.add_argument(
+        "--npts", type=int, help="Override du nombre de points de la grille"
+    )
+    parser.add_argument(
+        "--log-level",
+        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
+        default="INFO",
+        help="Niveau de verbosité",
+    )
+    parser.add_argument("--log-file", type=Path, help="Chemin vers un fichier de log")
     return parser.parse_args()

+
 def setup_logger(level: str, logfile: Path = None):
     handlers = [logging.StreamHandler()]
     if logfile:
-        handlers.append(logging.FileHandler(logfile, encoding='utf-8'))
+        handlers.append(logging.FileHandler(logfile, encoding="utf-8"))
     logging.basicConfig(
         level=getattr(logging, level),
-        format='[%(asctime)s] [%(levelname)s] %(message)s',
-        datefmt='%Y-%m-%d %H:%M:%S',
-        handlers=handlers
+        format="[%(asctime)s] [%(levelname)s] %(message)s",
+        datefmt="%Y-%m-%d %H:%M:%S",
+        handlers=handlers,
     )
     return logging.getLogger(__name__)

+
 # --- Script principal --------------------------------------------------------
 def main():
     args = parse_args()
@@ -125,22 +148,22 @@ def main():

     # Lecture de la config
     config = configparser.ConfigParser(
-        inline_comment_prefixes=('#',';'), interpolation=None
+        inline_comment_prefixes=("#", ";"), interpolation=None
     )
     config.read(args.ini)
-    scan = config['scan']
+    scan = config["scan"]

     # Extraction des paramètres
-    fmin   = scan.getfloat('fmin')
-    fmax   = scan.getfloat('fmax')
-    dlog   = scan.getfloat('dlog')
-    q0star = scan.getfloat('q0star')
-    alpha  = scan.getfloat('alpha')
-    m1     = scan.getfloat('m1')
-    m2     = scan.getfloat('m2')
-    phi0   = scan.getfloat('phi0')
-    tc     = scan.getfloat('tc')
-    tol    = scan.getfloat('tol')
+    fmin = scan.getfloat("fmin")
+    fmax = scan.getfloat("fmax")
+    dlog = scan.getfloat("dlog")
+    q0star = scan.getfloat("q0star")
+    alpha = scan.getfloat("alpha")
+    m1 = scan.getfloat("m1")
+    m2 = scan.getfloat("m2")
+    phi0 = scan.getfloat("phi0")
+    tc = scan.getfloat("tc")
+    tol = scan.getfloat("tol")

     # Instanciation des paramètres
     params = PhaseParams(m1, m2, q0star, alpha, phi0, tc, tol)
@@ -163,7 +186,7 @@ def main():
     out_dir.mkdir(parents=True, exist_ok=True)
     tag = f"q0star{q0star:.2f}_alpha{alpha:.2f}"
     output_csv = out_dir / f"09_phase_run_{tag}.dat"
-    meta_json  = output_csv.with_suffix(".meta.json")
+    meta_json = output_csv.with_suffix(".meta.json")

     # Dry-run : aperçu
     if args.dry_run:
@@ -175,7 +198,7 @@ def main():

     # Export du CSV
     if args.export_raw:
-        with open(output_csv, 'w', encoding='utf-8') as f:
+        with open(output_csv, "w", encoding="utf-8") as f:
             f.write("# sep=,\n")
             f.write("# q0star, alpha, f_Hz, phi_mcgt\n")
             for f_val, phi_val in zip(freqs, phi_mcgt):
@@ -184,26 +207,27 @@ def main():

         # Écriture du meta-JSON
         meta_data = {
-            "timestamp":  datetime.utcnow().isoformat() + "Z",
-            "q0star":     q0star,
-            "alpha":      alpha,
-            "m1":         m1,
-            "m2":         m2,
-            "phi0":       phi0,
-            "tc":         tc,
-            "tol":        tol,
-            "fmin_Hz":    float(freqs[0]),
-            "fmax_Hz":    float(freqs[-1]),
-            "dlog":       dlog,
-            "n_points":   len(freqs),
-            "log_level":  args.log_level,
+            "timestamp": datetime.utcnow().isoformat() + "Z",
+            "q0star": q0star,
+            "alpha": alpha,
+            "m1": m1,
+            "m2": m2,
+            "phi0": phi0,
+            "tc": tc,
+            "tol": tol,
+            "fmin_Hz": float(freqs[0]),
+            "fmax_Hz": float(freqs[-1]),
+            "dlog": dlog,
+            "n_points": len(freqs),
+            "log_level": args.log_level,
         }
-        with open(meta_json, 'w', encoding='utf-8') as f_meta:
+        with open(meta_json, "w", encoding="utf-8") as f_meta:
             json.dump(meta_data, f_meta, indent=2)
         logger.info("Meta-JSON exporté → %s", meta_json)

     # Message de fin
     logger.info("Génération terminée avec succès.")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter09/opt_poly_rebranch.py b/zz-scripts/chapter09/opt_poly_rebranch.py
index 05c57c4..744e56e 100755
--- a/zz-scripts/chapter09/opt_poly_rebranch.py
+++ b/zz-scripts/chapter09/opt_poly_rebranch.py
@@ -4,44 +4,87 @@
 Recherche automatique : polynôme (sur unwrap diff) + rebranch entier k optimisé.
 Applique la meilleure correction au CSV (backup) et met à jour le JSON métriques.
 """
+
 from __future__ import annotations
-import argparse, json, os
+import argparse
+import json
 from pathlib import Path
-from typing import List, Tuple
+from typing import Tuple
 import numpy as np
 import pandas as pd

-def p95(a):
+
+def p95(a):
     a = np.asarray(a, float)
     a = a[np.isfinite(a)]
     return float(np.percentile(a, 95.0)) if a.size else float("nan")

+
 def parse_args():
     ap = argparse.ArgumentParser()
-    ap.add_argument("--csv", type=Path, required=True, help="Input CSV (use prepoly backup as start)")
+    ap.add_argument(
+        "--csv",
+        type=Path,
+        required=True,
+        help="Input CSV (use prepoly backup as start)",
+    )
     ap.add_argument("--meta", type=Path, required=True, help="JSON meta to update")
-    ap.add_argument("--fit-window", nargs=2, type=float, default=[30.0,250.0])
-    ap.add_argument("--metrics-window", nargs=2, type=float, default=[20.0,300.0])
-    ap.add_argument("--degrees", nargs="+", type=int, default=[3,4,5])
-    ap.add_argument("--bases", nargs="+", choices=["log10","hz"], default=["log10","hz"])
-    ap.add_argument("--k-range", nargs=2, type=int, default=[-100,100], help="Integer k search range (inclusive)")
-    ap.add_argument("--out-csv", type=Path, default=None, help="Output CSV (if omitted, overwrite --csv after backup)")
-    ap.add_argument("--out-poly-json", type=Path, default=None, help="Write detailed poly candidate JSON")
-    ap.add_argument("--backup", action="store_true", help="Write .backup before overwriting CSV")
+    ap.add_argument("--fit-window", nargs=2, type=float, default=[30.0, 250.0])
+    ap.add_argument("--metrics-window", nargs=2, type=float, default=[20.0, 300.0])
+    ap.add_argument("--degrees", nargs="+", type=int, default=[3, 4, 5])
+    ap.add_argument(
+        "--bases", nargs="+", choices=["log10", "hz"], default=["log10", "hz"]
+    )
+    ap.add_argument(
+        "--k-range",
+        nargs=2,
+        type=int,
+        default=[-100, 100],
+        help="Integer k search range (inclusive)",
+    )
+    ap.add_argument(
+        "--out-csv",
+        type=Path,
+        default=None,
+        help="Output CSV (if omitted, overwrite --csv after backup)",
+    )
+    ap.add_argument(
+        "--out-poly-json",
+        type=Path,
+        default=None,
+        help="Write detailed poly candidate JSON",
+    )
+    ap.add_argument(
+        "--backup", action="store_true", help="Write .backup before overwriting CSV"
+    )
     return ap.parse_args()

+
 def basis_x(f: np.ndarray, basis: str) -> np.ndarray:
     if basis == "log10":
         return np.log10(f)
     return f.copy()

-def eval_candidate(phi_cal: np.ndarray, phi_ref: np.ndarray, trend: np.ndarray, k:int, f:np.ndarray, m_eval:np.ndarray) -> Tuple[float,float,float]:
+
+def eval_candidate(
+    phi_cal: np.ndarray,
+    phi_ref: np.ndarray,
+    trend: np.ndarray,
+    k: int,
+    f: np.ndarray,
+    m_eval: np.ndarray,
+) -> Tuple[float, float, float]:
     # apply: candidate = phi_cal - trend - k*2pi
     cand = phi_cal - trend - (k * 2.0 * np.pi)
     # robust diff = unwrap(cand - ref)
     d = np.abs(np.unwrap((cand - phi_ref).astype(float)))
     d_eval = d[m_eval]
-    return float(np.nanpercentile(d_eval,95)) if d_eval.size else float("nan"), float(np.nanmean(d_eval)) if d_eval.size else float("nan"), float(np.nanmax(d_eval)) if d_eval.size else float("nan")
+    return (
+        float(np.nanpercentile(d_eval, 95)) if d_eval.size else float("nan"),
+        float(np.nanmean(d_eval)) if d_eval.size else float("nan"),
+        float(np.nanmax(d_eval)) if d_eval.size else float("nan"),
+    )
+

 def main():
     args = parse_args()
@@ -67,8 +110,20 @@ def main():

     f_lo_fit, f_hi_fit = sorted(map(float, args.fit_window))
     f_lo_eval, f_hi_eval = sorted(map(float, args.metrics_window))
-    m_fit = (f_all >= f_lo_fit) & (f_all <= f_hi_fit) & np.isfinite(f_all) & np.isfinite(phi_cal_all) & np.isfinite(phi_ref_all)
-    m_eval = (f_all >= f_lo_eval) & (f_all <= f_hi_eval) & np.isfinite(f_all) & np.isfinite(phi_cal_all) & np.isfinite(phi_ref_all)
+    m_fit = (
+        (f_all >= f_lo_fit)
+        & (f_all <= f_hi_fit)
+        & np.isfinite(f_all)
+        & np.isfinite(phi_cal_all)
+        & np.isfinite(phi_ref_all)
+    )
+    m_eval = (
+        (f_all >= f_lo_eval)
+        & (f_all <= f_hi_eval)
+        & np.isfinite(f_all)
+        & np.isfinite(phi_cal_all)
+        & np.isfinite(phi_ref_all)
+    )

     if not m_fit.any():
         raise SystemExit("Aucun point pour le fit dans la fenêtre demandée.")
@@ -91,33 +146,58 @@ def main():
         r_fit = r_unwrap[mask_x]
         for deg in args.degrees:
             try:
-                c_desc = np.polyfit(x_fit, r_fit, deg)   # coeffs descending
+                c_desc = np.polyfit(x_fit, r_fit, deg)  # coeffs descending
             except Exception as e:
                 print("polyfit failed:", e)
                 continue
             trend = np.polyval(c_desc, x)
             # scan k
             best_k_for_this = None
-            for k in range(kmin, kmax+1):
-                p95_val, mean_val, max_val = eval_candidate(phi_cal_all, phi_ref_all, trend, k, f_all, m_eval)
-                cand = dict(basis=basis, degree=deg, coeff_desc=[float(x) for x in c_desc.tolist()],
-                            k=k, p95=p95_val, mean=mean_val, max=max_val)
+            for k in range(kmin, kmax + 1):
+                p95_val, mean_val, max_val = eval_candidate(
+                    phi_cal_all, phi_ref_all, trend, k, f_all, m_eval
+                )
+                cand = dict(
+                    basis=basis,
+                    degree=deg,
+                    coeff_desc=[float(x) for x in c_desc.tolist()],
+                    k=k,
+                    p95=p95_val,
+                    mean=mean_val,
+                    max=max_val,
+                )
                 candidates.append(cand)
-                if best_k_for_this is None or (p95_val < best_k_for_this[0] or (p95_val==best_k_for_this[0] and mean_val < best_k_for_this[1])):
+                if best_k_for_this is None or (
+                    p95_val < best_k_for_this[0]
+                    or (p95_val == best_k_for_this[0] and mean_val < best_k_for_this[1])
+                ):
                     best_k_for_this = (p95_val, mean_val, max_val, k)
             # update global best
             bk = best_k_for_this
             if bk is not None:
                 p95_val, mean_val, max_val, kopt = bk
-                if best is None or (p95_val < best["p95"] or (p95_val == best["p95"] and mean_val < best["mean"])):
-                    best = {"basis":basis, "degree":deg, "coeff_desc":[float(x) for x in c_desc.tolist()],
-                            "k":int(kopt), "p95":float(p95_val), "mean":float(mean_val), "max":float(max_val)}
+                if best is None or (
+                    p95_val < best["p95"]
+                    or (p95_val == best["p95"] and mean_val < best["mean"])
+                ):
+                    best = {
+                        "basis": basis,
+                        "degree": deg,
+                        "coeff_desc": [float(x) for x in c_desc.tolist()],
+                        "k": int(kopt),
+                        "p95": float(p95_val),
+                        "mean": float(mean_val),
+                        "max": float(max_val),
+                    }

     if best is None:
         raise SystemExit("Aucune solution candidate trouvée.")

     # apply best to data and save
-    basis = best["basis"]; deg = best["degree"]; c_desc = np.asarray(best["coeff_desc"], float); k = int(best["k"])
+    basis = best["basis"]
+    deg = best["degree"]
+    c_desc = np.asarray(best["coeff_desc"], float)
+    k = int(best["k"])
     x_all = basis_x(f_all, basis)
     trend_all = np.polyval(c_desc, x_all)
     phi_new = phi_cal_all - trend_all - (k * 2.0 * np.pi)
@@ -133,7 +213,7 @@ def main():
     d = np.abs(np.unwrap((phi_new - phi_ref_all).astype(float)))[m_eval]
     metrics_active = {
         "mean_abs_20_300": float(np.nanmean(d)),
-        "p95_abs_20_300": float(np.nanpercentile(d,95)),
+        "p95_abs_20_300": float(np.nanpercentile(d, 95)),
         "max_abs_20_300": float(np.nanmax(d)),
         "variant": f"calibrated+poly_deg{deg}_{basis}_fit{int(f_lo_fit)}-{int(f_hi_fit)}_k{k}",
     }
@@ -142,20 +222,26 @@ def main():
     meta = json.load(open(META)) if META.exists() else {}
     meta["metrics_active"] = metrics_active
     meta.setdefault("poly_correction", {})  # store details
-    meta["poly_correction"].update({
-        "applied": True,
-        "from_column": "phi_mcgt_cal",
-        "basis": basis,
-        "degree": deg,
-        "fit_window_Hz": [float(f_lo_fit), float(f_hi_fit)],
-        "metrics_window_Hz": [float(f_lo_eval), float(f_hi_eval)],
-        "coeff_desc": [float(x) for x in c_desc.tolist()],
-        "k_cycles": k
-    })
+    meta["poly_correction"].update(
+        {
+            "applied": True,
+            "from_column": "phi_mcgt_cal",
+            "basis": basis,
+            "degree": deg,
+            "fit_window_Hz": [float(f_lo_fit), float(f_hi_fit)],
+            "metrics_window_Hz": [float(f_lo_eval), float(f_hi_eval)],
+            "coeff_desc": [float(x) for x in c_desc.tolist()],
+            "k_cycles": k,
+        }
+    )
     open(META, "w").write(json.dumps(meta, indent=2))

     if args.out_poly_json:
-        json.dump({"candidates": candidates, "best": best}, open(args.out_poly_json, "w"), indent=2)
+        json.dump(
+            {"candidates": candidates, "best": best},
+            open(args.out_poly_json, "w"),
+            indent=2,
+        )

     print("=== BEST SOLUTION ===")
     print(best)
@@ -164,5 +250,6 @@ def main():
     if args.backup:
         print("Backup kept at:", str(bak))

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter09/plot_fig01_phase_overlay.py b/zz-scripts/chapter09/plot_fig01_phase_overlay.py
index 291ca52..ed77cc7 100755
--- a/zz-scripts/chapter09/plot_fig01_phase_overlay.py
+++ b/zz-scripts/chapter09/plot_fig01_phase_overlay.py
@@ -10,31 +10,41 @@ Figure 01 — Overlay φ_ref vs φ_MCGT + inset résidu (version corrigée)
 """

 from __future__ import annotations
-import argparse, configparser, json, logging
+import argparse
+import configparser
+import json
+import logging
 from pathlib import Path
-import numpy as np, pandas as pd
+import numpy as np
+import pandas as pd
 import matplotlib.pyplot as plt
 from matplotlib.lines import Line2D

-DEF_IN   = Path("zz-data/chapter09/09_phases_mcgt.csv")
+DEF_IN = Path("zz-data/chapter09/09_phases_mcgt.csv")
 DEF_META = Path("zz-data/chapter09/09_metrics_phase.json")
-DEF_INI  = Path("zz-configuration/gw_phase.ini")
-DEF_OUT  = Path("zz-figures/chapter09/fig_01_phase_overlay.png")
+DEF_INI = Path("zz-configuration/gw_phase.ini")
+DEF_OUT = Path("zz-figures/chapter09/fig_01_phase_overlay.png")
+

 # ---------------- utils
 def setup_logger(level="INFO"):
-    logging.basicConfig(level=getattr(logging, level.upper(), logging.INFO),
-                        format='[%(asctime)s] [%(levelname)s] %(message)s',
-                        datefmt='%Y-%m-%d %H:%M:%S')
+    logging.basicConfig(
+        level=getattr(logging, level.upper(), logging.INFO),
+        format="[%(asctime)s] [%(levelname)s] %(message)s",
+        datefmt="%Y-%m-%d %H:%M:%S",
+    )
     return logging.getLogger("fig01")

+
 def p95(a: np.ndarray) -> float:
     a = np.asarray(a, float)
     a = a[np.isfinite(a)]
     return float(np.percentile(a, 95.0)) if a.size else float("nan")

+
 def principal_diff(a: np.ndarray, b: np.ndarray) -> np.ndarray:
-    return (a - b + np.pi) % (2.0*np.pi) - np.pi
+    return (a - b + np.pi) % (2.0 * np.pi) - np.pi
+

 def enforce_monotone_freq(f, arrays, log):
     f = np.asarray(f, float)
@@ -43,89 +53,141 @@ def enforce_monotone_freq(f, arrays, log):
     keep = np.ones_like(f_sorted, bool)
     keep[1:] = np.diff(f_sorted) > 0
     if np.any(~keep):
-        log.warning("Fréquences dupliquées → %d doublons supprimés.", int((~keep).sum()))
+        log.warning(
+            "Fréquences dupliquées → %d doublons supprimés.", int((~keep).sum())
+        )
     out = {k: np.asarray(v, float)[order][keep] for k, v in arrays.items()}
     return f_sorted[keep], out

+
 def mask_flat_tail(y: np.ndarray, min_run=3, atol=1e-12):
     y = np.asarray(y, float)
     n = y.size
-    if n < min_run+1: return y, n-1
-    run, last = 0, n-1
-    for i in range(n-1, 0, -1):
-        if np.isfinite(y[i]) and np.isfinite(y[i-1]) and abs(y[i]-y[i-1]) < atol:
+    if n < min_run + 1:
+        return y, n - 1
+    run, last = 0, n - 1
+    for i in range(n - 1, 0, -1):
+        if np.isfinite(y[i]) and np.isfinite(y[i - 1]) and abs(y[i] - y[i - 1]) < atol:
             run += 1
             if run >= min_run:
                 last = i - run
                 break
         else:
             run = 0
-    if run >= min_run and last < n-1:
-        yy = y.copy(); yy[last+1:] = np.nan
+    if run >= min_run and last < n - 1:
+        yy = y.copy()
+        yy[last + 1 :] = np.nan
         return yy, last
-    return y, n-1
+    return y, n - 1
+

 def pick_anchor_frequency(f: np.ndarray, fmin: float, fmax: float) -> float:
-    if 100.0 >= fmin and 100.0 <= fmax: return 100.0
-    return float(np.exp(0.5*(np.log(max(fmin,1e-12))+np.log(max(fmax,1e-12)))))
+    if 100.0 >= fmin and 100.0 <= fmax:
+        return 100.0
+    return float(np.exp(0.5 * (np.log(max(fmin, 1e-12)) + np.log(max(fmax, 1e-12)))))
+

 def interp_at(x, xp, fp):
-    xp = np.asarray(xp, float); fp = np.asarray(fp, float)
+    xp = np.asarray(xp, float)
+    fp = np.asarray(fp, float)
     m = np.isfinite(xp) & np.isfinite(fp)
     return float(np.interp(x, xp[m], fp[m])) if np.any(m) else float("nan")

+
 def load_meta_and_ini(meta_path: Path, ini_path: Path, log):
     grid = {"fmin_Hz": 10.0, "fmax_Hz": 2048.0, "dlog10": 0.01}
-    calib = {"enabled": False, "model": "phi0,tc", "weight": "1/f2",
-             "phi0_hat_rad": 0.0, "tc_hat_s": 0.0,
-             "window_Hz": [20.0,300.0], "used_window_Hz": None}
+    calib = {
+        "enabled": False,
+        "model": "phi0,tc",
+        "weight": "1/f2",
+        "phi0_hat_rad": 0.0,
+        "tc_hat_s": 0.0,
+        "window_Hz": [20.0, 300.0],
+        "used_window_Hz": None,
+    }
     variant = None
     if meta_path.exists():
         try:
             meta = json.loads(meta_path.read_text())
             c = meta.get("calibration", {}) or {}
             calib["enabled"] = bool(c.get("enabled", calib["enabled"]))
-            calib["model"]   = str(c.get("mode", c.get("model_used", calib["model"])))
+            calib["model"] = str(c.get("mode", c.get("model_used", calib["model"])))
             calib["phi0_hat_rad"] = float(c.get("phi0_hat_rad", calib["phi0_hat_rad"]))
-            calib["tc_hat_s"]     = float(c.get("tc_hat_s", calib["tc_hat_s"]))
-            if "window_Hz" in c and isinstance(c["window_Hz"], (list,tuple)) and len(c["window_Hz"])>=2:
-                calib["window_Hz"] = [float(c["window_Hz"][0]), float(c["window_Hz"][1])]
-            if "used_window_Hz" in c and isinstance(c["used_window_Hz"], (list,tuple)) and len(c["used_window_Hz"])>=2:
-                calib["used_window_Hz"] = [float(c["used_window_Hz"][0]), float(c["used_window_Hz"][1])]
+            calib["tc_hat_s"] = float(c.get("tc_hat_s", calib["tc_hat_s"]))
+            if (
+                "window_Hz" in c
+                and isinstance(c["window_Hz"], (list, tuple))
+                and len(c["window_Hz"]) >= 2
+            ):
+                calib["window_Hz"] = [
+                    float(c["window_Hz"][0]),
+                    float(c["window_Hz"][1]),
+                ]
+            if (
+                "used_window_Hz" in c
+                and isinstance(c["used_window_Hz"], (list, tuple))
+                and len(c["used_window_Hz"]) >= 2
+            ):
+                calib["used_window_Hz"] = [
+                    float(c["used_window_Hz"][0]),
+                    float(c["used_window_Hz"][1]),
+                ]
             variant = (meta.get("metrics_active", {}) or {}).get("variant", None)
         except Exception as e:
             log.warning("Lecture JSON méta échouée (%s).", e)
     if ini_path.exists():
         try:
-            cp = configparser.ConfigParser(inline_comment_prefixes=('#',';'), interpolation=None)
+            cp = configparser.ConfigParser(
+                inline_comment_prefixes=("#", ";"), interpolation=None
+            )
             cp.read(ini_path)
             if "scan" in cp:
                 s = cp["scan"]
                 grid["fmin_Hz"] = s.getfloat("fmin", fallback=grid["fmin_Hz"])
                 grid["fmax_Hz"] = s.getfloat("fmax", fallback=grid["fmax_Hz"])
-                grid["dlog10"]  = s.getfloat("dlog", fallback=grid["dlog10"])
+                grid["dlog10"] = s.getfloat("dlog", fallback=grid["dlog10"])
         except Exception as e:
             log.warning("Lecture INI échouée (%s).", e)
     return grid, calib, variant

+
 # ---------------- CLI
 def parse_args():
-    ap = argparse.ArgumentParser(description="Figure 01 — Overlay φ_ref vs φ_MCGT + inset résidu")
-    ap.add_argument("--csv",  type=Path, default=DEF_IN)
+    ap = argparse.ArgumentParser(
+        description="Figure 01 — Overlay φ_ref vs φ_MCGT + inset résidu"
+    )
+    ap.add_argument("--csv", type=Path, default=DEF_IN)
     ap.add_argument("--meta", type=Path, default=DEF_META)
-    ap.add_argument("--ini",  type=Path, default=DEF_INI)
-    ap.add_argument("--out",  type=Path, default=DEF_OUT)
-    ap.add_argument("--display-variant", choices=["auto","phi_mcgt","phi_mcgt_cal","phi_mcgt_raw"],
-                    default="auto", help="Variante affichée (auto: phi_mcgt > cal > raw)")
-    ap.add_argument("--force-fit", action="store_true", help="Forcer un fit visuel même si variante *cal*")
-    ap.add_argument("--shade", nargs=2, type=float, default=[20.0,300.0], metavar=("F1","F2"))
+    ap.add_argument("--ini", type=Path, default=DEF_INI)
+    ap.add_argument("--out", type=Path, default=DEF_OUT)
+    ap.add_argument(
+        "--display-variant",
+        choices=["auto", "phi_mcgt", "phi_mcgt_cal", "phi_mcgt_raw"],
+        default="auto",
+        help="Variante affichée (auto: phi_mcgt > cal > raw)",
+    )
+    ap.add_argument(
+        "--force-fit",
+        action="store_true",
+        help="Forcer un fit visuel même si variante *cal*",
+    )
+    ap.add_argument(
+        "--shade", nargs=2, type=float, default=[20.0, 300.0], metavar=("F1", "F2")
+    )
     ap.add_argument("--show-residual", action="store_true")
-    ap.add_argument("--anchor-policy", choices=["if-not-calibrated","always","never"], default="if-not-calibrated")
+    ap.add_argument(
+        "--anchor-policy",
+        choices=["if-not-calibrated", "always", "never"],
+        default="if-not-calibrated",
+    )
     ap.add_argument("--dpi", type=int, default=300)
     ap.add_argument("--save-pdf", action="store_true")
-    ap.add_argument("--log-level", choices=["DEBUG","INFO","WARNING","ERROR"], default="INFO")
+    ap.add_argument(
+        "--log-level", choices=["DEBUG", "INFO", "WARNING", "ERROR"], default="INFO"
+    )
     return ap.parse_args()

+
 # ---------------- main
 def main():
     args = parse_args()
@@ -134,66 +196,94 @@ def main():
     if not args.csv.exists():
         raise SystemExit(f"Introuvable : {args.csv}")
     df = pd.read_csv(args.csv)
-    if "f_Hz" not in df.columns: raise SystemExit("Colonne manquante: f_Hz")
+    if "f_Hz" not in df.columns:
+        raise SystemExit("Colonne manquante: f_Hz")

     # Sélection variante (priorité corrigée)
     cols = df.columns.tolist()
     disp = args.display_variant
     if disp == "auto":
-        if "phi_mcgt" in cols: disp = "phi_mcgt"
-        elif "phi_mcgt_cal" in cols: disp = "phi_mcgt_cal"
-        elif "phi_mcgt_raw" in cols: disp = "phi_mcgt_raw"
-        else: raise SystemExit("Aucune colonne phi_mcgt* trouvée.")
-    if disp not in cols: raise SystemExit(f"Colonne {disp} introuvable. Colonnes={cols}")
+        if "phi_mcgt" in cols:
+            disp = "phi_mcgt"
+        elif "phi_mcgt_cal" in cols:
+            disp = "phi_mcgt_cal"
+        elif "phi_mcgt_raw" in cols:
+            disp = "phi_mcgt_raw"
+        else:
+            raise SystemExit("Aucune colonne phi_mcgt* trouvée.")
+    if disp not in cols:
+        raise SystemExit(f"Colonne {disp} introuvable. Colonnes={cols}")

     f_raw = df["f_Hz"].to_numpy(float)
-    phi_ref_raw = df["phi_ref"].to_numpy(float) if "phi_ref" in cols else np.full_like(f_raw, np.nan)
+    phi_ref_raw = (
+        df["phi_ref"].to_numpy(float)
+        if "phi_ref" in cols
+        else np.full_like(f_raw, np.nan)
+    )
     phi_mcg_raw = df[disp].to_numpy(float)

     grid, calib, variant_meta = load_meta_and_ini(args.meta, args.ini, log)
-    log.info("Calibration meta: enabled=%s, model=%s, window=%s",
-             calib["enabled"], calib["model"], calib["window_Hz"])
+    log.info(
+        "Calibration meta: enabled=%s, model=%s, window=%s",
+        calib["enabled"],
+        calib["model"],
+        calib["window_Hz"],
+    )

     # Tri et alignement
-    f, arrs = enforce_monotone_freq(f_raw, {"ref":phi_ref_raw, "mcg":phi_mcg_raw}, log)
-    ref = arrs["ref"]; mcg = arrs["mcg"]
+    f, arrs = enforce_monotone_freq(
+        f_raw, {"ref": phi_ref_raw, "mcg": phi_mcg_raw}, log
+    )
+    ref = arrs["ref"]
+    mcg = arrs["mcg"]

     # Unwrap de la ref pour rendu + masque plateau
     ref_u = np.unwrap(ref) if np.isfinite(ref).any() else ref
     ref_u, last_valid = mask_flat_tail(ref_u, min_run=3, atol=1e-12)
-    if last_valid < ref_u.size-1:
+    if last_valid < ref_u.size - 1:
         log.info("Plateau terminal φ_ref: masquage > f=%.3f Hz", float(f[last_valid]))

     # --- Rebranch canonique (k par médiane des cycles) ---
     f1, f2 = sorted(map(float, args.shade))
-    mask_band = (f>=f1) & (f<=f2) & np.isfinite(ref) & np.isfinite(mcg)
-    if not np.any(mask_band): raise SystemExit("Aucun point dans la bande métriques.")
-    two_pi = 2.0*np.pi
+    mask_band = (f >= f1) & (f <= f2) & np.isfinite(ref) & np.isfinite(mcg)
+    if not np.any(mask_band):
+        raise SystemExit("Aucun point dans la bande métriques.")
+    two_pi = 2.0 * np.pi
     k = int(np.round(np.nanmedian((mcg[mask_band] - ref[mask_band]) / two_pi)))
     log.info("k (médiane des cycles) = %d", k)

-    mcg_rebran = mcg - k*two_pi  # appliqué aussi à l'affichage (clé de la superposition)
+    mcg_rebran = (
+        mcg - k * two_pi
+    )  # appliqué aussi à l'affichage (clé de la superposition)

     # Rendu: unwrap pour lisser visuellement
     mcg_disp = np.unwrap(mcg_rebran) if np.isfinite(mcg_rebran).any() else mcg_rebran

     # Fit visuel (phi0, tc) uniquement si variante non calibrée (ou forcé)
-    variant_is_cal = ("cal" in disp.lower()) or (variant_meta and "cal" in str(variant_meta).lower())
-    do_fit = (args.anchor_policy=="always") or \
-             (args.anchor_policy=="if-not-calibrated" and (not variant_is_cal)) or \
-             args.force_fit
+    variant_is_cal = ("cal" in disp.lower()) or (
+        variant_meta and "cal" in str(variant_meta).lower()
+    )
+    do_fit = (
+        (args.anchor_policy == "always")
+        or (args.anchor_policy == "if-not-calibrated" and (not variant_is_cal))
+        or args.force_fit
+    )
     if do_fit:
         # petit fit linéaire ref_u - mcg_disp ≈ dphi0 + 2π f dtc (poids 1/f²)
         m = mask_band & np.isfinite(ref_u) & np.isfinite(mcg_disp)
         if m.sum() >= 3:
-            ff = f[m]; y = (ref_u[m] - mcg_disp[m])
-            w = 1.0/(ff**2)
-            A = np.vstack([np.ones_like(ff), 2.0*np.pi*ff]).T
-            ATA = (A.T*w) @ A; ATy = (A.T*w) @ y
+            ff = f[m]
+            y = ref_u[m] - mcg_disp[m]
+            w = 1.0 / (ff**2)
+            A = np.vstack([np.ones_like(ff), 2.0 * np.pi * ff]).T
+            ATA = (A.T * w) @ A
+            ATy = (A.T * w) @ y
             try:
                 dphi0, dtc = np.linalg.solve(ATA, ATy)
-                mcg_disp = mcg_disp + dphi0 + (2.0*np.pi)*f*dtc
-                log.info("Fit visuel: dphi0=%.3e rad, dtc=%.3e s", float(dphi0), float(dtc))
+                mcg_disp = mcg_disp + dphi0 + (2.0 * np.pi) * f * dtc
+                log.info(
+                    "Fit visuel: dphi0=%.3e rad, dtc=%.3e s", float(dphi0), float(dtc)
+                )
             except np.linalg.LinAlgError:
                 log.warning("Fit visuel instable, ignoré.")
         else:
@@ -201,12 +291,20 @@ def main():

     # --- Résidu & métriques (principal, après rebranch) ---
     dphi = principal_diff(mcg_rebran, ref)
-    m2 = (f>=f1) & (f<=f2) & np.isfinite(dphi)
+    m2 = (f >= f1) & (f <= f2) & np.isfinite(dphi)
     mean_abs = float(np.nanmean(np.abs(dphi[m2])))
-    p95_abs  = float(p95(np.abs(dphi[m2])))
-    max_abs  = float(np.nanmax(np.abs(dphi[m2])))
-    log.info("|Δφ| %g–%g Hz (après rebranch k=%d): mean=%.3f ; p95=%.3f ; max=%.3f (n=%d)",
-             f1, f2, k, mean_abs, p95_abs, max_abs, int(m2.sum()))
+    p95_abs = float(p95(np.abs(dphi[m2])))
+    max_abs = float(np.nanmax(np.abs(dphi[m2])))
+    log.info(
+        "|Δφ| %g–%g Hz (après rebranch k=%d): mean=%.3f ; p95=%.3f ; max=%.3f (n=%d)",
+        f1,
+        f2,
+        k,
+        mean_abs,
+        p95_abs,
+        max_abs,
+        int(m2.sum()),
+    )

     # ---------------- figure
     args.out.parent.mkdir(parents=True, exist_ok=True)
@@ -214,11 +312,18 @@ def main():
     ax = fig.add_subplot(111)

     ax.plot(f, ref_u, lw=2.4, label=r"$\phi_{\rm ref}$ (IMRPhenomD)", zorder=3)
-    ax.plot(f, mcg_disp, lw=1.8, ls="--",
-            label=fr"$\phi_{{\rm MCGT}}$ (affichée: {disp}, rebranch k={k})", zorder=2)
+    ax.plot(
+        f,
+        mcg_disp,
+        lw=1.8,
+        ls="--",
+        label=rf"$\phi_{{\rm MCGT}}$ (affichée: {disp}, rebranch k={k})",
+        zorder=2,
+    )

     ax.set_xscale("log")
-    xmin = max(f1/5.0, float(np.nanmin(f))*0.98); xmax = float(np.nanmax(f))
+    xmin = max(f1 / 5.0, float(np.nanmin(f)) * 0.98)
+    xmax = float(np.nanmax(f))
     ax.set_xlim(xmin, xmax)
     ax.axvspan(f1, f2, color="0.90", alpha=0.6)
     ax.grid(True, which="both", ls=":", alpha=0.4)
@@ -228,46 +333,68 @@ def main():
     # Légende compacte
     legend_w = 0.50
     bbox = (1.0 - legend_w - 0.02, 0.54, legend_w, 0.42)
-    cal_txt = f"Calage: {calib.get('model','phi0,tc')} (enabled={calib.get('enabled',False)})"
+    cal_txt = (
+        f"Calage: {calib.get('model','phi0,tc')} (enabled={calib.get('enabled',False)})"
+    )
     grid_txt = f"Grille: [{grid['fmin_Hz']:.0f}-{grid['fmax_Hz']:.0f}] Hz, dlog10={grid['dlog10']:.3f}"
     metrics_txt = f"|Δφ| {int(f1)}–{int(f2)} Hz (principal, k={k}): mean={mean_abs:.3f} rad ; p95={p95_abs:.3f} rad"
     handles, labels = ax.get_legend_handles_labels()
-    extra = [Line2D([], [], color='none', label=cal_txt),
-             Line2D([], [], color='none', label=grid_txt),
-             Line2D([], [], color='none', label=metrics_txt)]
-    leg = ax.legend(handles + extra, labels + [cal_txt, grid_txt, metrics_txt],
-                    loc="upper left", bbox_to_anchor=bbox, frameon=True, framealpha=0.95)
-    for t in leg.get_texts(): t.set_fontsize(9)
+    extra = [
+        Line2D([], [], color="none", label=cal_txt),
+        Line2D([], [], color="none", label=grid_txt),
+        Line2D([], [], color="none", label=metrics_txt),
+    ]
+    leg = ax.legend(
+        handles + extra,
+        labels + [cal_txt, grid_txt, metrics_txt],
+        loc="upper left",
+        bbox_to_anchor=bbox,
+        frameon=True,
+        framealpha=0.95,
+    )
+    for t in leg.get_texts():
+        t.set_fontsize(9)

     # Inset résidu (log-log) — cohérent avec métriques
     if args.show_residual and np.isfinite(dphi).any():
         inset = ax.inset_axes([0.60, 0.07, 0.35, 0.32])
-        absd = np.abs(dphi); eps = 1e-12
-        absd = np.where(absd<=0, eps, absd)
+        absd = np.abs(dphi)
+        eps = 1e-12
+        absd = np.where(absd <= 0, eps, absd)
         inset.plot(f, absd, lw=1.1)
-        inset.set_xscale("log"); inset.set_yscale("log")
+        inset.set_xscale("log")
+        inset.set_yscale("log")
         inset.set_xlim(xmin, xmax)
         finite = absd[np.isfinite(absd)]
         if finite.size:
-            ymin = max(eps, float(np.nanmin(finite[finite>0]))*0.9)
-            ymax = float(np.nanmax(finite))*1.1
+            ymin = max(eps, float(np.nanmin(finite[finite > 0])) * 0.9)
+            ymax = float(np.nanmax(finite)) * 1.1
             inset.set_ylim(ymin, ymax)
         inset.set_title("Résidu principal $|\\Delta\\phi|$", fontsize=9)
         inset.grid(True, which="both", ls=":", alpha=0.3)
         inset.text(
-            0.98, 0.02,
+            0.98,
+            0.02,
             f"mean={mean_abs:.2f} rad\np95={p95_abs:.2f} rad\nmax={max_abs:.2f} rad",
-            transform=inset.transAxes, va="bottom", ha="right", fontsize=8,
-            bbox=dict(boxstyle="round", facecolor="white", alpha=0.85)
+            transform=inset.transAxes,
+            va="bottom",
+            ha="right",
+            fontsize=8,
+            bbox=dict(boxstyle="round", facecolor="white", alpha=0.85),
         )

     # Titre & marges
-    ax.set_title("Comparaison des phases $\\phi_{\\rm ref}$ vs $\\phi_{\\rm MCGT}$", fontsize=16, pad=18)
+    ax.set_title(
+        "Comparaison des phases $\\phi_{\\rm ref}$ vs $\\phi_{\\rm MCGT}$",
+        fontsize=16,
+        pad=18,
+    )
     fig.tight_layout(rect=[0.035, 0.035, 0.985, 0.965])
     fig.savefig(args.out, dpi=int(args.dpi), bbox_inches="tight", pad_inches=0.03)
     if args.save_pdf:
         fig.savefig(args.out.with_suffix(".pdf"), dpi=int(args.dpi))
     log.info("Figure écrite → %s", str(args.out))

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter09/plot_fig02_residual_phase.py b/zz-scripts/chapter09/plot_fig02_residual_phase.py
index 0a06976..8e9bd48 100755
--- a/zz-scripts/chapter09/plot_fig02_residual_phase.py
+++ b/zz-scripts/chapter09/plot_fig02_residual_phase.py
@@ -19,6 +19,7 @@ Exemple:
     --dpi 300 --marker-size 3 --line-width 0.9 \
     --gap-thresh-log10 0.12 --log-level INFO
 """
+
 from __future__ import annotations

 import argparse
@@ -84,13 +85,14 @@ def load_meta(meta_path: Path) -> dict:

 def principal_diff(a: np.ndarray, b: np.ndarray) -> np.ndarray:
     """Δφ_principal ∈ (−π, π]"""
-    return (np.asarray(a, float) - np.asarray(b, float) + np.pi) % (2.0*np.pi) - np.pi
+    return (np.asarray(a, float) - np.asarray(b, float) + np.pi) % (2.0 * np.pi) - np.pi


-def k_rebranch_median(phi_m: np.ndarray, phi_r: np.ndarray,
-                      f: np.ndarray, f1: float, f2: float) -> int:
+def k_rebranch_median(
+    phi_m: np.ndarray, phi_r: np.ndarray, f: np.ndarray, f1: float, f2: float
+) -> int:
     """k = round(median((φ_m − φ_r)/2π)) sur [f1,f2]."""
-    two_pi = 2.0*np.pi
+    two_pi = 2.0 * np.pi
     m = (f >= f1) & (f <= f2) & np.isfinite(phi_m) & np.isfinite(phi_r)
     if not np.any(m):
         return 0
@@ -99,16 +101,24 @@ def k_rebranch_median(phi_m: np.ndarray, phi_r: np.ndarray,

 # -------------------- script --------------------
 def main():
-    ap = argparse.ArgumentParser(description="Figure 02 — Résidu |Δφ| par bandes + panneau compact")
-    ap.add_argument("--csv",  type=Path, required=True)
-    ap.add_argument("--meta", type=Path, default=Path("zz-data/chapter09/09_metrics_phase.json"))
-    ap.add_argument("--out",  type=Path, required=True)
-    ap.add_argument("--dpi",  type=int, default=300)
-    ap.add_argument("--bands", nargs="+", type=float, default=[20, 300, 300, 1000, 1000, 2000])
+    ap = argparse.ArgumentParser(
+        description="Figure 02 — Résidu |Δφ| par bandes + panneau compact"
+    )
+    ap.add_argument("--csv", type=Path, required=True)
+    ap.add_argument(
+        "--meta", type=Path, default=Path("zz-data/chapter09/09_metrics_phase.json")
+    )
+    ap.add_argument("--out", type=Path, required=True)
+    ap.add_argument("--dpi", type=int, default=300)
+    ap.add_argument(
+        "--bands", nargs="+", type=float, default=[20, 300, 300, 1000, 1000, 2000]
+    )
     ap.add_argument("--marker-size", type=float, default=3.0)
-    ap.add_argument("--line-width",  type=float, default=0.9)
+    ap.add_argument("--line-width", type=float, default=0.9)
     ap.add_argument("--gap-thresh-log10", type=float, default=0.12)
-    ap.add_argument("--log-level", choices=["DEBUG","INFO","WARNING","ERROR"], default="INFO")
+    ap.add_argument(
+        "--log-level", choices=["DEBUG", "INFO", "WARNING", "ERROR"], default="INFO"
+    )
     args = ap.parse_args()

     log = setup_logger(args.log_level)
@@ -124,15 +134,19 @@ def main():
             raise SystemExit(f"Colonne manquante: {c}")

     # variante active
-    if "phi_mcgt" in df:          phi_col = "phi_mcgt"
-    elif "phi_mcgt_cal" in df:    phi_col = "phi_mcgt_cal"
-    elif "phi_mcgt_raw" in df:    phi_col = "phi_mcgt_raw"
-    else: raise SystemExit("Aucune colonne phi_mcgt* disponible.")
+    if "phi_mcgt" in df:
+        phi_col = "phi_mcgt"
+    elif "phi_mcgt_cal" in df:
+        phi_col = "phi_mcgt_cal"
+    elif "phi_mcgt_raw" in df:
+        phi_col = "phi_mcgt_raw"
+    else:
+        raise SystemExit("Aucune colonne phi_mcgt* disponible.")
     log.info("Variante active: %s", phi_col)

     # tri / nettoyage basique
     order = np.argsort(df["f_Hz"].to_numpy(float))
-    f   = df["f_Hz"].to_numpy(float)[order]
+    f = df["f_Hz"].to_numpy(float)[order]
     ref = df["phi_ref"].to_numpy(float)[order]
     mcg = df[phi_col].to_numpy(float)[order]
     m = np.isfinite(f) & np.isfinite(ref) & np.isfinite(mcg)
@@ -147,7 +161,7 @@ def main():
     log.info("Rebranch k (20–300 Hz) = %d cycles", k)

     # résidu canonique = |Δφ_principal| après rebranch k
-    absd_full = np.abs(principal_diff(mcg - k*(2.0*np.pi), ref))
+    absd_full = np.abs(principal_diff(mcg - k * (2.0 * np.pi), ref))
     # eps pour échelle log (affichage uniquement) — les stats sont calculées AVANT ce remplacement
     eps = 1e-12
     absd_plot = np.where((~np.isfinite(absd_full)) | (absd_full <= 0), eps, absd_full)
@@ -155,37 +169,47 @@ def main():
     # stats 20–300 pour panneau compact
     m20300 = (f >= f20) & (f <= f300) & np.isfinite(absd_full)
     mean20 = float(np.nanmean(absd_full[m20300])) if m20300.any() else float("nan")
-    p9520  = float(p95(absd_full[m20300])) if m20300.any() else float("nan")
-    log.info("Stats 20–300 Hz: mean=%.3f  p95=%.3f  max=%.3f",
-             mean20, p9520, float(np.nanmax(absd_full[m20300])) if m20300.any() else float("nan"))
+    p9520 = float(p95(absd_full[m20300])) if m20300.any() else float("nan")
+    log.info(
+        "Stats 20–300 Hz: mean=%.3f  p95=%.3f  max=%.3f",
+        mean20,
+        p9520,
+        float(np.nanmax(absd_full[m20300])) if m20300.any() else float("nan"),
+    )

     # ---------------- figure & layout ----------------
     fig = plt.figure(figsize=(12.6, 8.2))
     gs = gridspec.GridSpec(
-        nrows=3, ncols=2,
-        width_ratios=[1.0, 0.38],
-        wspace=0.08, hspace=0.30
+        nrows=3, ncols=2, width_ratios=[1.0, 0.38], wspace=0.08, hspace=0.30
     )

     # axes
     axs = [fig.add_subplot(gs[i, 0]) for i in range(3)]
-    ax_right = fig.add_subplot(gs[:, 1]); ax_right.axis("off")
+    ax_right = fig.add_subplot(gs[:, 1])
+    ax_right.axis("off")

     # titre + espace vertical accru
     fig.suptitle(
         r"Résidu de phase $|\Delta\phi|$ par bande de fréquence  "
         r"($\phi_{\rm ref}$ vs $\phi_{\rm MCGT}$)",
-        fontsize=22, weight="bold", y=0.985
+        fontsize=22,
+        weight="bold",
+        y=0.985,
     )
     # pousse les sous-graphiques plus bas pour laisser un gap sous le titre
     fig.subplots_adjust(top=0.88, bottom=0.08)  # <— plus d’espace que précédemment

     # styles
     shade_color = "0.92"
-    marker_kw = dict(marker="o", markersize=float(args.marker_size),
-                     markeredgecolor="k", markeredgewidth=0.25,
-                     linestyle="", zorder=4)
-    line_kw   = dict(lw=float(args.line_width), solid_capstyle="butt", zorder=3)
+    marker_kw = dict(
+        marker="o",
+        markersize=float(args.marker_size),
+        markeredgecolor="k",
+        markeredgewidth=0.25,
+        linestyle="",
+        zorder=4,
+    )
+    line_kw = dict(lw=float(args.line_width), solid_capstyle="butt", zorder=3)

     # ---------------- tracé panneaux ----------------
     for i, (ax, (blo, bhi)) in enumerate(zip(axs, bands)):
@@ -194,13 +218,14 @@ def main():
         db_plot = absd_plot[mb]
         n_pts = int(mb.sum())
         mean_b = float(np.nanmean(db)) if n_pts else float("nan")
-        p95_b  = float(p95(db))        if n_pts else float("nan")
-        max_b  = float(np.nanmax(db))  if n_pts else float("nan")
+        p95_b = float(p95(db)) if n_pts else float("nan")
+        max_b = float(np.nanmax(db)) if n_pts else float("nan")

         ax.set_title(
             f"{int(blo)}-{int(bhi)} Hz  n={n_pts} — mean={mean_b:.3f}  "
             f"p95={p95_b:.3f}  max={max_b:.3f}",
-            loc="left", fontsize=11
+            loc="left",
+            fontsize=11,
         )
         ax.set_xscale("log")
         ax.set_yscale("log")
@@ -218,25 +243,48 @@ def main():
                     ax.plot(fb[seg], db_plot[seg], color="#1f77b4", **line_kw)

             # ligne p95
-            ax.axhline(p95_b if np.isfinite(p95_b) else np.nan, color="r", lw=1.0, ls=":")
+            ax.axhline(
+                p95_b if np.isfinite(p95_b) else np.nan, color="r", lw=1.0, ls=":"
+            )

             # étiquette p95 — géométrique au centre, SOUS la ligne (coords log-y)
             if np.isfinite(p95_b):
-                x_p = 10 ** ((np.log10(blo) + np.log10(bhi)) / 2.0) if fb.size >= 2 else fb[0]
-                y_p = p95_b / (1.15 if i != 2 else 1.10)  # même placement que la version validée
+                x_p = (
+                    10 ** ((np.log10(blo) + np.log10(bhi)) / 2.0)
+                    if fb.size >= 2
+                    else fb[0]
+                )
+                y_p = p95_b / (
+                    1.15 if i != 2 else 1.10
+                )  # même placement que la version validée
                 ax.text(
-                    x_p, y_p, f"$p95={p95_b:.3f}\\,\\mathrm{{rad}}$",
-                    color="r", ha="center", va="top", fontsize=9,
-                    bbox=dict(boxstyle="round,pad=0.25",
-                              facecolor=(shade_color if i == 0 else "white"),
-                              alpha=0.95, edgecolor="0.6")
+                    x_p,
+                    y_p,
+                    f"$p95={p95_b:.3f}\\,\\mathrm{{rad}}$",
+                    color="r",
+                    ha="center",
+                    va="top",
+                    fontsize=9,
+                    bbox=dict(
+                        boxstyle="round,pad=0.25",
+                        facecolor=(shade_color if i == 0 else "white"),
+                        alpha=0.95,
+                        edgecolor="0.6",
+                    ),
                 )
         else:
-            ax.text(0.5, 0.5, "no data", transform=ax.transAxes,
-                    ha="center", va="center", fontsize=10)
+            ax.text(
+                0.5,
+                0.5,
+                "no data",
+                transform=ax.transAxes,
+                ha="center",
+                va="center",
+                fontsize=10,
+            )

         # axes & labels
-        ax.set_xlim(max(10.0, blo/1.1), min(2048.0, bhi*1.1))
+        ax.set_xlim(max(10.0, blo / 1.1), min(2048.0, bhi * 1.1))
         ax.set_ylabel(r"$|\Delta\phi|$ [rad]")
         if i == 2:
             ax.set_xlabel("Fréquence $f$ [Hz]")
@@ -247,39 +295,58 @@ def main():
     box_styles = ax_right.inset_axes([0.08, 0.64, 0.84, 0.18])  # x,y,w,h (en coord. ax)
     box_styles.axis("off")

-    h_points = Line2D([], [], marker="o", linestyle="",
-                      markeredgecolor="k", markeredgewidth=0.25,
-                      markersize=args.marker_size, color="#1f77b4",
-                      label="données (points)")
-    h_line   = Line2D([], [], color="#1f77b4", lw=args.line_width, label="runs contigus (ligne)")
-    h_p95    = Line2D([], [], color="r", lw=1.0, linestyle=":", label="p95 (bandes)")
-    h_shade  = Line2D([], [], color=shade_color, lw=6, label=f"Bande {int(f20)}–{int(f300)} Hz")
-
-    leg1 = box_styles.legend([h_points, h_line, h_p95, h_shade],
-                             [h.get_label() for h in [h_points, h_line, h_p95, h_shade]],
-                             loc="upper center", frameon=True, fontsize=10,
-                             borderpad=0.6, handlelength=2.8, ncol=1)
+    h_points = Line2D(
+        [],
+        [],
+        marker="o",
+        linestyle="",
+        markeredgecolor="k",
+        markeredgewidth=0.25,
+        markersize=args.marker_size,
+        color="#1f77b4",
+        label="données (points)",
+    )
+    h_line = Line2D(
+        [], [], color="#1f77b4", lw=args.line_width, label="runs contigus (ligne)"
+    )
+    h_p95 = Line2D([], [], color="r", lw=1.0, linestyle=":", label="p95 (bandes)")
+    h_shade = Line2D(
+        [], [], color=shade_color, lw=6, label=f"Bande {int(f20)}–{int(f300)} Hz"
+    )
+
+    leg1 = box_styles.legend(
+        [h_points, h_line, h_p95, h_shade],
+        [h.get_label() for h in [h_points, h_line, h_p95, h_shade]],
+        loc="upper center",
+        frameon=True,
+        fontsize=10,
+        borderpad=0.6,
+        handlelength=2.8,
+        ncol=1,
+    )
     for t in leg1.get_texts():
         t.set_fontsize(10)

     # 2) panneau compact "Calage + stats 20–300 Hz" (plus bas, avec air au bas)
-    box_meta = ax_right.inset_axes([0.14, 0.37, 0.72, 0.20])  # abaissé pour laisser de l’espace au panneau du bas
+    box_meta = ax_right.inset_axes(
+        [0.14, 0.37, 0.72, 0.20]
+    )  # abaissé pour laisser de l’espace au panneau du bas
     box_meta.axis("off")

     cal = meta.get("calibration", {}) if isinstance(meta, dict) else {}
-    cal_model   = cal.get("model", cal.get("mode", "phi0,tc"))
+    cal_model = cal.get("model", cal.get("mode", "phi0,tc"))
     cal_enabled = cal.get("enabled", False)
-    phi0_hat    = cal.get("phi0_hat_rad", cal.get("phi0_hat", "N/A"))
-    tc_hat      = cal.get("tc_hat_s",   cal.get("tc_hat",   "N/A"))
+    phi0_hat = cal.get("phi0_hat_rad", cal.get("phi0_hat", "N/A"))
+    tc_hat = cal.get("tc_hat_s", cal.get("tc_hat", "N/A"))

-    if isinstance(phi0_hat, (int,float)):
+    if isinstance(phi0_hat, (int, float)):
         s_phi0 = f"φ₀={phi0_hat:.3e} rad"
     else:
         s_phi0 = f"φ₀={phi0_hat}"
-    if isinstance(tc_hat, (int,float)):
-        s_tc   = f"t_c={tc_hat:.3e} s"
+    if isinstance(tc_hat, (int, float)):
+        s_tc = f"t_c={tc_hat:.3e} s"
     else:
-        s_tc   = f"t_c={tc_hat}"
+        s_tc = f"t_c={tc_hat}"

     meta_text = (
         f"Calage: {cal_model} (enabled={cal_enabled})\n"
@@ -288,9 +355,15 @@ def main():
         f"mean={mean20:.3f} rad ;  p95={p9520:.3f} rad"
     )

-    box_meta.text(0.5, 0.5, meta_text, ha="center", va="center", fontsize=10,
-                  bbox=dict(boxstyle="round", facecolor="white",
-                            alpha=0.96, edgecolor="0.6"))
+    box_meta.text(
+        0.5,
+        0.5,
+        meta_text,
+        ha="center",
+        va="center",
+        fontsize=10,
+        bbox=dict(boxstyle="round", facecolor="white", alpha=0.96, edgecolor="0.6"),
+    )

     # ---------------- sortie ----------------
     args.out.parent.mkdir(parents=True, exist_ok=True)
diff --git a/zz-scripts/chapter09/plot_fig03_hist_absdphi_20_300.py b/zz-scripts/chapter09/plot_fig03_hist_absdphi_20_300.py
index 7af3df6..1fb1683 100755
--- a/zz-scripts/chapter09/plot_fig03_hist_absdphi_20_300.py
+++ b/zz-scripts/chapter09/plot_fig03_hist_absdphi_20_300.py
@@ -11,54 +11,87 @@ python zz-scripts/chapter09/plot_fig03_hist_absdphi_20_300.py \
   --out zz-figures/chapter09/fig_03_hist_absdphi_20_300.png \
   --mode principal --bins 50 --window 20 300 --xscale log --dpi 300 --log-level INFO
 """
+
 from pathlib import Path
-import argparse, logging, json
+import argparse
+import logging
+import json
 import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt

 # -------- Defaults
-DEF_CSV  = Path("zz-data/chapter09/09_phases_mcgt.csv")
+DEF_CSV = Path("zz-data/chapter09/09_phases_mcgt.csv")
 DEF_DIFF = Path("zz-data/chapter09/09_phase_diff.csv")
 DEF_META = Path("zz-data/chapter09/09_metrics_phase.json")
-DEF_OUT  = Path("zz-figures/chapter09/fig_03_hist_absdphi_20_300.png")
+DEF_OUT = Path("zz-figures/chapter09/fig_03_hist_absdphi_20_300.png")
+

 # -------- Utils
 def setup_logger(level: str):
     logging.basicConfig(
         level=getattr(logging, level.upper(), logging.INFO),
-        format='[%(asctime)s] [%(levelname)s] %(message)s',
-        datefmt='%Y-%m-%d %H:%M:%S'
+        format="[%(asctime)s] [%(levelname)s] %(message)s",
+        datefmt="%Y-%m-%d %H:%M:%S",
     )
     return logging.getLogger("fig03")

+
 def principal_phase_diff(a: np.ndarray, b: np.ndarray) -> np.ndarray:
     """((a-b+π) mod 2π) − π  ∈ (−π, π]"""
-    return (np.asarray(a, float) - np.asarray(b, float) + np.pi) % (2*np.pi) - np.pi
+    return (np.asarray(a, float) - np.asarray(b, float) + np.pi) % (2 * np.pi) - np.pi
+

 def parse_args():
-    p = argparse.ArgumentParser(description="Tracer fig_03 – Histogramme |Δφ| (20–300 Hz)")
-    p.add_argument("--diff", type=Path, default=DEF_DIFF,
-                   help="CSV 09_phase_diff.csv (préféré si présent, contient 'abs_dphi')")
-    p.add_argument("--csv", type=Path, default=DEF_CSV,
-                   help="CSV 09_phases_mcgt.csv (fallback)")
-    p.add_argument("--meta", type=Path, default=DEF_META,
-                   help="JSON méta (calage/grille) pour annotation")
+    p = argparse.ArgumentParser(
+        description="Tracer fig_03 – Histogramme |Δφ| (20–300 Hz)"
+    )
+    p.add_argument(
+        "--diff",
+        type=Path,
+        default=DEF_DIFF,
+        help="CSV 09_phase_diff.csv (préféré si présent, contient 'abs_dphi')",
+    )
+    p.add_argument(
+        "--csv", type=Path, default=DEF_CSV, help="CSV 09_phases_mcgt.csv (fallback)"
+    )
+    p.add_argument(
+        "--meta",
+        type=Path,
+        default=DEF_META,
+        help="JSON méta (calage/grille) pour annotation",
+    )
     p.add_argument("--out", type=Path, default=DEF_OUT, help="PNG de sortie")
     p.add_argument("--svg", action="store_true", help="Écrire aussi un .svg")
     p.add_argument("--bins", type=int, default=50, help="Nombre de bins")
-    p.add_argument("--window", nargs=2, type=float, default=[20.0, 300.0],
-                   metavar=("FMIN","FMAX"), help="Fenêtre fréquentielle [Hz]")
-    p.add_argument("--xscale", choices=["linear","log"], default="log",
-                   help="Échelle horizontale")
-    p.add_argument("--mode", choices=["principal","unwrap","raw"], default="principal",
-                   help="Définition du résidu si --csv est utilisé (défaut: principal)")
-    p.add_argument("--no-lines", action="store_true", help="Ne pas tracer les lignes des stats")
+    p.add_argument(
+        "--window",
+        nargs=2,
+        type=float,
+        default=[20.0, 300.0],
+        metavar=("FMIN", "FMAX"),
+        help="Fenêtre fréquentielle [Hz]",
+    )
+    p.add_argument(
+        "--xscale", choices=["linear", "log"], default="log", help="Échelle horizontale"
+    )
+    p.add_argument(
+        "--mode",
+        choices=["principal", "unwrap", "raw"],
+        default="principal",
+        help="Définition du résidu si --csv est utilisé (défaut: principal)",
+    )
+    p.add_argument(
+        "--no-lines", action="store_true", help="Ne pas tracer les lignes des stats"
+    )
     p.add_argument("--legend-loc", default="upper right", help="loc Matplotlib")
     p.add_argument("--dpi", type=int, default=300, help="DPI de sortie")
-    p.add_argument("--log-level", choices=["DEBUG","INFO","WARNING","ERROR"], default="INFO")
+    p.add_argument(
+        "--log-level", choices=["DEBUG", "INFO", "WARNING", "ERROR"], default="INFO"
+    )
     return p.parse_args()

+
 # -------- Main
 def main():
     args = parse_args()
@@ -77,7 +110,9 @@ def main():
             data_label = args.diff.name
             log.info("Chargé diff CSV: %s (%d points).", args.diff, len(df))
         else:
-            log.warning("%s existe mais colonnes manquantes -> fallback sur --csv", args.diff)
+            log.warning(
+                "%s existe mais colonnes manquantes -> fallback sur --csv", args.diff
+            )

     if abs_dphi is None:
         if not args.csv.exists():
@@ -109,16 +144,20 @@ def main():
             info_mode = "abs(unwrap(φ_MCGT−φ_ref))"
             k_used = None
         else:  # principal
-            two_pi = 2*np.pi
+            two_pi = 2 * np.pi
             if np.any(mask_win):
-                k_used = int(np.round(np.nanmedian((phi_m[mask_win]-phi_r[mask_win]) / two_pi)))
+                k_used = int(
+                    np.round(np.nanmedian((phi_m[mask_win] - phi_r[mask_win]) / two_pi))
+                )
             else:
                 k_used = 0
-            dphi = principal_phase_diff(phi_m - k_used*two_pi, phi_r)
+            dphi = principal_phase_diff(phi_m - k_used * two_pi, phi_r)
             abs_dphi = np.abs(dphi)
             info_mode = f"principal diff (k={k_used})"
         data_label = f"{args.csv.name} • {variant} • {info_mode}"
-        log.info("Chargé mcgt CSV: %s (%d points). Mode=%s", args.csv, len(mc), args.mode)
+        log.info(
+            "Chargé mcgt CSV: %s (%d points). Mode=%s", args.csv, len(mc), args.mode
+        )

     # --- Fenêtre & stats
     fmin, fmax = sorted(map(float, args.window))
@@ -136,57 +175,97 @@ def main():
         return float(np.percentile(a, 95.0)) if a.size else np.nan

     mean_abs = float(np.nanmean(vals))
-    med_abs  = float(np.nanmedian(vals))
-    p95_abs  = _p95(vals)
-    max_abs  = float(np.nanmax(vals))
+    med_abs = float(np.nanmedian(vals))
+    p95_abs = _p95(vals)
+    max_abs = float(np.nanmax(vals))

-    log.info("Fenêtre %.0f–%.0f Hz : n=%d (zeros=%d). mean=%.3g median=%.3g p95=%.3g max=%.3g",
-             fmin, fmax, n_total, n_zero, mean_abs, med_abs, p95_abs, max_abs)
+    log.info(
+        "Fenêtre %.0f–%.0f Hz : n=%d (zeros=%d). mean=%.3g median=%.3g p95=%.3g max=%.3g",
+        fmin,
+        fmax,
+        n_total,
+        n_zero,
+        mean_abs,
+        med_abs,
+        p95_abs,
+        max_abs,
+    )

     # --- Tracé
-    plt.rcParams.update({
-        "font.size": 11, "axes.titlesize": 16, "axes.labelsize": 12,
-        "legend.fontsize": 10, "xtick.labelsize": 10, "ytick.labelsize": 10,
-    })
+    plt.rcParams.update(
+        {
+            "font.size": 11,
+            "axes.titlesize": 16,
+            "axes.labelsize": 12,
+            "legend.fontsize": 10,
+            "xtick.labelsize": 10,
+            "ytick.labelsize": 10,
+        }
+    )
     fig, ax = plt.subplots(figsize=(12.8, 6.0))

     if args.xscale == "log" and pos.size > 0:
         vmin, vmax = pos.min(), pos.max()
-        edge_lo = max(vmin, 10**(np.floor(np.log10(vmin)) - 1))
+        edge_lo = max(vmin, 10 ** (np.floor(np.log10(vmin)) - 1))
         edge_hi = vmax * 1.2
         bins = np.logspace(np.log10(edge_lo), np.log10(edge_hi), args.bins + 1)
         ax.hist(pos, bins=bins, alpha=0.75, edgecolor="k", label="données (>0)")
         ax.set_xscale("log")
         if n_zero > 0:
-            ax.text(0.02, 0.92, f"n_zero = {n_zero}", transform=ax.transAxes, fontsize=9,
-                    va="top", bbox=dict(boxstyle="round,pad=0.2", facecolor="white",
-                                        edgecolor="gray", linewidth=0.6))
+            ax.text(
+                0.02,
+                0.92,
+                f"n_zero = {n_zero}",
+                transform=ax.transAxes,
+                fontsize=9,
+                va="top",
+                bbox=dict(
+                    boxstyle="round,pad=0.2",
+                    facecolor="white",
+                    edgecolor="gray",
+                    linewidth=0.6,
+                ),
+            )
     else:
         ax.hist(vals, bins=args.bins, alpha=0.75, edgecolor="k")

     ax.set_xlabel(rf"$|\Delta\phi|$ [rad]  ({int(fmin)}–{int(fmax)} Hz)")
     ax.set_ylabel("Comptes")
-    ax.set_title(rf"Histogramme du résidu de phase $|\Delta\phi|$  ({int(fmin)}–{int(fmax)} Hz)")
+    ax.set_title(
+        rf"Histogramme du résidu de phase $|\Delta\phi|$  ({int(fmin)}–{int(fmax)} Hz)"
+    )

-    ax.grid(which='major', linestyle='-', linewidth=0.6, alpha=0.45)
-    ax.grid(which='minor', linestyle=':', linewidth=0.4, alpha=0.35)
+    ax.grid(which="major", linestyle="-", linewidth=0.6, alpha=0.45)
+    ax.grid(which="minor", linestyle=":", linewidth=0.4, alpha=0.35)

     if not args.no_lines and (pos.size > 0 or args.xscale == "linear"):
-        ax.axvline(med_abs,  color='C0', linestyle='--', linewidth=1.6, label='médiane')
-        ax.axvline(mean_abs, color='C1', linestyle='--', linewidth=1.6, label='moyenne')
-        ax.axvline(p95_abs,  color='C2', linestyle=':',  linewidth=1.6, label='p95')
-        ax.axvline(max_abs,  color='C3', linestyle=':',  linewidth=1.6, label='max')
+        ax.axvline(med_abs, color="C0", linestyle="--", linewidth=1.6, label="médiane")
+        ax.axvline(mean_abs, color="C1", linestyle="--", linewidth=1.6, label="moyenne")
+        ax.axvline(p95_abs, color="C2", linestyle=":", linewidth=1.6, label="p95")
+        ax.axvline(max_abs, color="C3", linestyle=":", linewidth=1.6, label="max")

     handles, labels = ax.get_legend_handles_labels()
     if handles:
-        ax.legend(handles, labels, loc='center left', bbox_to_anchor=(1.02, 0.5), frameon=True)
+        ax.legend(
+            handles, labels, loc="center left", bbox_to_anchor=(1.02, 0.5), frameon=True
+        )

     # --- Boîtes d'info
-    ax.text(0.02, 0.95,
-            f"Source: {data_label}\n"
-            f"n={n_total}  mean={mean_abs:.3g}  median={med_abs:.3g}  p95={p95_abs:.3g}  max={max_abs:.3g}",
-            transform=ax.transAxes, fontsize=9, va="top",
-            bbox=dict(boxstyle="round,pad=0.4", facecolor="white", edgecolor="black", linewidth=0.7))
+    ax.text(
+        0.02,
+        0.95,
+        f"Source: {data_label}\n"
+        f"n={n_total}  mean={mean_abs:.3g}  median={med_abs:.3g}  p95={p95_abs:.3g}  max={max_abs:.3g}",
+        transform=ax.transAxes,
+        fontsize=9,
+        va="top",
+        bbox=dict(
+            boxstyle="round,pad=0.4",
+            facecolor="white",
+            edgecolor="black",
+            linewidth=0.7,
+        ),
+    )

     # méta (si fournie)
     cal_lines = []
@@ -195,37 +274,68 @@ def main():
             meta = json.loads(Path(args.meta).read_text())
             cal = meta.get("calibration", {})
             grid = meta.get("grid_used", meta.get("grid", {}))
-            cal_lines.append(f"Calage: {cal.get('model', cal.get('mode','phi0,tc'))} "
-                             f"(enabled={cal.get('enabled', False)})")
+            cal_lines.append(
+                f"Calage: {cal.get('model', cal.get('mode','phi0,tc'))} "
+                f"(enabled={cal.get('enabled', False)})"
+            )
             if "phi0_hat_rad" in cal or "tc_hat_s" in cal:
                 phi0 = cal.get("phi0_hat_rad", np.nan)
-                tc   = cal.get("tc_hat_s", np.nan)
+                tc = cal.get("tc_hat_s", np.nan)
                 cal_lines.append(f"φ0={phi0:.3g} rad,  t_c={tc:.3g} s")
             if grid:
-                cal_lines.append(f"Grille: [{int(grid.get('fmin_Hz', fmin))}-{int(grid.get('fmax_Hz', fmax))}] Hz, "
-                                 f"dlog10={grid.get('dlog10','?')}")
+                cal_lines.append(
+                    f"Grille: [{int(grid.get('fmin_Hz', fmin))}-{int(grid.get('fmax_Hz', fmax))}] Hz, "
+                    f"dlog10={grid.get('dlog10','?')}"
+                )
         except Exception as e:
             cal_lines.append(f"(meta illisible: {e})")
     else:
         cal_lines.append("(meta indisponible)")

-    cal_lines.append(f"|Δφ| {int(fmin)}–{int(fmax)} Hz : mean={mean_abs:.3g} rad ; p95={p95_abs:.3g} rad")
-    ax.text(0.02, 0.82, "\n".join(cal_lines), transform=ax.transAxes, fontsize=9, va="top",
-            bbox=dict(boxstyle="round,pad=0.4", facecolor="white", edgecolor="black", linewidth=0.7))
+    cal_lines.append(
+        f"|Δφ| {int(fmin)}–{int(fmax)} Hz : mean={mean_abs:.3g} rad ; p95={p95_abs:.3g} rad"
+    )
+    ax.text(
+        0.02,
+        0.82,
+        "\n".join(cal_lines),
+        transform=ax.transAxes,
+        fontsize=9,
+        va="top",
+        bbox=dict(
+            boxstyle="round,pad=0.4",
+            facecolor="white",
+            edgecolor="black",
+            linewidth=0.7,
+        ),
+    )

     # p95 rappel
     if np.isfinite(p95_abs) and p95_abs > 0:
-        ax.text(0.80, 0.95, f"p95 = {p95_abs:.3f} rad", transform=ax.transAxes, fontsize=10, va="top",
-                bbox=dict(boxstyle="round,pad=0.3", facecolor="#f2f2f2", edgecolor="gray", linewidth=0.6))
+        ax.text(
+            0.80,
+            0.95,
+            f"p95 = {p95_abs:.3f} rad",
+            transform=ax.transAxes,
+            fontsize=10,
+            va="top",
+            bbox=dict(
+                boxstyle="round,pad=0.3",
+                facecolor="#f2f2f2",
+                edgecolor="gray",
+                linewidth=0.6,
+            ),
+        )

     # Save
     args.out.parent.mkdir(parents=True, exist_ok=True)
     plt.tight_layout(rect=[0, 0, 0.88, 1.0])
-    plt.savefig(args.out, dpi=args.dpi, bbox_inches='tight')
+    plt.savefig(args.out, dpi=args.dpi, bbox_inches="tight")
     log.info("PNG écrit → %s", args.out)
     if args.svg:
-        plt.savefig(args.out.with_suffix(".svg"), bbox_inches='tight')
-        log.info("SVG écrit → %s", args.out.with_suffix('.svg'))
+        plt.savefig(args.out.with_suffix(".svg"), bbox_inches="tight")
+        log.info("SVG écrit → %s", args.out.with_suffix(".svg"))
+

 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter09/plot_fig04_absdphi_milestones_vs_f.py b/zz-scripts/chapter09/plot_fig04_absdphi_milestones_vs_f.py
index fe121a1..659f910 100755
--- a/zz-scripts/chapter09/plot_fig04_absdphi_milestones_vs_f.py
+++ b/zz-scripts/chapter09/plot_fig04_absdphi_milestones_vs_f.py
@@ -31,30 +31,35 @@ Exemple:
 """

 from pathlib import Path
-import argparse, logging, json
+import argparse
+import logging
+import json
 import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt

-DEF_DIFF       = Path("zz-data/chapter09/09_phase_diff.csv")
-DEF_CSV        = Path("zz-data/chapter09/09_phases_mcgt.csv")
-DEF_META       = Path("zz-data/chapter09/09_metrics_phase.json")
+DEF_DIFF = Path("zz-data/chapter09/09_phase_diff.csv")
+DEF_CSV = Path("zz-data/chapter09/09_phases_mcgt.csv")
+DEF_META = Path("zz-data/chapter09/09_metrics_phase.json")
 DEF_MILESTONES = Path("zz-data/chapter09/09_comparison_milestones.csv")
-DEF_OUT        = Path("zz-figures/chapter09/fig_04_milestones_absdphi_vs_f.png")
+DEF_OUT = Path("zz-figures/chapter09/fig_04_milestones_absdphi_vs_f.png")


 # ---------------- utilitaires ----------------

+
 def setup_logger(level: str):
-    logging.basicConfig(level=getattr(logging, level),
-                        format='[%(asctime)s] [%(levelname)s] %(message)s',
-                        datefmt='%Y-%m-%d %H:%M:%S')
+    logging.basicConfig(
+        level=getattr(logging, level),
+        format="[%(asctime)s] [%(levelname)s] %(message)s",
+        datefmt="%Y-%m-%d %H:%M:%S",
+    )
     return logging.getLogger("fig04")


 def principal_diff(a: np.ndarray, b: np.ndarray) -> np.ndarray:
     """((a-b+π) mod 2π) − π  ∈ (−π, π]"""
-    return (np.asarray(a, float) - np.asarray(b, float) + np.pi) % (2.0*np.pi) - np.pi
+    return (np.asarray(a, float) - np.asarray(b, float) + np.pi) % (2.0 * np.pi) - np.pi


 def _safe_pos(arr: np.ndarray, eps: float = 1e-12) -> np.ndarray:
@@ -69,7 +74,7 @@ def _yerr_clip_for_log(y: np.ndarray, sigma: np.ndarray, eps: float = 1e-12):
     """Retourne yerr asymétrique [bas, haut] en veillant à ne pas descendre sous eps en log."""
     y = _safe_pos(y, eps)
     s = np.asarray(sigma, float)
-    low  = np.clip(np.minimum(s, y - eps), 0.0, None)
+    low = np.clip(np.minimum(s, y - eps), 0.0, None)
     high = np.copy(s)
     return np.vstack([low, high])

@@ -107,37 +112,88 @@ def pick_variant(df: pd.DataFrame) -> str:
     for c in ("phi_mcgt", "phi_mcgt_cal", "phi_mcgt_raw"):
         if c in df.columns:
             return c
-    raise SystemExit("Aucune colonne phi_mcgt*, CSV invalide pour reconstruction du fond.")
+    raise SystemExit(
+        "Aucune colonne phi_mcgt*, CSV invalide pour reconstruction du fond."
+    )


 # ---------------- CLI ----------------

+
 def parse_args():
-    ap = argparse.ArgumentParser(description="fig_04 – |Δφ|(f) + milestones (principal, calage cohérent)")
-    ap.add_argument("--diff",       type=Path, default=DEF_DIFF,   help="CSV fond (f_Hz, abs_dphi). Prioritaire si présent.")
-    ap.add_argument("--csv",        type=Path, default=DEF_CSV,    help="CSV phases (fallback si --diff absent).")
-    ap.add_argument("--meta",       type=Path, default=DEF_META,   help="JSON méta (pour calage).")
-    ap.add_argument("--milestones", type=Path, default=DEF_MILESTONES, help="CSV milestones (requis).")
-    ap.add_argument("--out",        type=Path, default=DEF_OUT,    help="PNG de sortie.")
-    ap.add_argument("--log-level", choices=["DEBUG","INFO","WARNING","ERROR"], default="INFO")
-
-    ap.add_argument("--window", nargs=2, type=float, default=[20.0, 300.0],
-                    metavar=("FMIN","FMAX"), help="Bande ombrée [Hz] (affichage)")
-    ap.add_argument("--xlim", nargs=2, type=float, default=None, metavar=("XMIN","XMAX"),
-                    help="Limites X (log). Auto sinon.")
-    ap.add_argument("--ylim", nargs=2, type=float, default=None, metavar=("YMIN","YMAX"),
-                    help="Limites Y (log). Auto sinon.")
-
-    ap.add_argument("--with_errorbar", action="store_true", help="Afficher ±σ si disponible.")
-    ap.add_argument("--show_autres", action="store_true", help="Afficher les milestones 'autres'.")
-    ap.add_argument("--apply-calibration", choices=["auto","on","off"], default="auto",
-                    help="Appliquer (phi0, tc) aux milestones et au fond si reconstruit. 'auto' => selon meta.enabled.")
+    ap = argparse.ArgumentParser(
+        description="fig_04 – |Δφ|(f) + milestones (principal, calage cohérent)"
+    )
+    ap.add_argument(
+        "--diff",
+        type=Path,
+        default=DEF_DIFF,
+        help="CSV fond (f_Hz, abs_dphi). Prioritaire si présent.",
+    )
+    ap.add_argument(
+        "--csv",
+        type=Path,
+        default=DEF_CSV,
+        help="CSV phases (fallback si --diff absent).",
+    )
+    ap.add_argument(
+        "--meta", type=Path, default=DEF_META, help="JSON méta (pour calage)."
+    )
+    ap.add_argument(
+        "--milestones",
+        type=Path,
+        default=DEF_MILESTONES,
+        help="CSV milestones (requis).",
+    )
+    ap.add_argument("--out", type=Path, default=DEF_OUT, help="PNG de sortie.")
+    ap.add_argument(
+        "--log-level", choices=["DEBUG", "INFO", "WARNING", "ERROR"], default="INFO"
+    )
+
+    ap.add_argument(
+        "--window",
+        nargs=2,
+        type=float,
+        default=[20.0, 300.0],
+        metavar=("FMIN", "FMAX"),
+        help="Bande ombrée [Hz] (affichage)",
+    )
+    ap.add_argument(
+        "--xlim",
+        nargs=2,
+        type=float,
+        default=None,
+        metavar=("XMIN", "XMAX"),
+        help="Limites X (log). Auto sinon.",
+    )
+    ap.add_argument(
+        "--ylim",
+        nargs=2,
+        type=float,
+        default=None,
+        metavar=("YMIN", "YMAX"),
+        help="Limites Y (log). Auto sinon.",
+    )
+
+    ap.add_argument(
+        "--with_errorbar", action="store_true", help="Afficher ±σ si disponible."
+    )
+    ap.add_argument(
+        "--show_autres", action="store_true", help="Afficher les milestones 'autres'."
+    )
+    ap.add_argument(
+        "--apply-calibration",
+        choices=["auto", "on", "off"],
+        default="auto",
+        help="Appliquer (phi0, tc) aux milestones et au fond si reconstruit. 'auto' => selon meta.enabled.",
+    )
     ap.add_argument("--dpi", type=int, default=300)
     return ap.parse_args()


 # ---------------- main ----------------

+
 def main():
     args = parse_args()
     log = setup_logger(args.log_level)
@@ -146,12 +202,27 @@ def main():
     meta = load_meta(args.meta)
     cal = meta.get("calibration", {}) if isinstance(meta, dict) else {}
     cal_enabled = bool(cal.get("enabled", False))
-    phi0_hat = float(cal.get("phi0_hat_rad", 0.0)) if isinstance(cal.get("phi0_hat_rad", 0.0), (int, float)) else 0.0
-    tc_hat   = float(cal.get("tc_hat_s",     0.0)) if isinstance(cal.get("tc_hat_s",     0.0), (int, float)) else 0.0
-
-    apply_cal = (args.apply_calibration == "on") or (args.apply_calibration == "auto" and cal_enabled)
-    log.info("Calibration meta: enabled=%s ; apply_cal=%s ; phi0=%.3e rad ; t_c=%.3e s",
-             cal_enabled, apply_cal, phi0_hat, tc_hat)
+    phi0_hat = (
+        float(cal.get("phi0_hat_rad", 0.0))
+        if isinstance(cal.get("phi0_hat_rad", 0.0), (int, float))
+        else 0.0
+    )
+    tc_hat = (
+        float(cal.get("tc_hat_s", 0.0))
+        if isinstance(cal.get("tc_hat_s", 0.0), (int, float))
+        else 0.0
+    )
+
+    apply_cal = (args.apply_calibration == "on") or (
+        args.apply_calibration == "auto" and cal_enabled
+    )
+    log.info(
+        "Calibration meta: enabled=%s ; apply_cal=%s ; phi0=%.3e rad ; t_c=%.3e s",
+        cal_enabled,
+        apply_cal,
+        phi0_hat,
+        tc_hat,
+    )

     fmin_shade, fmax_shade = sorted(map(float, args.window))

@@ -159,7 +230,7 @@ def main():
     if not args.milestones.exists():
         raise SystemExit(f"Fichier milestones introuvable: {args.milestones}")
     M = pd.read_csv(args.milestones)
-    need = {"event","f_Hz","phi_mcgt_at_fpeak","obs_phase"}
+    need = {"event", "f_Hz", "phi_mcgt_at_fpeak", "obs_phase"}
     if not need.issubset(M.columns):
         raise SystemExit(f"{args.milestones} doit contenir {need}")

@@ -169,15 +240,24 @@ def main():

     # appliquer calage identique si demandé
     if apply_cal:
-        phi_m = phi_m + phi0_hat + 2.0*np.pi*fpk*tc_hat
+        phi_m = phi_m + phi0_hat + 2.0 * np.pi * fpk * tc_hat

     # différence principale (!!!)
     dphi = np.abs(principal_diff(phi_m, phi_o))

     sigma = M["sigma_phase"].to_numpy(float) if "sigma_phase" in M.columns else None
-    raw_cls = M.get("classe", pd.Series(["autres"] * len(M))).astype(str).str.lower().str.replace(" ", "").str.replace("-", "")
-    cls = np.where(raw_cls.isin(["primary","primaire"]), "primaire",
-          np.where(raw_cls.isin(["ordre2","order2","ordredeux"]), "ordre2", "autres"))
+    raw_cls = (
+        M.get("classe", pd.Series(["autres"] * len(M)))
+        .astype(str)
+        .str.lower()
+        .str.replace(" ", "")
+        .str.replace("-", "")
+    )
+    cls = np.where(
+        raw_cls.isin(["primary", "primaire"]),
+        "primaire",
+        np.where(raw_cls.isin(["ordre2", "order2", "ordredeux"]), "ordre2", "autres"),
+    )

     # garder uniquement points finis
     mfin = np.isfinite(fpk) & np.isfinite(dphi)
@@ -190,43 +270,57 @@ def main():
     # --- FOND |Δφ|(f) :
     # 1) si --diff OK: on suppose cohérent (abs_dphi déjà principal). Sinon:
     # 2) reconstruire depuis --csv avec principal + calage identique + rebranch k (20–300) pour compat.
-    f_bg = np.array([]); ad_bg = np.array([])
+    f_bg = np.array([])
+    ad_bg = np.array([])
     if args.diff.exists():
         D = pd.read_csv(args.diff)
-        if {"f_Hz","abs_dphi"}.issubset(D.columns):
-            f_bg  = D["f_Hz"].to_numpy(float)
+        if {"f_Hz", "abs_dphi"}.issubset(D.columns):
+            f_bg = D["f_Hz"].to_numpy(float)
             ad_bg = D["abs_dphi"].to_numpy(float)
             m = np.isfinite(f_bg) & np.isfinite(ad_bg) & (f_bg > 0)
             f_bg, ad_bg = f_bg[m], ad_bg[m]
             log.info("Fond chargé depuis --diff: %s (%d pts).", args.diff, f_bg.size)
         else:
-            log.warning("%s ne contient pas (f_Hz, abs_dphi) -> reconstruction", args.diff)
+            log.warning(
+                "%s ne contient pas (f_Hz, abs_dphi) -> reconstruction", args.diff
+            )

     if f_bg.size == 0 and args.csv.exists():
         C = pd.read_csv(args.csv)
-        need2 = {"f_Hz","phi_ref"}
+        need2 = {"f_Hz", "phi_ref"}
         if not need2.issubset(C.columns):
             raise SystemExit(f"{args.csv} doit contenir {need2}")
         var = pick_variant(C)
         f = C["f_Hz"].to_numpy(float)
         ref = C["phi_ref"].to_numpy(float)
-        mc  = C[var].to_numpy(float)
+        mc = C[var].to_numpy(float)

         # appliquer calage identique si demandé
         if apply_cal:
-            mc = mc + phi0_hat + 2.0*np.pi*f*tc_hat
+            mc = mc + phi0_hat + 2.0 * np.pi * f * tc_hat

         # rebranch k fond: médiane cycles sur 20–300 (cohérent avec nos autres figures)
-        mask_win = (f >= fmin_shade) & (f <= fmax_shade) & np.isfinite(mc) & np.isfinite(ref)
-        two_pi = 2.0*np.pi
-        k = int(np.round(np.nanmedian((mc[mask_win] - ref[mask_win]) / two_pi))) if np.any(mask_win) else 0
-        mc_reb = mc - k*two_pi
+        mask_win = (
+            (f >= fmin_shade) & (f <= fmax_shade) & np.isfinite(mc) & np.isfinite(ref)
+        )
+        two_pi = 2.0 * np.pi
+        k = (
+            int(np.round(np.nanmedian((mc[mask_win] - ref[mask_win]) / two_pi)))
+            if np.any(mask_win)
+            else 0
+        )
+        mc_reb = mc - k * two_pi

         ad = np.abs(principal_diff(mc_reb, ref))
         m = np.isfinite(f) & np.isfinite(ad) & (f > 0)
         f_bg, ad_bg = f[m], ad[m]
-        log.info("Fond reconstruit depuis --csv (var=%s, k=%d, apply_cal=%s) → %d pts.",
-                 var, k, apply_cal, f_bg.size)
+        log.info(
+            "Fond reconstruit depuis --csv (var=%s, k=%d, apply_cal=%s) → %d pts.",
+            var,
+            k,
+            apply_cal,
+            f_bg.size,
+        )

     # --- Axes
     f_all = fpk if f_bg.size == 0 else np.concatenate([fpk, f_bg])
@@ -244,25 +338,46 @@ def main():
         ymin, ymax = map(float, args.ylim)
         ymin = max(ymin, 1e-12)

-    log.info("xlim=[%.3g, %.3g] Hz ; ylim=[%.3g, %.3g] rad ; N_milestones=%d", xmin, xmax, ymin, ymax, fpk.size)
+    log.info(
+        "xlim=[%.3g, %.3g] Hz ; ylim=[%.3g, %.3g] rad ; N_milestones=%d",
+        xmin,
+        xmax,
+        ymin,
+        ymax,
+        fpk.size,
+    )

     # --- Figure
     fig = plt.figure(figsize=(11.5, 7.4))
     ax = fig.add_subplot(111)

     # Bande 20–300
-    ax.axvspan(fmin_shade, fmax_shade, color="0.88", alpha=0.6, zorder=0, label=f"Bande {int(fmin_shade)}–{int(fmax_shade)} Hz")
+    ax.axvspan(
+        fmin_shade,
+        fmax_shade,
+        color="0.88",
+        alpha=0.6,
+        zorder=0,
+        label=f"Bande {int(fmin_shade)}–{int(fmax_shade)} Hz",
+    )

     # Fond
     if f_bg.size:
         vis = (f_bg >= xmin) & (f_bg <= xmax)
         if np.any(vis):
-            ax.plot(f_bg[vis], _safe_pos(ad_bg[vis]), lw=1.8, alpha=0.85, color="C0",
-                    label=r"$|\Delta\phi|(f)$ (principal)", zorder=1)
+            ax.plot(
+                f_bg[vis],
+                _safe_pos(ad_bg[vis]),
+                lw=1.8,
+                alpha=0.85,
+                color="C0",
+                label=r"$|\Delta\phi|(f)$ (principal)",
+                zorder=1,
+            )

     # Groupes milestones
-    m_pri = (cls == "primaire")
-    m_o2  = (cls == "ordre2")
+    m_pri = cls == "primaire"
+    m_o2 = cls == "ordre2"
     m_aut = ~(m_pri | m_o2)

     def plot_group(mask, marker, color, label, z=4):
@@ -276,36 +391,70 @@ def main():
             # 1) avec sigma
             if np.any(m_ok):
                 yerr = _yerr_clip_for_log(y[m_ok], s[m_ok])
-                ax.errorbar(x[m_ok], y[m_ok], yerr=yerr, fmt=marker, ms=7,
-                            mfc="none" if marker in ("o","s") else None,
-                            mec=color, ecolor=color, elinewidth=1.0, capsize=2.5,
-                            color=color, lw=0.0, label=label, zorder=z)
+                ax.errorbar(
+                    x[m_ok],
+                    y[m_ok],
+                    yerr=yerr,
+                    fmt=marker,
+                    ms=7,
+                    mfc="none" if marker in ("o", "s") else None,
+                    mec=color,
+                    ecolor=color,
+                    elinewidth=1.0,
+                    capsize=2.5,
+                    color=color,
+                    lw=0.0,
+                    label=label,
+                    zorder=z,
+                )
                 label = None
             # 2) sans sigma
             if np.any(~m_ok):
-                ax.scatter(x[~m_ok], y[~m_ok], s=64, marker=marker, color=color,
-                           edgecolors="none", label=label, zorder=z)
+                ax.scatter(
+                    x[~m_ok],
+                    y[~m_ok],
+                    s=64,
+                    marker=marker,
+                    color=color,
+                    edgecolors="none",
+                    label=label,
+                    zorder=z,
+                )
         else:
-            ax.scatter(x, y, s=64, marker=marker, color=color, edgecolors="none",
-                       label=label, zorder=z)
+            ax.scatter(
+                x,
+                y,
+                s=64,
+                marker=marker,
+                color=color,
+                edgecolors="none",
+                label=label,
+                zorder=z,
+            )

     plot_group(m_pri, "o", "C1", "Milestones (primaire) (±σ)")
-    plot_group(m_o2,  "s", "C2", "Milestones (ordre 2) (±σ)")
+    plot_group(m_o2, "s", "C2", "Milestones (ordre 2) (±σ)")
     if args.show_autres:
         plot_group(m_aut, "x", "C4", "Milestones (autres)")

     # Axes / style
-    ax.set_xscale("log"); ax.set_yscale("log")
-    ax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax)
+    ax.set_xscale("log")
+    ax.set_yscale("log")
+    ax.set_xlim(xmin, xmax)
+    ax.set_ylim(ymin, ymax)
     ax.set_xlabel(r"Fréquence $f$ [Hz]")
     ax.set_ylabel(r"$|\Delta\phi|$ [rad]")
     ax.grid(True, which="both", ls=":", alpha=0.45)

     # Titres
-    title = r"Validation par milestones : $|\Delta\phi|(f)$ aux fréquences caractéristiques"
+    title = (
+        r"Validation par milestones : $|\Delta\phi|(f)$ aux fréquences caractéristiques"
+    )
     fig.suptitle(title, fontsize=18, fontweight="semibold", y=0.98)
-    subtitle = f"Comparaison MCGT vs milestones GWTC-3 — N_milestones={int(fpk.size)}" \
-               + ("" if not apply_cal else " — calage appliqué")
+    subtitle = (
+        f"Comparaison MCGT vs milestones GWTC-3 — N_milestones={int(fpk.size)}"
+        + ("" if not apply_cal else " — calage appliqué")
+    )
     fig.text(0.5, 0.905, subtitle, ha="center", fontsize=13)

     # Légende dédupliquée
@@ -313,7 +462,14 @@ def main():
     uniq = {}
     for hh, ll in zip(h, l):
         uniq[ll] = hh
-    ax.legend(uniq.values(), uniq.keys(), loc="lower right", frameon=True, facecolor="white", framealpha=0.95)
+    ax.legend(
+        uniq.values(),
+        uniq.keys(),
+        loc="lower right",
+        frameon=True,
+        facecolor="white",
+        framealpha=0.95,
+    )

     # Sauvegarde
     args.out.parent.mkdir(parents=True, exist_ok=True)
diff --git a/zz-scripts/chapter09/plot_fig05_scatter_phi_at_fpeak.py b/zz-scripts/chapter09/plot_fig05_scatter_phi_at_fpeak.py
index b2f261f..8584413 100755
--- a/zz-scripts/chapter09/plot_fig05_scatter_phi_at_fpeak.py
+++ b/zz-scripts/chapter09/plot_fig05_scatter_phi_at_fpeak.py
@@ -46,15 +46,15 @@ import matplotlib.pyplot as plt

 # ---------- Defaults ----------
 DEF_MILESTONES = Path("zz-data/chapter09/09_comparison_milestones.csv")
-DEF_OUT        = Path("zz-figures/chapter09/fig_05_scatter_phi_at_fpeak.png")
+DEF_OUT = Path("zz-figures/chapter09/fig_05_scatter_phi_at_fpeak.png")


 # ---------- Utils ----------
 def setup_logger(level: str) -> logging.Logger:
     logging.basicConfig(
         level=getattr(logging, level),
-        format='[%(asctime)s] [%(levelname)s] %(message)s',
-        datefmt='%Y-%m-%d %H:%M:%S'
+        format="[%(asctime)s] [%(levelname)s] %(message)s",
+        datefmt="%Y-%m-%d %H:%M:%S",
     )
     return logging.getLogger("fig05")

@@ -98,24 +98,34 @@ def robust_stats(residual: np.ndarray) -> Tuple[float, float, float, float, int]
     if a.size == 0:
         return (np.nan, np.nan, np.nan, np.nan, 0)
     mean = float(np.nanmean(a))
-    med  = float(np.nanmedian(a))
-    p95  = float(np.nanpercentile(a, 95))
-    mx   = float(np.nanmax(a))
+    med = float(np.nanmedian(a))
+    p95 = float(np.nanpercentile(a, 95))
+    mx = float(np.nanmax(a))
     return (mean, med, p95, mx, a.size)


 def parse_args():
     ap = argparse.ArgumentParser(description="Fig.05 — φ_ref vs φ_MCGT aux f_peak (±σ)")
-    ap.add_argument("--milestones", type=Path, default=DEF_MILESTONES,
-                    help="CSV milestones (phi_ref_at_fpeak, phi_mcgt_at_fpeak, ...)")
-    ap.add_argument("--out",        type=Path, default=DEF_OUT,
-                    help="Image de sortie (PNG)")
-    ap.add_argument("--pdf", action="store_true",
-                    help="Écrire aussi un PDF à côté du PNG")
-    ap.add_argument("--align", choices=["principal", "none"], default="principal",
-                    help="Alignement visuel de y sur x modulo 2π (défaut=principal)")
+    ap.add_argument(
+        "--milestones",
+        type=Path,
+        default=DEF_MILESTONES,
+        help="CSV milestones (phi_ref_at_fpeak, phi_mcgt_at_fpeak, ...)",
+    )
+    ap.add_argument("--out", type=Path, default=DEF_OUT, help="Image de sortie (PNG)")
+    ap.add_argument(
+        "--pdf", action="store_true", help="Écrire aussi un PDF à côté du PNG"
+    )
+    ap.add_argument(
+        "--align",
+        choices=["principal", "none"],
+        default="principal",
+        help="Alignement visuel de y sur x modulo 2π (défaut=principal)",
+    )
     ap.add_argument("--dpi", type=int, default=300, help="DPI du PNG")
-    ap.add_argument("--log-level", choices=["DEBUG","INFO","WARNING","ERROR"], default="INFO")
+    ap.add_argument(
+        "--log-level", choices=["DEBUG", "INFO", "WARNING", "ERROR"], default="INFO"
+    )
     return ap.parse_args()


@@ -169,21 +179,29 @@ def main():
         log.info("Alignement désactivé (valeurs brutes).")

     # Résidus principaux (pour métriques et annot)
-    res_principal = (y - x + np.pi) % (2.0*np.pi) - np.pi
+    res_principal = (y - x + np.pi) % (2.0 * np.pi) - np.pi
     abs_res = np.abs(res_principal)
     mean_abs, med_abs, p95_abs, max_abs, n_eff = robust_stats(abs_res)
-    log.info("Métriques |Δφ|_principal : mean=%.3f  median=%.3f  p95=%.3f  max=%.3f  (N=%d)",
-             mean_abs, med_abs, p95_abs, max_abs, n_eff)
+    log.info(
+        "Métriques |Δφ|_principal : mean=%.3f  median=%.3f  p95=%.3f  max=%.3f  (N=%d)",
+        mean_abs,
+        med_abs,
+        p95_abs,
+        max_abs,
+        n_eff,
+    )

     # Prépare figure
     args.out.parent.mkdir(parents=True, exist_ok=True)

     fig = plt.figure(figsize=(7.8, 7.6))
-    ax  = fig.add_subplot(111)
+    ax = fig.add_subplot(111)

     fig.suptitle(
         r"Comparaison ponctuelle aux $f_{\rm peak}$ : $\phi_{\rm ref}$ vs $\phi_{\rm MCGT}$",
-        fontsize=18, fontweight="semibold", y=0.97
+        fontsize=18,
+        fontweight="semibold",
+        y=0.97,
     )
     fig.subplots_adjust(top=0.90, bottom=0.10, left=0.12, right=0.98)

@@ -191,8 +209,8 @@ def main():
     cmap = class_color_map()
     masks = {
         "primaire": (cls == "primaire"),
-        "ordre2":   (cls == "ordre2"),
-        "autres":   (cls == "autres"),
+        "ordre2": (cls == "ordre2"),
+        "autres": (cls == "autres"),
     }

     # Limites (après alignement visuel si activé)
@@ -214,9 +232,22 @@ def main():
             sg = sigma[m]
             yerr = np.where(np.isfinite(sg) & (sg > 0), sg, np.nan)
             if np.any(np.isfinite(yerr)):
-                ax.errorbar(xg, yg, yerr=yerr, fmt="o", ms=5, mew=0.0,
-                            ecolor=color, elinewidth=0.9, capsize=2.5,
-                            mfc=color, mec="none", color=color, alpha=0.85, label=f"{name}")
+                ax.errorbar(
+                    xg,
+                    yg,
+                    yerr=yerr,
+                    fmt="o",
+                    ms=5,
+                    mew=0.0,
+                    ecolor=color,
+                    elinewidth=0.9,
+                    capsize=2.5,
+                    mfc=color,
+                    mec="none",
+                    color=color,
+                    alpha=0.85,
+                    label=f"{name}",
+                )
             else:
                 ax.scatter(xg, yg, s=28, color=color, label=f"{name}", alpha=0.9)
         else:
@@ -224,22 +255,43 @@ def main():

     ax.set_xlim(lo, hi)
     ax.set_ylim(lo, hi)
-    ax.set_aspect('equal', adjustable='box')
+    ax.set_aspect("equal", adjustable="box")
     ax.grid(True, ls=":", alpha=0.4)
     ax.set_xlabel(r"$\phi_{\rm ref}(f_{\rm peak})$ [rad]")
-    ax.set_ylabel(r"$\phi_{\rm MCGT}(f_{\rm peak})$ [rad]" + ("  (alignement principal)" if args.align == "principal" else ""))
+    ax.set_ylabel(
+        r"$\phi_{\rm MCGT}(f_{\rm peak})$ [rad]"
+        + ("  (alignement principal)" if args.align == "principal" else "")
+    )

     h, l = ax.get_legend_handles_labels()
     uniq = {}
     for hh, ll in zip(h, l):
         uniq[ll] = hh
-    ax.legend(uniq.values(), uniq.keys(), frameon=True, facecolor="white", framealpha=0.95, loc="upper left")
+    ax.legend(
+        uniq.values(),
+        uniq.keys(),
+        frameon=True,
+        facecolor="white",
+        framealpha=0.95,
+        loc="upper left",
+    )

-    meta_txt = (f"N={n_eff}  |Δφ|_principal  —  mean={mean_abs:.3f}  "
-                f"median={med_abs:.3f}  p95={p95_abs:.3f}  max={max_abs:.3f} rad")
-    ax.text(0.02, 0.02, meta_txt, transform=ax.transAxes, ha="left", va="bottom",
-            fontsize=9, bbox=dict(boxstyle="round,pad=0.35", facecolor="white",
-                                  edgecolor="0.6", alpha=0.95))
+    meta_txt = (
+        f"N={n_eff}  |Δφ|_principal  —  mean={mean_abs:.3f}  "
+        f"median={med_abs:.3f}  p95={p95_abs:.3f}  max={max_abs:.3f} rad"
+    )
+    ax.text(
+        0.02,
+        0.02,
+        meta_txt,
+        transform=ax.transAxes,
+        ha="left",
+        va="bottom",
+        fontsize=9,
+        bbox=dict(
+            boxstyle="round,pad=0.35", facecolor="white", edgecolor="0.6", alpha=0.95
+        ),
+    )

     fig.savefig(args.out, dpi=int(args.dpi), bbox_inches="tight")
     log.info("PNG écrit → %s", args.out)
diff --git a/zz-scripts/chapter10/add_phi_at_fpeak.py b/zz-scripts/chapter10/add_phi_at_fpeak.py
index 4c320a0..8c58b14 100755
--- a/zz-scripts/chapter10/add_phi_at_fpeak.py
+++ b/zz-scripts/chapter10/add_phi_at_fpeak.py
@@ -13,6 +13,7 @@ python zz-scripts/chapter10/add_phi_at_fpeak.py \
   --out zz-data/chapter10/10_mc_results.circ.with_fpeak.csv \
   --thresh 1e3 --backup
 """
+
 from __future__ import annotations
 import argparse
 import os
@@ -27,36 +28,54 @@ import pandas as pd
 from mcgt.backends.ref_phase import compute_phi_ref
 from mcgt.phase import phi_mcgt

+
 def wrap_phase(phi):
     """Reduce phi to [-pi, pi). Accepts scalars or numpy arrays."""
     a = np.asarray(phi, dtype=float)
     return (a + np.pi) % (2 * np.pi) - np.pi

+
 def safe_float(x):
     try:
         return float(x)
     except Exception:
         return np.nan

+
 def nearest_index(arr, val):
     arr = np.asarray(arr)
     if arr.size == 0:
         return None
     return int(np.abs(arr - val).argmin())

+
 def main():
     p = argparse.ArgumentParser()
     p.add_argument("--results", required=True, help="CSV results input")
-    p.add_argument("--ref-grid", required=True, help="CSV reference grid (first col frequencies)")
-    p.add_argument("--out", default=None, help="output CSV (if omitted add .with_fpeak.csv)")
-    p.add_argument("--thresh", type=float, default=1e3, help="threshold to consider phi aberrant (abs)")
-    p.add_argument("--backup", action="store_true", help="write .bak of original results")
+    p.add_argument(
+        "--ref-grid", required=True, help="CSV reference grid (first col frequencies)"
+    )
+    p.add_argument(
+        "--out", default=None, help="output CSV (if omitted add .with_fpeak.csv)"
+    )
+    p.add_argument(
+        "--thresh",
+        type=float,
+        default=1e3,
+        help="threshold to consider phi aberrant (abs)",
+    )
+    p.add_argument(
+        "--backup", action="store_true", help="write .bak of original results"
+    )
     args = p.parse_args()

     # prepare logging
     log_path = "zz-data/chapter10/add_phi_at_fpeak_errors.log"
-    logging.basicConfig(filename=log_path, level=logging.INFO,
-                        format="%(asctime)s %(levelname)s: %(message)s")
+    logging.basicConfig(
+        filename=log_path,
+        level=logging.INFO,
+        format="%(asctime)s %(levelname)s: %(message)s",
+    )
     logging.getLogger().addHandler(logging.StreamHandler())

     # read files
@@ -69,7 +88,7 @@ def main():

     # read reference grid
     try:
-        f_ref = np.loadtxt(args.ref_grid, delimiter=',', skiprows=1, usecols=[0])
+        f_ref = np.loadtxt(args.ref_grid, delimiter=",", skiprows=1, usecols=[0])
     except Exception as e:
         logging.error(f"Failed to load ref-grid '{args.ref_grid}': {e}")
         raise
@@ -78,7 +97,9 @@ def main():
     f_ref = np.asarray(f_ref)
     f_ref = f_ref[np.isfinite(f_ref)]
     if f_ref.size < 2:
-        raise SystemExit("Critical: ref-grid contains <2 valid frequencies after cleaning.")
+        raise SystemExit(
+            "Critical: ref-grid contains <2 valid frequencies after cleaning."
+        )

     # ensure sorted
     if not np.all(np.diff(f_ref) > 0):
@@ -90,8 +111,16 @@ def main():
     out_phi_mcgt_col = "phi_mcgt_fpeak"

     # create output arrays initialized from existing columns if present
-    phi_ref_out = df[out_phi_ref_col].copy() if out_phi_ref_col in df.columns else pd.Series([np.nan]*len(df), index=df.index)
-    phi_mcgt_out = df[out_phi_mcgt_col].copy() if out_phi_mcgt_col in df.columns else pd.Series([np.nan]*len(df), index=df.index)
+    phi_ref_out = (
+        df[out_phi_ref_col].copy()
+        if out_phi_ref_col in df.columns
+        else pd.Series([np.nan] * len(df), index=df.index)
+    )
+    phi_mcgt_out = (
+        df[out_phi_mcgt_col].copy()
+        if out_phi_mcgt_col in df.columns
+        else pd.Series([np.nan] * len(df), index=df.index)
+    )

     n_problem = 0
     n_fixed = 0
@@ -103,7 +132,10 @@ def main():
             existing_ok = False
             if not pd.isna(phi_ref_out.iloc[i]) and not pd.isna(phi_mcgt_out.iloc[i]):
                 try:
-                    if abs(float(phi_mcgt_out.iloc[i])) < args.thresh and abs(float(phi_ref_out.iloc[i])) < args.thresh:
+                    if (
+                        abs(float(phi_mcgt_out.iloc[i])) < args.thresh
+                        and abs(float(phi_ref_out.iloc[i])) < args.thresh
+                    ):
                         existing_ok = True
                 except Exception:
                     existing_ok = False
@@ -122,14 +154,18 @@ def main():
                 phi_ref_full = np.asarray(phi_ref_full, dtype=float)
                 if phi_ref_full.size != f_ref.size:
                     # if compute_phi_ref returned fewer points, still ok; we will pick nearest index via np.abs
-                    logging.debug(f"phi_ref length {phi_ref_full.size} != f_ref length {f_ref.size} for id={idx}")
+                    logging.debug(
+                        f"phi_ref length {phi_ref_full.size} != f_ref length {f_ref.size} for id={idx}"
+                    )
             except Exception as e:
-                logging.warning(f"compute_phi_ref error for id={idx} (m1={m1},m2={m2}): {e}")
+                logging.warning(
+                    f"compute_phi_ref error for id={idx} (m1={m1},m2={m2}): {e}"
+                )
                 raise

             # choose f_peak: prefer existing 'f_peak' or 'fpeak' column if present and finite
             f_peak = None
-            for cand in ("f_peak","fpeak","f_peak_Hz","fpeak_Hz"):
+            for cand in ("f_peak", "fpeak", "f_peak_Hz", "fpeak_Hz"):
                 if cand in df.columns:
                     ft = row.get(cand)
                     if not (pd.isna(ft) or not np.isfinite(safe_float(ft))):
@@ -138,7 +174,12 @@ def main():
             if f_peak is None:
                 # fallback: choose a frequency in the band [20,300] if available, else median of f_ref
                 f_min, f_max = np.min(f_ref), np.max(f_ref)
-                if (20 >= f_min) and (20 <= f_max) and (300 >= f_min) and (300 <= f_max):
+                if (
+                    (20 >= f_min)
+                    and (20 <= f_max)
+                    and (300 >= f_min)
+                    and (300 <= f_max)
+                ):
                     f_peak = 100.0  # generic fallback inside band; we could refine if you have a better rule
                 else:
                     f_peak = float(np.median(f_ref))
@@ -160,7 +201,7 @@ def main():

             # compute phi_mcgt (may raise)
             theta = {}
-            for key in ("m1","m2","q0star","alpha","phi0","tc","dist","incl"):
+            for key in ("m1", "m2", "q0star", "alpha", "phi0", "tc", "dist", "incl"):
                 if key in row:
                     theta[key] = safe_float(row[key])
             try:
@@ -180,8 +221,15 @@ def main():
             phi_mcgt_wr = wrap_phase(phi_mcgt_at_fpeak)

             # check thresholds
-            if abs(phi_ref_wr) > args.thresh or abs(phi_mcgt_wr) > args.thresh or not np.isfinite(phi_ref_wr) or not np.isfinite(phi_mcgt_wr):
-                logging.info(f"Abnormal phi for id={idx}: phi_ref={phi_ref_wr}, phi_mcgt={phi_mcgt_wr} -> set NaN")
+            if (
+                abs(phi_ref_wr) > args.thresh
+                or abs(phi_mcgt_wr) > args.thresh
+                or not np.isfinite(phi_ref_wr)
+                or not np.isfinite(phi_mcgt_wr)
+            ):
+                logging.info(
+                    f"Abnormal phi for id={idx}: phi_ref={phi_ref_wr}, phi_mcgt={phi_mcgt_wr} -> set NaN"
+                )
                 phi_ref_out.iloc[i] = np.nan
                 phi_mcgt_out.iloc[i] = np.nan
                 n_problem += 1
@@ -214,15 +262,17 @@ def main():
         "n_rows": int(len(df)),
         "n_fixed": int(n_fixed),
         "n_problem": int(n_problem),
-        "out": os.path.abspath(out_path)
+        "out": os.path.abspath(out_path),
     }
     manifest_path = out_path + ".manifest.json"
     import json
+
     with open(manifest_path, "w") as fh:
         json.dump(manifest, fh, indent=2)
     logging.info(f"Wrote: {out_path}  (fixed={n_fixed}, problems={n_problem})")
     logging.info(f"Manifest: {manifest_path}")
     logging.info(f"Error log: {log_path}")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter10/bootstrap_topk_p95.py b/zz-scripts/chapter10/bootstrap_topk_p95.py
index cadb4bb..b3464b7 100755
--- a/zz-scripts/chapter10/bootstrap_topk_p95.py
+++ b/zz-scripts/chapter10/bootstrap_topk_p95.py
@@ -25,6 +25,7 @@ Notes
 - Si un fichier de résidus manque pour un ID, le script conserve
   la valeur p95 issue de results.csv (si fournie) et marque p95_ci=null.
 """
+
 from __future__ import annotations

 import argparse
@@ -64,7 +65,14 @@ def detect_abscol(df: pd.DataFrame) -> str | None:
     """Tente détecter la colonne contenant les valeurs |Δφ|."""
     candidates = [c.lower() for c in df.columns]
     mapping = {
-        "absdphi": ["absdphi", "abs_dphi", "abs_d_phi", "abs(phi_diff)", "|Δφ|", "abs(delta_phi)"],
+        "absdphi": [
+            "absdphi",
+            "abs_dphi",
+            "abs_d_phi",
+            "abs(phi_diff)",
+            "|Δφ|",
+            "abs(delta_phi)",
+        ],
         "absdeltaphi": ["absdeltaphi", "abs_delta_phi", "abs(delta_phi)"],
         "absd": ["absd"],
     }
@@ -86,7 +94,9 @@ def detect_abscol(df: pd.DataFrame) -> str | None:
     return None


-def bootstrap_p95_from_array(arr: np.ndarray, B: int, rng: np.random.Generator) -> np.ndarray:
+def bootstrap_p95_from_array(
+    arr: np.ndarray, B: int, rng: np.random.Generator
+) -> np.ndarray:
     """
     Retourne les B valeurs bootstrapées du p95 calculées à partir d'`arr`.
     - arr : 1D array des valeurs |Δφ|(f) sur la fenêtre d'intérêt
@@ -99,7 +109,7 @@ def bootstrap_p95_from_array(arr: np.ndarray, B: int, rng: np.random.Generator)
         return np.array([], dtype=float)
     # indices shape (B, L)
     idx = rng.integers(0, L, size=(B, L))
-    samples = arr[idx]          # shape (B, L)
+    samples = arr[idx]  # shape (B, L)
     # np.nanpercentile with axis=1
     try:
         p95s = np.nanpercentile(samples, 95, axis=1, method="linear")
@@ -113,19 +123,37 @@ def bootstrap_p95_from_array(arr: np.ndarray, B: int, rng: np.random.Generator)
 # Main
 # -----------------------
 def main(argv=None):
-    p = argparse.ArgumentParser(description="Bootstrap p95 pour top-K (fichiers de résidus requis).")
+    p = argparse.ArgumentParser(
+        description="Bootstrap p95 pour top-K (fichiers de résidus requis)."
+    )
     p.add_argument("--best", required=True, help="JSON top-K (10_mc_best.json)")
-    p.add_argument("--results", required=False, help="CSV results (10_mc_results.csv) — utilisé en fallback")
-    p.add_argument("--B", type=int, default=1000, help="Nombre de rééchantillonnages bootstrap (défaut: 1000)")
-    p.add_argument("--seed", type=int, default=12345, help="Seed RNG pour reproductibilité")
-    p.add_argument("--resid-dir", default="zz-data/chapter10/topk_residuals",
-                   help="Répertoire contenant les fichiers de résidus par id")
+    p.add_argument(
+        "--results",
+        required=False,
+        help="CSV results (10_mc_results.csv) — utilisé en fallback",
+    )
+    p.add_argument(
+        "--B",
+        type=int,
+        default=1000,
+        help="Nombre de rééchantillonnages bootstrap (défaut: 1000)",
+    )
+    p.add_argument(
+        "--seed", type=int, default=12345, help="Seed RNG pour reproductibilité"
+    )
+    p.add_argument(
+        "--resid-dir",
+        default="zz-data/chapter10/topk_residuals",
+        help="Répertoire contenant les fichiers de résidus par id",
+    )
     p.add_argument("--out", required=True, help="Fichier JSON de sortie (augmenté)")
     p.add_argument("--log-level", default="INFO", help="Niveau de logs")
     args = p.parse_args(argv)

-    logging.basicConfig(level=getattr(logging, args.log_level.upper(), logging.INFO),
-                        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s")
+    logging.basicConfig(
+        level=getattr(logging, args.log_level.upper(), logging.INFO),
+        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
+    )
     _logger.info("Lancement bootstrap_topk_p95.py")
     best_path = Path(args.best)
     out_path = Path(args.out)
@@ -143,7 +171,14 @@ def main(argv=None):
     if not top_k:
         _logger.warning("Aucun top_k trouvé dans %s. Rien à faire.", best_path)
         # write minimal output
-        out = {"top_k": [], "meta": {"B": args.B, "seed": args.seed, "created_at": datetime.utcnow().isoformat() + "Z"}}
+        out = {
+            "top_k": [],
+            "meta": {
+                "B": args.B,
+                "seed": args.seed,
+                "created_at": datetime.utcnow().isoformat() + "Z",
+            },
+        }
         out_path.parent.mkdir(parents=True, exist_ok=True)
         with out_path.open("w", encoding="utf-8") as fh:
             json.dump(out, fh, indent=2, sort_keys=True)
@@ -172,7 +207,11 @@ def main(argv=None):

         resid_file = find_resid_file(resid_dir, id_)
         if resid_file is None:
-            _logger.warning("Fichier de résidus non trouvé pour id=%d (cherche dans %s). Fallback p95 si dispo.", id_, resid_dir)
+            _logger.warning(
+                "Fichier de résidus non trouvé pour id=%d (cherche dans %s). Fallback p95 si dispo.",
+                id_,
+                resid_dir,
+            )
             # fallback : copy p95 from results_df if available
             if results_df is not None:
                 row = results_df.loc[results_df["id"] == id_]
@@ -195,7 +234,9 @@ def main(argv=None):
         try:
             dfr = pd.read_csv(resid_file)
         except Exception as e:
-            _logger.exception("Impossible de lire %s pour id=%d : %s", resid_file, id_, e)
+            _logger.exception(
+                "Impossible de lire %s pour id=%d : %s", resid_file, id_, e
+            )
             ent["p95_boot_median"] = None
             ent["p95_ci"] = None
             ent["n_points_resid"] = None
@@ -204,7 +245,11 @@ def main(argv=None):

         col = detect_abscol(dfr)
         if col is None:
-            _logger.warning("Impossible de détecter colonne |Δφ| dans %s pour id=%d", resid_file, id_)
+            _logger.warning(
+                "Impossible de détecter colonne |Δφ| dans %s pour id=%d",
+                resid_file,
+                id_,
+            )
             ent["p95_boot_median"] = None
             ent["p95_ci"] = None
             ent["n_points_resid"] = None
@@ -245,7 +290,14 @@ def main(argv=None):
         ent["n_points_resid"] = int(n_points)
         ent["resid_file"] = str(resid_file)
         enriched.append(ent)
-        _logger.info("id=%d: p95_boot_median=%.6g  p95_ci=[%.6g, %.6g]  n=%d", id_, med, low, high, n_points)
+        _logger.info(
+            "id=%d: p95_boot_median=%.6g  p95_ci=[%.6g, %.6g]  n=%d",
+            id_,
+            med,
+            low,
+            high,
+            n_points,
+        )

     # écriture du JSON de sortie
     out_obj = {
diff --git a/zz-scripts/chapter10/check_metrics_consistency.py b/zz-scripts/chapter10/check_metrics_consistency.py
index 7e63b5e..e4a9a62 100755
--- a/zz-scripts/chapter10/check_metrics_consistency.py
+++ b/zz-scripts/chapter10/check_metrics_consistency.py
@@ -16,6 +16,7 @@ Usage:
       --manifest zz-data/chapter10/10_mc_run_manifest.json \
       --rtol 1e-6 --atol 1e-12
 """
+
 from __future__ import annotations
 import argparse
 import json
@@ -26,7 +27,9 @@ import pandas as pd
 import numpy as np
 import logging

-logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
+logging.basicConfig(
+    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s"
+)
 logger = logging.getLogger("check_metrics_consistency")


@@ -48,7 +51,10 @@ def compare_close(a, b, rtol=1e-6, atol=1e-12) -> bool:


 def main(argv: list[str] | None = None) -> int:
-    p = argparse.ArgumentParser(prog="check_metrics_consistency.py", description="QC quick-check des métriques MC")
+    p = argparse.ArgumentParser(
+        prog="check_metrics_consistency.py",
+        description="QC quick-check des métriques MC",
+    )
     p.add_argument("--results", required=True, help="CSV résultats (agrégé) à vérifier")
     p.add_argument("--manifest", required=True, help="Manifest JSON du run")
     p.add_argument("--rtol", type=float, default=1e-6)
@@ -70,10 +76,19 @@ def main(argv: list[str] | None = None) -> int:
     # normalisation noms colonnes courants (tolérance)
     df_cols = {c: c for c in df.columns}
     # required metrics expected
-    required_cols = ["id", "p95_20_300", "mean_20_300", "max_20_300", "n_20_300", "status"]
+    required_cols = [
+        "id",
+        "p95_20_300",
+        "mean_20_300",
+        "max_20_300",
+        "n_20_300",
+        "status",
+    ]
     missing = [c for c in required_cols if c not in df.columns]
     if missing:
-        logger.error("Colonnes essentielles manquantes dans %s : %s", results_p, missing)
+        logger.error(
+            "Colonnes essentielles manquantes dans %s : %s", results_p, missing
+        )
         # c'est critique : on ne peut pas poursuivre certaines vérifs
         return 2

@@ -91,7 +106,13 @@ def main(argv: list[str] | None = None) -> int:
         p95_p95 = float(np.nanpercentile(df["p95_20_300"], 95))
     p95_max = float(np.nanmax(df["p95_20_300"]))

-    logger.info("p95_20_300 : min=%.6f mean=%.6f p95=%.6f max=%.6f", p95_min, p95_mean, p95_p95, p95_max)
+    logger.info(
+        "p95_20_300 : min=%.6f mean=%.6f p95=%.6f max=%.6f",
+        p95_min,
+        p95_mean,
+        p95_p95,
+        p95_max,
+    )

     # lecture manifeste
     logger.info("Chargement manifeste: %s", manifest_p)
@@ -104,7 +125,9 @@ def main(argv: list[str] | None = None) -> int:
     if "n_rows_results" in sizes:
         exp = int(sizes["n_rows_results"])
         if exp != n_total:
-            errors.append(f"Mismatch n_rows_results: manifest={exp} vs detected={n_total}")
+            errors.append(
+                f"Mismatch n_rows_results: manifest={exp} vs detected={n_total}"
+            )
         else:
             logger.info("n_rows_results OK: %d", n_total)

@@ -126,22 +149,30 @@ def main(argv: list[str] | None = None) -> int:
             pth = pathlib.Path(path)
             actual = sha256_of_file(pth)
             if expected is None:
-                logger.warning("Pas de sha256 attendu pour %s (clé %s) dans manifest", path, key)
+                logger.warning(
+                    "Pas de sha256 attendu pour %s (clé %s) dans manifest", path, key
+                )
                 continue
             if actual is None:
                 errors.append(f"Fichier absent pour hash check: {path}")
             elif actual != expected:
-                errors.append(f"Hash mismatch pour {path}: manifest={expected} vs actual={actual}")
+                errors.append(
+                    f"Hash mismatch pour {path}: manifest={expected} vs actual={actual}"
+                )
             else:
                 logger.info("Hash OK: %s", path)

     # Optionnel : si le manifeste contient des métriques de référence, les comparer
-    ref_metrics = manifest.get("metrics_reference") or manifest.get("metrics_active") or {}
+    ref_metrics = (
+        manifest.get("metrics_reference") or manifest.get("metrics_active") or {}
+    )
     # exemple : p95_abs_20_300 dans anciens manifests
     if "p95_abs_20_300" in ref_metrics:
         ref_p95 = float(ref_metrics["p95_abs_20_300"])
         if not compare_close(ref_p95, p95_p95, rtol=args.rtol, atol=args.atol):
-            errors.append(f"p95_abs_20_300 mismatch: manifest={ref_p95} vs recomputed_p95={p95_p95}")
+            errors.append(
+                f"p95_abs_20_300 mismatch: manifest={ref_p95} vs recomputed_p95={p95_p95}"
+            )

     # verdict
     if errors:
diff --git a/zz-scripts/chapter10/diag_phi_fpeak.py b/zz-scripts/chapter10/diag_phi_fpeak.py
index 47f1ea1..64b0ba0 100755
--- a/zz-scripts/chapter10/diag_phi_fpeak.py
+++ b/zz-scripts/chapter10/diag_phi_fpeak.py
@@ -10,6 +10,7 @@ python zz-scripts/chapter10/diag_phi_fpeak.py \
   --out-diagnostics zz-data/chapter10/10_diag_phi_fpeak_report.csv \
   --thresh 1e3
 """
+
 from __future__ import annotations
 import argparse
 import csv
@@ -19,31 +20,43 @@ import pandas as pd
 from mcgt.backends.ref_phase import compute_phi_ref
 from mcgt.phase import phi_mcgt

+
 def safe_float(x):
     try:
         return float(x)
     except Exception:
         return np.nan

+
 def is_bad_phi(val, thresh):
-    if val is None: return True
+    if val is None:
+        return True
     try:
         v = float(val)
     except Exception:
         return True
-    if math.isnan(v) or math.isinf(v): return True
+    if math.isnan(v) or math.isinf(v):
+        return True
     return abs(v) > thresh

+
 def main():
     p = argparse.ArgumentParser()
     p.add_argument("--results", required=True)
     p.add_argument("--ref-grid", required=True)
-    p.add_argument("--out-diagnostics", default="zz-data/chapter10/10_diag_phi_fpeak_report.csv")
-    p.add_argument("--thresh", type=float, default=1e3, help="seuil pour considérer une phi aberrante")
+    p.add_argument(
+        "--out-diagnostics", default="zz-data/chapter10/10_diag_phi_fpeak_report.csv"
+    )
+    p.add_argument(
+        "--thresh",
+        type=float,
+        default=1e3,
+        help="seuil pour considérer une phi aberrante",
+    )
     args = p.parse_args()

     df = pd.read_csv(args.results)
-    f_ref = np.loadtxt(args.ref_grid, delimiter=',', skiprows=1, usecols=[0])
+    f_ref = np.loadtxt(args.ref_grid, delimiter=",", skiprows=1, usecols=[0])
     # ensure sorted and finite
     f_ref = np.asarray(f_ref)
     f_ref = f_ref[np.isfinite(f_ref)]
@@ -54,19 +67,21 @@ def main():
     for i, row in df.iterrows():
         rec = dict(id=int(row.get("id", -1)))
         # basic params
-        rec['idx'] = i
-        rec['m1'] = row.get("m1")
-        rec['m2'] = row.get("m2")
-        rec['k'] = row.get("k", "")
+        rec["idx"] = i
+        rec["m1"] = row.get("m1")
+        rec["m2"] = row.get("m2")
+        rec["k"] = row.get("k", "")
         # existing recorded phi values
-        rec['phi_ref_fpeak_recorded'] = row.get("phi_ref_fpeak", "")
-        rec['phi_mcgt_fpeak_recorded'] = row.get("phi_mcgt_fpeak", "")
+        rec["phi_ref_fpeak_recorded"] = row.get("phi_ref_fpeak", "")
+        rec["phi_mcgt_fpeak_recorded"] = row.get("phi_mcgt_fpeak", "")

         bad_flag = False
         msg = ""

         # quick detect recorded bad values
-        if is_bad_phi(rec['phi_mcgt_fpeak_recorded'], args.thresh) or is_bad_phi(rec['phi_ref_fpeak_recorded'], args.thresh):
+        if is_bad_phi(rec["phi_mcgt_fpeak_recorded"], args.thresh) or is_bad_phi(
+            rec["phi_ref_fpeak_recorded"], args.thresh
+        ):
             bad_flag = True
             msg += "recorded_phi_bad;"

@@ -80,12 +95,14 @@ def main():
             phi_ref_full = compute_phi_ref(f_ref, m1, m2)
             # pick f_peak estimate: if f_peak column exists use it, else use argmin|phi'|? for now try f_peak column
             f_peak = None
-            for cand in ("f_peak","fpeak","f_peak_Hz"):
+            for cand in ("f_peak", "fpeak", "f_peak_Hz"):
                 if cand in row:
                     f_peak = row.get(cand)
                     break
             # if f_peak not present, fall back to single point (e.g. median of f_ref)
-            if f_peak is None or (isinstance(f_peak, float) and (np.isnan(f_peak) or np.isinf(f_peak))):
+            if f_peak is None or (
+                isinstance(f_peak, float) and (np.isnan(f_peak) or np.isinf(f_peak))
+            ):
                 f_peak = float(np.median(f_ref))
             else:
                 f_peak = float(f_peak)
@@ -96,7 +113,7 @@ def main():

             # compute phi_mcgt on full grid (use theta from row)
             theta = {}
-            for key in ("m1","m2","q0star","alpha","phi0","tc","dist","incl"):
+            for key in ("m1", "m2", "q0star", "alpha", "phi0", "tc", "dist", "incl"):
                 if key in row:
                     theta[key] = safe_float(row[key])
             # compute phi_mcgt may raise; catch it
@@ -108,10 +125,10 @@ def main():
                 msg += f"phi_mcgt_error:{e};"
                 bad_flag = True

-            rec['f_peak_used'] = f_peak
-            rec['f_ref_len'] = int(f_ref.size)
-            rec['phi_ref_at_fpeak_recomputed'] = phi_ref_at_fpeak
-            rec['phi_mcgt_at_fpeak_recomputed'] = phi_mcgt_at_fpeak
+            rec["f_peak_used"] = f_peak
+            rec["f_ref_len"] = int(f_ref.size)
+            rec["phi_ref_at_fpeak_recomputed"] = phi_ref_at_fpeak
+            rec["phi_mcgt_at_fpeak_recomputed"] = phi_mcgt_at_fpeak

             # mark if recomputed values are bad
             if phi_mcgt_at_fpeak is None or is_bad_phi(phi_mcgt_at_fpeak, args.thresh):
@@ -121,31 +138,44 @@ def main():
         except Exception as e:
             bad_flag = True
             msg += f"recompute_error:{e};"
-            rec['f_peak_used'] = None
-            rec['f_ref_len'] = int(f_ref.size)
-            rec['phi_ref_at_fpeak_recomputed'] = ""
-            rec['phi_mcgt_at_fpeak_recomputed'] = ""
+            rec["f_peak_used"] = None
+            rec["f_ref_len"] = int(f_ref.size)
+            rec["phi_ref_at_fpeak_recomputed"] = ""
+            rec["phi_mcgt_at_fpeak_recomputed"] = ""

-        rec['bad'] = bad_flag
-        rec['msg'] = msg
+        rec["bad"] = bad_flag
+        rec["msg"] = msg
         rows.append(rec)

     # write diagnostics CSV of problematic rows only
-    out_rows = [r for r in rows if r['bad']]
+    out_rows = [r for r in rows if r["bad"]]
     if not out_rows:
         print("Aucun cas problématique détecté selon le seuil.")
     else:
-        keys = ['idx','id','m1','m2','k','f_peak_used','f_ref_len',
-                'phi_ref_fpeak_recorded','phi_ref_at_fpeak_recomputed',
-                'phi_mcgt_fpeak_recorded','phi_mcgt_at_fpeak_recomputed',
-                'msg']
-        with open(args.out_diagnostics, "w", newline='') as fh:
+        keys = [
+            "idx",
+            "id",
+            "m1",
+            "m2",
+            "k",
+            "f_peak_used",
+            "f_ref_len",
+            "phi_ref_fpeak_recorded",
+            "phi_ref_at_fpeak_recomputed",
+            "phi_mcgt_fpeak_recorded",
+            "phi_mcgt_at_fpeak_recomputed",
+            "msg",
+        ]
+        with open(args.out_diagnostics, "w", newline="") as fh:
             w = csv.DictWriter(fh, fieldnames=keys)
             w.writeheader()
             for r in out_rows:
-                roww = {k: r.get(k,"") for k in keys}
+                roww = {k: r.get(k, "") for k in keys}
                 w.writerow(roww)
-        print(f"Wrote diagnostics ({len(out_rows)} problematic rows) -> {args.out_diagnostics}")
+        print(
+            f"Wrote diagnostics ({len(out_rows)} problematic rows) -> {args.out_diagnostics}"
+        )
+

 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter10/eval_primary_metrics_20_300.py b/zz-scripts/chapter10/eval_primary_metrics_20_300.py
index 946f984..f9bb4a9 100755
--- a/zz-scripts/chapter10/eval_primary_metrics_20_300.py
+++ b/zz-scripts/chapter10/eval_primary_metrics_20_300.py
@@ -19,6 +19,7 @@ Sorties :
  - CSV résultats : id,θ,k,mean_20_300,p95_20_300,max_20_300,n_20_300,status,error_code,wall_time_s,worker_id,model,score
  - JSON top-K : top-K trié par 'score' (ici = p95 par défaut)
 """
+
 from __future__ import annotations

 import argparse
@@ -36,13 +37,13 @@ from joblib import Parallel, delayed
 # Importer les backends locaux (doivent exister dans le dépôt)
 try:
     from mcgt.backends.ref_phase import compute_phi_ref, ref_cache_info
-except Exception as e:
+except Exception:
     compute_phi_ref = None
     ref_cache_info = None

 try:
     from mcgt.phase import phi_mcgt
-except Exception as e:
+except Exception:
     phi_mcgt = None

 # ---------------------------------------------------------------------
@@ -65,12 +66,17 @@ ERR_CODES = {
     "UNKNOWN": "UNKNOWN",
 }

+
 # ---------------------------------------------------------------------
 # Utils IO safe (atomique)
 # ---------------------------------------------------------------------
-def safe_write_csv(df: pd.DataFrame, path: str, overwrite: bool = False, **kwargs) -> None:
+def safe_write_csv(
+    df: pd.DataFrame, path: str, overwrite: bool = False, **kwargs
+) -> None:
     if os.path.exists(path) and not overwrite:
-        raise SystemExit(f"Refuse d'écraser {path} — relancer avec --overwrite ou supprimer le fichier.")
+        raise SystemExit(
+            f"Refuse d'écraser {path} — relancer avec --overwrite ou supprimer le fichier."
+        )
     tmp = path + ".part"
     df.to_csv(tmp, index=False, float_format="%.6f", **kwargs)
     os.replace(tmp, path)
@@ -78,7 +84,9 @@ def safe_write_csv(df: pd.DataFrame, path: str, overwrite: bool = False, **kwarg

 def safe_write_json(obj: Any, path: str, overwrite: bool = False) -> None:
     if os.path.exists(path) and not overwrite:
-        raise SystemExit(f"Refuse d'écraser {path} — relancer avec --overwrite ou supprimer le fichier.")
+        raise SystemExit(
+            f"Refuse d'écraser {path} — relancer avec --overwrite ou supprimer le fichier."
+        )
     tmp = path + ".part"
     with open(tmp, "w", encoding="utf-8") as f:
         json.dump(obj, f, ensure_ascii=False, indent=2, sort_keys=True)
@@ -90,11 +98,17 @@ def safe_write_json(obj: Any, path: str, overwrite: bool = False) -> None:
 # ---------------------------------------------------------------------
 # Calculs principaux
 # ---------------------------------------------------------------------
-def compute_rebranch_k(phi_mcgt: np.ndarray, phi_ref: np.ndarray, f_hz: np.ndarray,
-                       window: Tuple[float, float] = WINDOW_DEFAULT) -> int:
+def compute_rebranch_k(
+    phi_mcgt: np.ndarray,
+    phi_ref: np.ndarray,
+    f_hz: np.ndarray,
+    window: Tuple[float, float] = WINDOW_DEFAULT,
+) -> int:
     """Calcul de k via median((φ_mcgt − φ_ref)/2π) sur la fenêtre window."""
     fmin, fmax = window
-    mask = (f_hz >= fmin) & (f_hz <= fmax) & np.isfinite(phi_mcgt) & np.isfinite(phi_ref)
+    mask = (
+        (f_hz >= fmin) & (f_hz <= fmax) & np.isfinite(phi_mcgt) & np.isfinite(phi_ref)
+    )
     if not np.any(mask):
         raise ValueError("NAN_IN_WINDOW")
     cycles = (phi_mcgt[mask] - phi_ref[mask]) / (2.0 * np.pi)
@@ -103,7 +117,9 @@ def compute_rebranch_k(phi_mcgt: np.ndarray, phi_ref: np.ndarray, f_hz: np.ndarr
     return k


-def delta_phi_principal(phi_mcgt: np.ndarray, phi_ref: np.ndarray, k: int) -> np.ndarray:
+def delta_phi_principal(
+    phi_mcgt: np.ndarray, phi_ref: np.ndarray, k: int
+) -> np.ndarray:
     """Retourne Δφ_principal = ((φ_mcgt - k·2π) - φ_ref + π) mod 2π - π."""
     raw = (phi_mcgt - k * 2.0 * np.pi) - phi_ref
     # opération modulo sur floats
@@ -127,16 +143,27 @@ def metrics_from_absdphi(absdphi: np.ndarray) -> Dict[str, Any]:
 # ---------------------------------------------------------------------
 # Évaluation d'un sample (unité de travail)
 # ---------------------------------------------------------------------
-def evaluate_sample(row: pd.Series, f_hz: np.ndarray, window: Tuple[float, float]) -> Dict[str, Any]:
+def evaluate_sample(
+    row: pd.Series, f_hz: np.ndarray, window: Tuple[float, float]
+) -> Dict[str, Any]:
     """Évalue les métriques pour un sample (pandas Series). Retourne dict de sortie."""
     t0 = time.time()
     result = {}
     sid = int(row["id"])
-    result.update({"id": sid, "m1": float(row["m1"]), "m2": float(row["m2"]),
-                   "q0star": float(row["q0star"]), "alpha": float(row["alpha"]),
-                   "phi0": float(row.get("phi0", 0.0)), "tc": float(row.get("tc", 0.0)),
-                   "dist": float(row.get("dist", 1000.0)), "incl": float(row.get("incl", 0.0)),
-                   "seed": int(row.get("seed", 0))})
+    result.update(
+        {
+            "id": sid,
+            "m1": float(row["m1"]),
+            "m2": float(row["m2"]),
+            "q0star": float(row["q0star"]),
+            "alpha": float(row["alpha"]),
+            "phi0": float(row.get("phi0", 0.0)),
+            "tc": float(row.get("tc", 0.0)),
+            "dist": float(row.get("dist", 1000.0)),
+            "incl": float(row.get("incl", 0.0)),
+            "seed": int(row.get("seed", 0)),
+        }
+    )
     try:
         # 1) calculer phi_ref via backend (peut lever)
         if compute_phi_ref is None:
@@ -148,10 +175,16 @@ def evaluate_sample(row: pd.Series, f_hz: np.ndarray, window: Tuple[float, float
         # 2) construire theta et appeler forward
         if phi_mcgt is None:
             raise RuntimeError(ERR_CODES["FORWARD_MISSING"])
-        theta = {"m1": float(row["m1"]), "m2": float(row["m2"]),
-                 "q0star": float(row["q0star"]), "alpha": float(row["alpha"]),
-                 "phi0": float(row.get("phi0", 0.0)), "tc": float(row.get("tc", 0.0)),
-                 "dist": float(row.get("dist", 1000.0)), "incl": float(row.get("incl", 0.0))}
+        theta = {
+            "m1": float(row["m1"]),
+            "m2": float(row["m2"]),
+            "q0star": float(row["q0star"]),
+            "alpha": float(row["alpha"]),
+            "phi0": float(row.get("phi0", 0.0)),
+            "tc": float(row.get("tc", 0.0)),
+            "dist": float(row.get("dist", 1000.0)),
+            "incl": float(row.get("incl", 0.0)),
+        }
         phi_m = phi_mcgt(f_hz, theta, model=row.get("model", "default"))
         if phi_m.shape != f_hz.shape:
             raise RuntimeError(ERR_CODES["GRID_MISMATCH"])
@@ -171,37 +204,60 @@ def evaluate_sample(row: pd.Series, f_hz: np.ndarray, window: Tuple[float, float
         met = metrics_from_absdphi(absd_win)

         # fill result
-        result.update({
-            "k": int(k),
-            "mean_20_300": met["mean"],
-            "p95_20_300": met["p95"],
-            "max_20_300": met["max"],
-            "n_20_300": int(met["n"]),
-            "status": "ok",
-            "error_code": "",
-            "wall_time_s": float(time.time() - t0),
-            "model": row.get("model", "default"),
-            # score = p95 par défaut (réécrit après si jalons pénalisés)
-            "score": float(met["p95"]),
-        })
+        result.update(
+            {
+                "k": int(k),
+                "mean_20_300": met["mean"],
+                "p95_20_300": met["p95"],
+                "max_20_300": met["max"],
+                "n_20_300": int(met["n"]),
+                "status": "ok",
+                "error_code": "",
+                "wall_time_s": float(time.time() - t0),
+                "model": row.get("model", "default"),
+                # score = p95 par défaut (réécrit après si jalons pénalisés)
+                "score": float(met["p95"]),
+            }
+        )
         return result

     except ValueError as ve:
         msg = str(ve)
         code = ERR_CODES.get(msg, ERR_CODES["UNKNOWN"])
-        result.update({"status": "failed", "error_code": code, "wall_time_s": float(time.time() - t0), "score": float("nan")})
+        result.update(
+            {
+                "status": "failed",
+                "error_code": code,
+                "wall_time_s": float(time.time() - t0),
+                "score": float("nan"),
+            }
+        )
         logging.debug("Sample %s failed ValueError: %s", sid, msg)
         return result
     except RuntimeError as re:
         msg = str(re)
         # si runtime vient d'un code interne string comme "REF_BACKEND_MISSING"
         code = msg if msg in ERR_CODES.values() else ERR_CODES["UNKNOWN"]
-        result.update({"status": "failed", "error_code": code, "wall_time_s": float(time.time() - t0), "score": float("nan")})
+        result.update(
+            {
+                "status": "failed",
+                "error_code": code,
+                "wall_time_s": float(time.time() - t0),
+                "score": float("nan"),
+            }
+        )
         logging.debug("Sample %s failed RuntimeError: %s", sid, msg)
         return result
-    except Exception as ex:
+    except Exception:
         logging.exception("Erreur inattendue pour sample %s", sid)
-        result.update({"status": "failed", "error_code": ERR_CODES["UNKNOWN"], "wall_time_s": float(time.time() - t0), "score": float("nan")})
+        result.update(
+            {
+                "status": "failed",
+                "error_code": ERR_CODES["UNKNOWN"],
+                "wall_time_s": float(time.time() - t0),
+                "score": float("nan"),
+            }
+        )
         return result


@@ -209,37 +265,83 @@ def evaluate_sample(row: pd.Series, f_hz: np.ndarray, window: Tuple[float, float
 # Main
 # ---------------------------------------------------------------------
 def parse_args(argv=None):
-    p = argparse.ArgumentParser(description="Évaluer métriques |Δφ|_principal (20-300 Hz) pour un catalogue d'échantillons.")
-    p.add_argument('--samples', required=True, help='CSV samples (id,m1,m2,q0star,alpha,...)')
-    p.add_argument('--ref-grid', required=True, help='CSV grille de référence (f_Hz [, phi_ref]) — nous utilisons f_Hz pour calculer phi_ref via backend')
-    p.add_argument('--out-results', default='zz-data/chapter10/10_mc_results.csv', help='CSV résultats (sortie)')
-    p.add_argument('--out-best', default='zz-data/chapter10/10_mc_best.json', help='JSON top-K (sortie)')
-    p.add_argument('--batch', type=int, default=256, help='taille de batch pour logging')
-    p.add_argument('--n-workers', type=int, default=8, help='n_workers joblib')
-    p.add_argument('--K', type=int, default=50, help='Top-K à sauver dans out-best')
-    p.add_argument('--n-test', type=int, default=None, help='mode test: n premiers échantillons seulement')
-    p.add_argument('--jalons', default=None, help='(optionnel) fichier jalons — non utilisé ici, penalty via aggregate')
-    p.add_argument('--overwrite', action='store_true', help='Autorise l\'écrasement des fichiers de sortie')
-    p.add_argument('--log-level', default='INFO', help='Niveau de log')
+    p = argparse.ArgumentParser(
+        description="Évaluer métriques |Δφ|_principal (20-300 Hz) pour un catalogue d'échantillons."
+    )
+    p.add_argument(
+        "--samples", required=True, help="CSV samples (id,m1,m2,q0star,alpha,...)"
+    )
+    p.add_argument(
+        "--ref-grid",
+        required=True,
+        help="CSV grille de référence (f_Hz [, phi_ref]) — nous utilisons f_Hz pour calculer phi_ref via backend",
+    )
+    p.add_argument(
+        "--out-results",
+        default="zz-data/chapter10/10_mc_results.csv",
+        help="CSV résultats (sortie)",
+    )
+    p.add_argument(
+        "--out-best",
+        default="zz-data/chapter10/10_mc_best.json",
+        help="JSON top-K (sortie)",
+    )
+    p.add_argument(
+        "--batch", type=int, default=256, help="taille de batch pour logging"
+    )
+    p.add_argument("--n-workers", type=int, default=8, help="n_workers joblib")
+    p.add_argument("--K", type=int, default=50, help="Top-K à sauver dans out-best")
+    p.add_argument(
+        "--n-test",
+        type=int,
+        default=None,
+        help="mode test: n premiers échantillons seulement",
+    )
+    p.add_argument(
+        "--jalons",
+        default=None,
+        help="(optionnel) fichier jalons — non utilisé ici, penalty via aggregate",
+    )
+    p.add_argument(
+        "--overwrite",
+        action="store_true",
+        help="Autorise l'écrasement des fichiers de sortie",
+    )
+    p.add_argument("--log-level", default="INFO", help="Niveau de log")
     return p.parse_args(argv)

+
 def main(argv=None):
     args = parse_args(argv)
-    logging.basicConfig(level=getattr(logging, args.log_level.upper()), format="%(asctime)s [%(levelname)s] %(message)s")
+    logging.basicConfig(
+        level=getattr(logging, args.log_level.upper()),
+        format="%(asctime)s [%(levelname)s] %(message)s",
+    )
     logging.info("Lancement eval_metrics_principal_20_300.py")

     # checks imports
     if compute_phi_ref is None:
-        logging.error("Backend compute_phi_ref introuvable. Assurez-vous que mcgt.backends.ref_phase module est disponible.")
+        logging.error(
+            "Backend compute_phi_ref introuvable. Assurez-vous que mcgt.backends.ref_phase module est disponible."
+        )
     if phi_mcgt is None:
-        logging.error("Forward phi_mcgt introuvable. Assurez-vous que mcgt.phase.phi_mcgt est importable.")
+        logging.error(
+            "Forward phi_mcgt introuvable. Assurez-vous que mcgt.phase.phi_mcgt est importable."
+        )

     # charger grille f_Hz
     df_ref = pd.read_csv(args.ref_grid)
     if "f_Hz" not in df_ref.columns:
-        raise SystemExit(f"Fichier ref-grid {args.ref_grid} doit contenir une colonne 'f_Hz'.")
+        raise SystemExit(
+            f"Fichier ref-grid {args.ref_grid} doit contenir une colonne 'f_Hz'."
+        )
     f_hz = np.asarray(df_ref["f_Hz"].values, dtype=float)
-    logging.info("Grille de référence : %d pts (%.3f..%.3f Hz)", f_hz.size, float(f_hz.min()), float(f_hz.max()))
+    logging.info(
+        "Grille de référence : %d pts (%.3f..%.3f Hz)",
+        f_hz.size,
+        float(f_hz.min()),
+        float(f_hz.max()),
+    )

     # charger samples
     samples = pd.read_csv(args.samples)
@@ -252,7 +354,11 @@ def main(argv=None):
     window = WINDOW_DEFAULT
     work = []
     # joblib Parallel with chunksize automatic
-    logging.info("Démarrage évaluation en parallèle : batch=%d n_workers=%d", args.batch, args.n_workers)
+    logging.info(
+        "Démarrage évaluation en parallèle : batch=%d n_workers=%d",
+        args.batch,
+        args.n_workers,
+    )
     try:
         results = Parallel(n_jobs=args.n_workers, backend="loky")(
             delayed(evaluate_sample)(row, f_hz, window) for _, row in samples.iterrows()
@@ -264,9 +370,27 @@ def main(argv=None):
     # construire DataFrame résultats
     df_out = pd.DataFrame(results)
     # colonne ordering
-    cols = ["id","m1","m2","q0star","alpha","phi0","tc","dist","incl","k",
-            "mean_20_300","p95_20_300","max_20_300","n_20_300","status","error_code",
-            "wall_time_s","model","score"]
+    cols = [
+        "id",
+        "m1",
+        "m2",
+        "q0star",
+        "alpha",
+        "phi0",
+        "tc",
+        "dist",
+        "incl",
+        "k",
+        "mean_20_300",
+        "p95_20_300",
+        "max_20_300",
+        "n_20_300",
+        "status",
+        "error_code",
+        "wall_time_s",
+        "model",
+        "score",
+    ]
     # some fields may be missing if many failed; ensure they exist
     for c in cols:
         if c not in df_out.columns:
@@ -275,7 +399,9 @@ def main(argv=None):

     # safe write CSV
     safe_write_csv(df_out, args.out_results, overwrite=args.overwrite)
-    logging.info("Écriture des résultats (%d lignes) -> %s", len(df_out), args.out_results)
+    logging.info(
+        "Écriture des résultats (%d lignes) -> %s", len(df_out), args.out_results
+    )

     # top-K (tri par score ascendant : petite p95 = meilleur)
     df_ok = df_out[df_out["status"] == "ok"].copy()
@@ -283,7 +409,9 @@ def main(argv=None):
         topk = []
         logging.warning("Aucun sample valide (status==ok). Pas de top-K.")
     else:
-        df_ok = df_ok.sort_values(["score", "p95_20_300", "mean_20_300", "max_20_300", "id"])
+        df_ok = df_ok.sort_values(
+            ["score", "p95_20_300", "mean_20_300", "max_20_300", "id"]
+        )
         top = df_ok.head(args.K)
         topk = top.to_dict(orient="records")

@@ -304,5 +432,6 @@ def main(argv=None):
     logging.info("Terminé.")
     return 0

+
 if __name__ == "__main__":
     sys.exit(main())
diff --git a/zz-scripts/chapter10/generate_data_chapter10.py b/zz-scripts/chapter10/generate_data_chapter10.py
index 63a8797..f321953 100755
--- a/zz-scripts/chapter10/generate_data_chapter10.py
+++ b/zz-scripts/chapter10/generate_data_chapter10.py
@@ -46,15 +46,22 @@ Notes
 """

 from __future__ import annotations
-import argparse, sys, os, json, time, logging, hashlib, subprocess, textwrap, shutil
+import argparse
+import sys
+import os
+import json
+import time
+import logging
+import hashlib
+import subprocess
 from pathlib import Path
 from datetime import datetime

 # ---------------------------------------------------------------------
 # 0) Emplacements et scripts appelés
 # ---------------------------------------------------------------------
-HERE = Path(__file__).resolve().parent               # .../zz-scripts/chapter10
-ROOT = HERE.parent.parent                            # racine du dépôt
+HERE = Path(__file__).resolve().parent  # .../zz-scripts/chapter10
+ROOT = HERE.parent.parent  # racine du dépôt
 DDIR = ROOT / "zz-data" / "chapter10"
 FDIR = ROOT / "zz-figures" / "chapter10"
 REF_CH9 = ROOT / "zz-data" / "chapter09"
@@ -63,8 +70,8 @@ CACHE_REF = DDIR / ".cache_ref"
 SCRIPTS = {
     "samples": HERE / "generate_8d_samples.py",
     "metrics": HERE / "eval_metrics_principal_20_300.py",
-    "jalons":  HERE / "eval_milestones_fpeak.py",
-    "agg":     HERE / "aggregate_mc_runs.py",
+    "jalons": HERE / "eval_milestones_fpeak.py",
+    "agg": HERE / "aggregate_mc_runs.py",
 }

 DEFAULTS = {
@@ -80,25 +87,29 @@ DEFAULTS = {
     "jalons_ref": REF_CH9 / "09_comparison_milestones.csv",
 }

+
 # ---------------------------------------------------------------------
 # 1) Utilitaires génériques
 # ---------------------------------------------------------------------
 def sha256_file(path: Path) -> str:
     h = hashlib.sha256()
     with open(path, "rb") as f:
-        for chunk in iter(lambda: f.read(1<<20), b""):
+        for chunk in iter(lambda: f.read(1 << 20), b""):
             h.update(chunk)
     return h.hexdigest()

+
 def save_json(obj, path: Path):
     path.parent.mkdir(parents=True, exist_ok=True)
     with open(path, "w", encoding="utf-8") as f:
         json.dump(obj, f, indent=2, ensure_ascii=False, sort_keys=True)

+
 def load_json(path: Path):
     with open(path, "r", encoding="utf-8") as f:
         return json.load(f)

+
 def run_cmd(cmd: list[str], log: logging.Logger, check=True):
     log.debug("CMD: %s", " ".join(str(c) for c in cmd))
     cp = subprocess.run([str(c) for c in cmd], stdout=sys.stdout, stderr=sys.stderr)
@@ -106,11 +117,13 @@ def run_cmd(cmd: list[str], log: logging.Logger, check=True):
         raise SystemExit(cp.returncode)
     return cp.returncode

+
 def ensure_dirs(log: logging.Logger):
     for p in [DDIR, FDIR, CACHE_REF]:
         p.mkdir(parents=True, exist_ok=True)
         log.debug("ok dossier: %s", p)

+
 def write_default_config_if_missing(cfg_path: Path, log: logging.Logger):
     if cfg_path.exists():
         log.debug("Config déjà présente : %s", cfg_path)
@@ -119,19 +132,18 @@ def write_default_config_if_missing(cfg_path: Path, log: logging.Logger):
     template = {
         "model": "default",
         "priors": {
-            "m1":      {"min": 5.0,   "max": 80.0, "dist": "uniform"},
-            "m2":      {"min": 5.0,   "max": 80.0, "dist": "uniform"},
-            "q0star":  {"min": -0.3,  "max": 0.3,  "dist": "uniform"},
-            "alpha":   {"min": -1.0,  "max": 1.0,  "dist": "uniform"}
-        },
-        "nuisance": {
-            "phi0": 0.0, "tc": 0.0, "dist": 1000.0, "incl": 0.0
+            "m1": {"min": 5.0, "max": 80.0, "dist": "uniform"},
+            "m2": {"min": 5.0, "max": 80.0, "dist": "uniform"},
+            "q0star": {"min": -0.3, "max": 0.3, "dist": "uniform"},
+            "alpha": {"min": -1.0, "max": 1.0, "dist": "uniform"},
         },
-        "sobol": {"scramble": True, "seed": 12345}
+        "nuisance": {"phi0": 0.0, "tc": 0.0, "dist": 1000.0, "incl": 0.0},
+        "sobol": {"scramble": True, "seed": 12345},
     }
     save_json(template, cfg_path)
     log.info("Config absente → TEMPLATE créé : %s", cfg_path)

+
 def bref(path: Path) -> str:
     """Chemin abrégé pour affichage log."""
     try:
@@ -139,6 +151,7 @@ def bref(path: Path) -> str:
     except Exception:
         return str(path)

+
 # ---------------------------------------------------------------------
 # 2) Étapes du pipeline (fonctions)
 # ---------------------------------------------------------------------
@@ -161,8 +174,13 @@ def etape_1_preflight(args, log: logging.Logger):
     for k, sp in SCRIPTS.items():
         if not sp.exists():
             raise FileNotFoundError(f"Script manquant ({k}) : {sp}")
-    log.info("   ✓ Dossiers/entrées OK | grille=%s | jalons=%s | config=%s",
-             bref(ref_grid), bref(Path(args.jalons or DEFAULTS["jalons_ref"])), bref(cfg))
+    log.info(
+        "   ✓ Dossiers/entrées OK | grille=%s | jalons=%s | config=%s",
+        bref(ref_grid),
+        bref(Path(args.jalons or DEFAULTS["jalons_ref"])),
+        bref(cfg),
+    )
+

 def etape_2_samples_global(args, log: logging.Logger):
     if args.skip_samples:
@@ -170,13 +188,20 @@ def etape_2_samples_global(args, log: logging.Logger):
         return Path(args.samples_csv or DEFAULTS["samples_csv"])
     log.info("2) Génération des échantillons (global Sobol)")
     cmd = [
-        sys.executable, str(SCRIPTS["samples"]),
-        "--config", str(args.config or DEFAULTS["config"]),
-        "--n", str(args.n),
-        "--scheme", "sobol",
-        "--scramble", "on" if args.scramble else "off",
-        "--seed", str(args.seed),
-        "--out", str(args.samples_csv or DEFAULTS["samples_csv"]),
+        sys.executable,
+        str(SCRIPTS["samples"]),
+        "--config",
+        str(args.config or DEFAULTS["config"]),
+        "--n",
+        str(args.n),
+        "--scheme",
+        "sobol",
+        "--scramble",
+        "on" if args.scramble else "off",
+        "--seed",
+        str(args.seed),
+        "--out",
+        str(args.samples_csv or DEFAULTS["samples_csv"]),
     ]
     if args.overwrite:
         cmd.append("--overwrite")
@@ -187,21 +212,34 @@ def etape_2_samples_global(args, log: logging.Logger):
     log.info("   ✓ Échantillons écrits : %s", bref(out))
     return out

+
 def etape_3_metrics(args, log: logging.Logger, samples_csv: Path):
     if args.skip_metrics:
         log.info("3) Métriques canoniques — SKIP demandé")
-        return (Path(args.results_csv or DEFAULTS["results_csv"]), Path(args.best_json or DEFAULTS["best_json"]))
+        return (
+            Path(args.results_csv or DEFAULTS["results_csv"]),
+            Path(args.best_json or DEFAULTS["best_json"]),
+        )
     log.info("3) Évaluation des métriques canoniques |Δφ|_principal @ [20,300] Hz")
     cmd = [
-        sys.executable, str(SCRIPTS["metrics"]),
-        "--samples", str(samples_csv),
-        "--ref-grid", str(args.ref_grid or DEFAULTS["ref_grid"]),
-        "--out-results", str(args.results_csv or DEFAULTS["results_csv"]),
-        "--out-best", str(args.best_json or DEFAULTS["best_json"]),
-        "--batch", str(args.batch),
-        "--n-workers", str(args.n_workers),
-        "--K", str(args.K),
-        "--log-level", args.log_level,
+        sys.executable,
+        str(SCRIPTS["metrics"]),
+        "--samples",
+        str(samples_csv),
+        "--ref-grid",
+        str(args.ref_grid or DEFAULTS["ref_grid"]),
+        "--out-results",
+        str(args.results_csv or DEFAULTS["results_csv"]),
+        "--out-best",
+        str(args.best_json or DEFAULTS["best_json"]),
+        "--batch",
+        str(args.batch),
+        "--n-workers",
+        str(args.n_workers),
+        "--K",
+        str(args.K),
+        "--log-level",
+        args.log_level,
     ]
     if args.overwrite:
         cmd.append("--overwrite")
@@ -211,38 +249,59 @@ def etape_3_metrics(args, log: logging.Logger, samples_csv: Path):
     log.info("   ✓ Résultats : %s | Top-K provisoire : %s", bref(res), bref(best))
     return (res, best)

+
 def etape_4_jalons(args, log: logging.Logger, best_json: Path):
     if args.skip_jalons:
         log.info("4) Jalons f_peak — SKIP demandé")
         return None
     log.info("4) Évaluation jalons f_peak pour le top-K")
     cmd = [
-        sys.executable, str(SCRIPTS["jalons"]),
-        "--jalons", str(args.jalons or DEFAULTS["jalons_ref"]),
-        "--ref-grid", str(args.ref_grid or DEFAULTS["ref_grid"]),
-        "--samples", str(args.samples_csv or DEFAULTS["samples_csv"]),
-        "--best-json", str(best_json),
-        "--out", str(args.jalons_out or DEFAULTS["jalons_csv"]),
-        "--log-level", args.log_level,
+        sys.executable,
+        str(SCRIPTS["jalons"]),
+        "--jalons",
+        str(args.jalons or DEFAULTS["jalons_ref"]),
+        "--ref-grid",
+        str(args.ref_grid or DEFAULTS["ref_grid"]),
+        "--samples",
+        str(args.samples_csv or DEFAULTS["samples_csv"]),
+        "--best-json",
+        str(best_json),
+        "--out",
+        str(args.jalons_out or DEFAULTS["jalons_csv"]),
+        "--log-level",
+        args.log_level,
     ]
     run_cmd(cmd, log)
     out = Path(args.jalons_out or DEFAULTS["jalons_csv"])
     log.info("   ✓ Évaluation jalons écrite : %s", bref(out))
     return out

-def etape_5_agregat(args, log: logging.Logger, jalons_csv: Path | None, results_csv: Path):
+
+def etape_5_agregat(
+    args, log: logging.Logger, jalons_csv: Path | None, results_csv: Path
+):
     if args.skip_aggregate:
         log.info("5) Agrégation & score — SKIP demandé")
-        return (Path(args.results_agg_csv or DEFAULTS["results_agg_csv"]), Path(args.best_json or DEFAULTS["best_json"]))
+        return (
+            Path(args.results_agg_csv or DEFAULTS["results_agg_csv"]),
+            Path(args.best_json or DEFAULTS["best_json"]),
+        )
     log.info("5) Agrégation, scoring multi-objectif & top-K final")
     cmd = [
-        sys.executable, str(SCRIPTS["agg"]),
-        "--results", str(results_csv),
-        "--out-results", str(args.results_agg_csv or DEFAULTS["results_agg_csv"]),
-        "--out-best", str(args.best_json or DEFAULTS["best_json"]),
-        "--K", str(args.K),
-        "--lambda", str(args.lmbda),
-        "--log-level", args.log_level,
+        sys.executable,
+        str(SCRIPTS["agg"]),
+        "--results",
+        str(results_csv),
+        "--out-results",
+        str(args.results_agg_csv or DEFAULTS["results_agg_csv"]),
+        "--out-best",
+        str(args.best_json or DEFAULTS["best_json"]),
+        "--K",
+        str(args.K),
+        "--lambda",
+        str(args.lmbda),
+        "--log-level",
+        args.log_level,
     ]
     if (not args.skip_jalons) and jalons_csv is not None and jalons_csv.exists():
         cmd += ["--jalons", str(jalons_csv)]
@@ -251,14 +310,18 @@ def etape_5_agregat(args, log: logging.Logger, jalons_csv: Path | None, results_
     run_cmd(cmd, log)
     out_agg = Path(args.results_agg_csv or DEFAULTS["results_agg_csv"])
     best = Path(args.best_json or DEFAULTS["best_json"])
-    log.info("   ✓ Résultats agrégés : %s | Top-K final : %s", bref(out_agg), bref(best))
+    log.info(
+        "   ✓ Résultats agrégés : %s | Top-K final : %s", bref(out_agg), bref(best)
+    )
     return (out_agg, best)

+
 # ----------------------- Raffinement (option) ------------------------
 def _charger_topk(best_json: Path) -> list[dict]:
     bj = load_json(best_json)
     return bj.get("top_k") or bj.get("topK") or []

+
 def _calcul_boite_topk(topk: list[dict], shrink: float) -> dict:
     """
     Boîte englobante du top-K sur {m1,m2,q0star,alpha}, resserrée par facteur 'shrink'.
@@ -266,7 +329,8 @@ def _calcul_boite_topk(topk: list[dict], shrink: float) -> dict:
     on réduit la demi-largeur -> demi_largeur/shrink.
     """
     import numpy as np
-    keys = ["m1","m2","q0star","alpha"]
+
+    keys = ["m1", "m2", "q0star", "alpha"]
     out = {}
     for k in keys:
         vals = np.array([float(t[k]) for t in topk if k in t], dtype=float)
@@ -277,21 +341,26 @@ def _calcul_boite_topk(topk: list[dict], shrink: float) -> dict:
         out[k] = {"min": vmed - half, "max": vmed + half}
     return out

-def _ecrire_config_raffine(cfg_base: Path, cfg_out: Path, boite: dict, log: logging.Logger):
+
+def _ecrire_config_raffine(
+    cfg_base: Path, cfg_out: Path, boite: dict, log: logging.Logger
+):
     base = load_json(cfg_base)
     # Adapter les bornes uniquement sur les 4 paramètres libres
-    for key in ["m1","m2","q0star","alpha"]:
+    for key in ["m1", "m2", "q0star", "alpha"]:
         if key in base.get("priors", {}):
             base["priors"][key]["min"] = float(boite[key]["min"])
             base["priors"][key]["max"] = float(boite[key]["max"])
     save_json(base, cfg_out)
     log.info("   ↪ Config de raffinement écrite : %s", bref(cfg_out))

+
 def _assurer_ids_uniques(samples_path: Path, id_offset: int, log: logging.Logger):
     """
     Re-numérotation simple : id := id + id_offset (garantit unicité lors de fusions).
     """
     import pandas as pd
+
     df = pd.read_csv(samples_path)
     if "id" not in df.columns:
         raise RuntimeError("Le fichier d'échantillons n'a pas de colonne 'id'.")
@@ -301,8 +370,14 @@ def _assurer_ids_uniques(samples_path: Path, id_offset: int, log: logging.Logger
     os.replace(tmp, samples_path)
     log.info("   ↪ IDs décalés de +%d dans %s", id_offset, bref(samples_path))

-def etape_6_raffinement(args, log: logging.Logger, best_json_final: Path,
-                        samples_global: Path, results_global: Path):
+
+def etape_6_raffinement(
+    args,
+    log: logging.Logger,
+    best_json_final: Path,
+    samples_global: Path,
+    results_global: Path,
+):
     if not args.refine:
         log.info("6) Raffinement global — SKIP (désactivé)")
         return None
@@ -316,24 +391,34 @@ def etape_6_raffinement(args, log: logging.Logger, best_json_final: Path,
     # 6.1 Boîte englobante restreinte
     boite = _calcul_boite_topk(topk, args.refine_shrink)
     cfg_refine = DDIR / "10_mc_config.refine.json"
-    _ecrire_config_raffine(Path(args.config or DEFAULTS["config"]), cfg_refine, boite, log)
+    _ecrire_config_raffine(
+        Path(args.config or DEFAULTS["config"]), cfg_refine, boite, log
+    )

     # 6.2 Génération des échantillons (raffiné)
     samples_ref = DDIR / "10_mc_samples.refine.csv"
     cmdS = [
-        sys.executable, str(SCRIPTS["samples"]),
-        "--config", str(cfg_refine),
-        "--n", str(args.refine_n),
-        "--scheme", "sobol",
-        "--scramble", "on" if args.scramble else "off",
-        "--seed", str(args.seed),
-        "--out", str(samples_ref),
+        sys.executable,
+        str(SCRIPTS["samples"]),
+        "--config",
+        str(cfg_refine),
+        "--n",
+        str(args.refine_n),
+        "--scheme",
+        "sobol",
+        "--scramble",
+        "on" if args.scramble else "off",
+        "--seed",
+        str(args.seed),
+        "--out",
+        str(samples_ref),
         "--overwrite",
     ]
     run_cmd(cmdS, log)

     # Décaler les ids pour unicité (à partir du max de l'existant)
     import pandas as pd
+
     dfG = pd.read_csv(samples_global, usecols=["id"])
     max_id = int(dfG["id"].max())
     _assurer_ids_uniques(samples_ref, id_offset=max_id, log=log)
@@ -342,15 +427,24 @@ def etape_6_raffinement(args, log: logging.Logger, best_json_final: Path,
     results_ref = DDIR / "10_mc_results.refine.csv"
     best_ref = DDIR / "10_mc_best.refine.json"  # provisoire
     cmdM = [
-        sys.executable, str(SCRIPTS["metrics"]),
-        "--samples", str(samples_ref),
-        "--ref-grid", str(args.ref_grid or DEFAULTS["ref_grid"]),
-        "--out-results", str(results_ref),
-        "--out-best", str(best_ref),
-        "--batch", str(args.batch),
-        "--n-workers", str(args.n_workers),
-        "--K", str(args.K),
-        "--log-level", args.log_level,
+        sys.executable,
+        str(SCRIPTS["metrics"]),
+        "--samples",
+        str(samples_ref),
+        "--ref-grid",
+        str(args.ref_grid or DEFAULTS["ref_grid"]),
+        "--out-results",
+        str(results_ref),
+        "--out-best",
+        str(best_ref),
+        "--batch",
+        str(args.batch),
+        "--n-workers",
+        str(args.n_workers),
+        "--K",
+        str(args.K),
+        "--log-level",
+        args.log_level,
         "--overwrite",
     ]
     run_cmd(cmdM, log)
@@ -367,40 +461,50 @@ def etape_6_raffinement(args, log: logging.Logger, best_json_final: Path,
     # On laisse l'agrégateur recalc le top-K final et réutiliser le même fichier jalons.
     return merged

-def etape_7_resume(args, log: logging.Logger, results_final_csv: Path, best_json_final: Path):
-    import pandas as pd, numpy as np
+
+def etape_7_resume(
+    args, log: logging.Logger, results_final_csv: Path, best_json_final: Path
+):
+    import pandas as pd
+    import numpy as np
+
     log.info("7) Résumé & manifeste pipeline")
     df = pd.read_csv(results_final_csv)
     n_total = int(len(df))
-    n_ok = int((df["status"]=="ok").sum()) if "status" in df.columns else n_total
+    n_ok = int((df["status"] == "ok").sum()) if "status" in df.columns else n_total
     n_failed = n_total - n_ok
     p95 = df["p95_20_300"].dropna().values if "p95_20_300" in df.columns else []
     resume = {
         "generated_at": datetime.now().astimezone().isoformat(),
         "inputs": {
             "ref_grid": bref(Path(args.ref_grid or DEFAULTS["ref_grid"])),
-            "jalons": bref(Path(args.jalons or DEFAULTS["jalons_ref"])) if not args.skip_jalons else None,
+            "jalons": bref(Path(args.jalons or DEFAULTS["jalons_ref"]))
+            if not args.skip_jalons
+            else None,
             "config": bref(Path(args.config or DEFAULTS["config"])),
         },
         "results": {
             "csv_final": bref(results_final_csv),
             "best_json": bref(best_json_final),
-            "n_total": n_total, "n_ok": n_ok, "n_failed": n_failed,
+            "n_total": n_total,
+            "n_ok": n_ok,
+            "n_failed": n_failed,
             "p95_min": float(np.min(p95)) if len(p95) else None,
             "p95_median": float(np.median(p95)) if len(p95) else None,
             "p95_max": float(np.max(p95)) if len(p95) else None,
-        }
+        },
     }
     save_json(resume, Path(args.summary or DEFAULTS["summary"]))
     log.info("   ✓ Résumé écrit : %s", bref(Path(args.summary or DEFAULTS["summary"])))

+
 # ---------------------------------------------------------------------
 # 3) Argumentaire CLI
 # ---------------------------------------------------------------------
 def build_parser():
     p = argparse.ArgumentParser(
         formatter_class=argparse.RawTextHelpFormatter,
-        description="Chapitre 10 — Pipeline complet de génération/agrégation des données."
+        description="Chapitre 10 — Pipeline complet de génération/agrégation des données.",
     )
     # Fichiers d’E/S principaux
     p.add_argument("--config", default=str(DEFAULTS["config"]))
@@ -414,8 +518,12 @@ def build_parser():
     p.add_argument("--summary", default=str(DEFAULTS["summary"]))

     # Contrôles généraux
-    p.add_argument("--log-level", default="INFO", choices=["DEBUG","INFO","WARNING","ERROR"])
-    p.add_argument("--overwrite", action="store_true", help="autoriser l'écrasement des sorties")
+    p.add_argument(
+        "--log-level", default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR"]
+    )
+    p.add_argument(
+        "--overwrite", action="store_true", help="autoriser l'écrasement des sorties"
+    )
     p.add_argument("--skip-samples", action="store_true")
     p.add_argument("--skip-metrics", action="store_true")
     p.add_argument("--skip-jalons", action="store_true")
@@ -425,7 +533,12 @@ def build_parser():
     p.add_argument("--n", type=int, default=5000)
     p.add_argument("--scramble", default=True, action=argparse.BooleanOptionalAction)
     p.add_argument("--seed", type=int, default=12345)
-    p.add_argument("--sobol-offset", type=int, default=None, help="reprise/append de la séquence sobol")
+    p.add_argument(
+        "--sobol-offset",
+        type=int,
+        default=None,
+        help="reprise/append de la séquence sobol",
+    )

     # Paramètres métriques / agrégation
     p.add_argument("--batch", type=int, default=256)
@@ -434,19 +547,31 @@ def build_parser():
     p.add_argument("--lambda", dest="lmbda", type=float, default=0.2)

     # Raffinement global (option)
-    p.add_argument("--refine", action="store_true", help="activer un second lot raffiné (global bbox du top-K)")
+    p.add_argument(
+        "--refine",
+        action="store_true",
+        help="activer un second lot raffiné (global bbox du top-K)",
+    )
     p.add_argument("--refine-n", type=int, default=10000)
-    p.add_argument("--refine-shrink", type=float, default=3.0, help="facteur de rétrécissement par dimension (>1)")
+    p.add_argument(
+        "--refine-shrink",
+        type=float,
+        default=3.0,
+        help="facteur de rétrécissement par dimension (>1)",
+    )

     return p

+
 # ---------------------------------------------------------------------
 # 4) Main
 # ---------------------------------------------------------------------
 def main(argv=None):
     args = build_parser().parse_args(argv)
-    logging.basicConfig(level=getattr(logging, args.log_level),
-                        format="%(asctime)s [%(levelname)s] %(message)s")
+    logging.basicConfig(
+        level=getattr(logging, args.log_level),
+        format="%(asctime)s [%(levelname)s] %(message)s",
+    )
     log = logging.getLogger("mcgt.pipeline10")

     t0 = time.time()
@@ -464,10 +589,14 @@ def main(argv=None):
         jalons_csv = etape_4_jalons(args, log, best_json)

         # 5) Agrégation (global)
-        results_agg_csv, best_json_final = etape_5_agregat(args, log, jalons_csv, results_csv)
+        results_agg_csv, best_json_final = etape_5_agregat(
+            args, log, jalons_csv, results_csv
+        )

         # 6) Raffinement (option) puis re-agrégation sur la fusion
-        merged_csv = etape_6_raffinement(args, log, best_json_final, samples_csv, results_csv)
+        merged_csv = etape_6_raffinement(
+            args, log, best_json_final, samples_csv, results_csv
+        )
         if merged_csv is not None:
             # On relance une agrégation sur la fusion (avec les mêmes jalons)
             results_agg_csv, best_json_final = etape_5_agregat(
@@ -488,5 +617,6 @@ def main(argv=None):
         log.exception("Échec pipeline: %s", e)
         return 2

+
 if __name__ == "__main__":
     sys.exit(main())
diff --git a/zz-scripts/chapter10/inspect_topk_residuals.py b/zz-scripts/chapter10/inspect_topk_residuals.py
index 8f153fd..c3b49cb 100755
--- a/zz-scripts/chapter10/inspect_topk_residuals.py
+++ b/zz-scripts/chapter10/inspect_topk_residuals.py
@@ -7,70 +7,83 @@ Inspect top-K / best candidates:
 - sauvegarde overlays + histogramme des résidus (PNG)
 Usage: python zz-scripts/chapter10/inspect_topk_residuals.py --ids 3903,1624,...
 """
-import json, argparse, os
+
+import json
+import argparse
+import os
 import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt
 from mcgt.backends.ref_phase import compute_phi_ref
 from mcgt.phase import phi_mcgt

-def compute_abs_dphi_principal(phi_m, phi_r, f_Hz, window=(20.0,300.0)):
+
+def compute_abs_dphi_principal(phi_m, phi_r, f_Hz, window=(20.0, 300.0)):
     # mask window
     mask = (f_Hz >= window[0]) & (f_Hz <= window[1])
     if not mask.any():
         raise ValueError("Fenêtre vide")
     # calculate k as rounded median((phi_m - phi_r)/(2π)) on window, no unwrap
-    cycles = (phi_m[mask] - phi_r[mask]) / (2*np.pi)
+    cycles = (phi_m[mask] - phi_r[mask]) / (2 * np.pi)
     k = int(np.round(np.nanmedian(cycles)))
     # principal residual on full grid
-    dphi = ((phi_m - k*2*np.pi) - phi_r + np.pi) % (2*np.pi) - np.pi
+    dphi = ((phi_m - k * 2 * np.pi) - phi_r + np.pi) % (2 * np.pi) - np.pi
     return np.abs(dphi), k

+
 def main(args):
     # load files
     samples = pd.read_csv(args.samples).set_index("id")
-    ref = np.loadtxt(args.ref_grid, delimiter=',', skiprows=1, usecols=[0])
+    ref = np.loadtxt(args.ref_grid, delimiter=",", skiprows=1, usecols=[0])
     if args.ids:
         ids = [int(x) for x in args.ids.split(",")]
     else:
         best = json.load(open(args.best_json))["top_k"]
-        ids = [int(x["id"]) for x in best[:args.k]]
+        ids = [int(x["id"]) for x in best[: args.k]]
     os.makedirs(args.out_dir, exist_ok=True)

     for id_ in ids:
         row = samples.loc[int(id_)]
-        theta = dict(row[["m1","m2","q0star","alpha","phi0","tc","dist","incl"]])
+        theta = dict(row[["m1", "m2", "q0star", "alpha", "phi0", "tc", "dist", "incl"]])
         phi_r = compute_phi_ref(ref, float(row.m1), float(row.m2))
         phi_m = phi_mcgt(ref, theta)
-        abs_dphi, k = compute_abs_dphi_principal(phi_m, phi_r, ref, window=(20.0,300.0))
+        abs_dphi, k = compute_abs_dphi_principal(
+            phi_m, phi_r, ref, window=(20.0, 300.0)
+        )
         # save CSV
         outcsv = os.path.join(args.out_dir, f"10_topresiduals_id{id_:d}.csv")
-        df = pd.DataFrame({
-            "f_Hz": ref,
-            "phi_ref": phi_r,
-            "phi_mcgt": phi_m,
-            "abs_dphi_principal": abs_dphi
-        })
+        df = pd.DataFrame(
+            {
+                "f_Hz": ref,
+                "phi_ref": phi_r,
+                "phi_mcgt": phi_m,
+                "abs_dphi_principal": abs_dphi,
+            }
+        )
         df.to_csv(outcsv, index=False, float_format="%.12f")
         print(f"Wrote {outcsv} (k={k})")
         # overlay PNG
-        fig, ax = plt.subplots(figsize=(8,4))
+        fig, ax = plt.subplots(figsize=(8, 4))
         ax.semilogx(ref, phi_r, label="phi_ref")
         ax.semilogx(ref, phi_m, label="phi_mcgt")
-        ax.set_xlabel("f [Hz]"); ax.set_ylabel("phase [rad]")
+        ax.set_xlabel("f [Hz]")
+        ax.set_ylabel("phase [rad]")
         ax.set_title(f"Overlay id={id_:d} (k={k})")
         ax.legend()
         f_png = os.path.join(args.out_dir, f"overlay_id{id_:d}.png")
         fig.savefig(f_png, dpi=150)
         plt.close(fig)
         # resid histogram
-        fig, ax = plt.subplots(figsize=(6,3))
-        ax.hist(df.loc[(df.f_Hz>=20)&(df.f_Hz<=300),"abs_dphi_principal"], bins=30)
+        fig, ax = plt.subplots(figsize=(6, 3))
+        ax.hist(
+            df.loc[(df.f_Hz >= 20) & (df.f_Hz <= 300), "abs_dphi_principal"], bins=30
+        )
         ax.set_xlabel("|Δφ_principal| (20–300 Hz) [rad]")
         ax.set_ylabel("counts")
         fig.savefig(os.path.join(args.out_dir, f"dphi_hist_id{id_:d}.png"), dpi=150)
         plt.close(fig)

+
 if __name__ == "__main__":
     p = argparse.ArgumentParser()
     p.add_argument("--samples", default="zz-data/chapter10/10_mc_samples.csv")
diff --git a/zz-scripts/chapter10/plot_fig01_iso_p95_maps.py b/zz-scripts/chapter10/plot_fig01_iso_p95_maps.py
index 37aa9c3..2204d58 100755
--- a/zz-scripts/chapter10/plot_fig01_iso_p95_maps.py
+++ b/zz-scripts/chapter10/plot_fig01_iso_p95_maps.py
@@ -4,6 +4,7 @@
 plot_fig01_iso_p95_maps.py

 """
+
 from __future__ import annotations
 import argparse
 import warnings
@@ -16,13 +17,18 @@ import sys

 # ---------- utilities ----------

+
 def detect_p95_column(df: pd.DataFrame, hint: str | None):
     """Try to find the p95 column using hint or sensible defaults."""
     if hint and hint in df.columns:
         return hint
     candidates = [
-        "p95_20_300_recalc", "p95_20_300_circ", "p95_20_300",
-        "p95_circ", "p95_recalc", "p95"
+        "p95_20_300_recalc",
+        "p95_20_300_circ",
+        "p95_20_300",
+        "p95_circ",
+        "p95_recalc",
+        "p95",
     ]
     for c in candidates:
         if c in df.columns:
@@ -32,6 +38,7 @@ def detect_p95_column(df: pd.DataFrame, hint: str | None):
             return c
     raise KeyError("Aucune colonne 'p95' détectée dans le fichier results.")

+
 def read_and_validate(path, m1_col, m2_col, p95_col):
     """Read CSV and validate presence of required columns. Return trimmed DataFrame."""
     try:
@@ -47,6 +54,7 @@ def read_and_validate(path, m1_col, m2_col, p95_col):
         raise ValueError("Aucune donnée valide après suppression des NaN.")
     return df

+
 def make_triangulation_and_mask(x, y):
     """
     Build a triangulation for scattered (x,y). Return triang and a simple
@@ -55,8 +63,12 @@ def make_triangulation_and_mask(x, y):
     triang = tri.Triangulation(x, y)
     try:
         tris = triang.triangles
-        x1 = x[tris[:, 0]]; x2 = x[tris[:, 1]]; x3 = x[tris[:, 2]]
-        y1 = y[tris[:, 0]]; y2 = y[tris[:, 1]]; y3 = y[tris[:, 2]]
+        x1 = x[tris[:, 0]]
+        x2 = x[tris[:, 1]]
+        x3 = x[tris[:, 2]]
+        y1 = y[tris[:, 0]]
+        y2 = y[tris[:, 1]]
+        y3 = y[tris[:, 2]]
         areas = 0.5 * np.abs((x2 - x1) * (y3 - y1) - (x3 - x1) * (y2 - y1))
         mask = areas <= 0.0
         # triang.set_mask expects boolean mask with same length as triangles
@@ -66,20 +78,36 @@ def make_triangulation_and_mask(x, y):
         pass
     return triang

+
 # ---------- main ----------

+
 def main():
     ap = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
-    ap.add_argument("--results", required=True, help="CSV results (must contain m1,m2 and p95).")
-    ap.add_argument("--p95-col", default=None, help="p95 column name (auto detect if omitted)")
+    ap.add_argument(
+        "--results", required=True, help="CSV results (must contain m1,m2 and p95)."
+    )
+    ap.add_argument(
+        "--p95-col", default=None, help="p95 column name (auto detect if omitted)"
+    )
     ap.add_argument("--m1-col", default="m1", help="column name for m1")
     ap.add_argument("--m2-col", default="m2", help="column name for m2")
-    ap.add_argument("--out", default="zz-figures/chapter10/fig_01_iso_map_p95.png", help="output PNG file")
+    ap.add_argument(
+        "--out",
+        default="zz-figures/chapter10/fig_01_iso_map_p95.png",
+        help="output PNG file",
+    )
     ap.add_argument("--levels", type=int, default=16, help="number of contour levels")
     ap.add_argument("--cmap", default="viridis", help="colormap")
     ap.add_argument("--dpi", type=int, default=150, help="png dpi")
-    ap.add_argument("--title", default="Carte iso de p95 (m1 vs m2)", help="figure title")
-    ap.add_argument("--no-clip", action="store_true", help="do not clip color scale to percentiles (show full range)")
+    ap.add_argument(
+        "--title", default="Carte iso de p95 (m1 vs m2)", help="figure title"
+    )
+    ap.add_argument(
+        "--no-clip",
+        action="store_true",
+        help="do not clip color scale to percentiles (show full range)",
+    )
     args = ap.parse_args()

     # Read & detect columns
@@ -143,7 +171,9 @@ def main():
     cs = ax.tricontour(triang, z, levels=levels, colors="k", linewidths=0.45, alpha=0.5)

     # scatter overlay (points) - smaller, semi-transparent
-    ax.scatter(x, y, c='k', s=3, alpha=0.5, edgecolors='none', label="échantillons", zorder=5)
+    ax.scatter(
+        x, y, c="k", s=3, alpha=0.5, edgecolors="none", label="échantillons", zorder=5
+    )

     # colorbar (respect the norm)
     cbar = fig.colorbar(cf, ax=ax, shrink=0.8)
@@ -176,10 +206,13 @@ def main():
         fig.savefig(args.out, dpi=args.dpi)
         print(f"Wrote: {args.out}")
         if clipped:
-            print("Note: color scaling was clipped to percentiles (0.1%/99.9%). Use --no-clip to disable clipping.")
+            print(
+                "Note: color scaling was clipped to percentiles (0.1%/99.9%). Use --no-clip to disable clipping."
+            )
     except Exception as e:
         print(f"[ERROR] cannot write output file '{args.out}': {e}", file=sys.stderr)
         sys.exit(2)

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter10/plot_fig03_convergence_p95_vs_n.py b/zz-scripts/chapter10/plot_fig03_convergence_p95_vs_n.py
index d6ea1a4..64a2bd5 100755
--- a/zz-scripts/chapter10/plot_fig03_convergence_p95_vs_n.py
+++ b/zz-scripts/chapter10/plot_fig03_convergence_p95_vs_n.py
@@ -4,6 +4,7 @@
 plot_fig03_convergence_p95_vs_n.py

 """
+
 from __future__ import annotations
 import argparse
 import numpy as np
@@ -15,7 +16,13 @@ from mpl_toolkits.axes_grid1.inset_locator import inset_axes
 def detect_p95_column(df: pd.DataFrame, hint: str | None):
     if hint and hint in df.columns:
         return hint
-    for c in ["p95_20_300_recalc", "p95_20_300_circ", "p95_20_300", "p95_circ", "p95_recalc"]:
+    for c in [
+        "p95_20_300_recalc",
+        "p95_20_300_circ",
+        "p95_20_300",
+        "p95_circ",
+        "p95_recalc",
+    ]:
         if c in df.columns:
             return c
     for c in df.columns:
@@ -33,10 +40,12 @@ def trimmed_mean(arr: np.ndarray, alpha: float) -> float:
     if 2 * k >= n:
         return float(np.mean(arr))
     a = np.sort(arr)
-    return float(np.mean(a[k:n - k]))
+    return float(np.mean(a[k : n - k]))


-def compute_bootstrap_convergence(p95: np.ndarray, N_list: np.ndarray, B: int, seed: int, trim_alpha: float):
+def compute_bootstrap_convergence(
+    p95: np.ndarray, N_list: np.ndarray, B: int, seed: int, trim_alpha: float
+):
     rng = np.random.default_rng(seed)
     npoints = len(N_list)

@@ -68,24 +77,55 @@ def compute_bootstrap_convergence(p95: np.ndarray, N_list: np.ndarray, B: int, s
         median_low[i], median_high[i] = np.percentile(ests_median, [2.5, 97.5])
         tmean_low[i], tmean_high[i] = np.percentile(ests_tmean, [2.5, 97.5])

-    return (mean_est, mean_low, mean_high,
-            median_est, median_low, median_high,
-            tmean_est, tmean_low, tmean_high)
+    return (
+        mean_est,
+        mean_low,
+        mean_high,
+        median_est,
+        median_low,
+        median_high,
+        tmean_est,
+        tmean_low,
+        tmean_high,
+    )


 def main():
     p = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
     p.add_argument("--results", required=True, help="CSV results (with p95 column)")
-    p.add_argument("--p95-col", default=None, help="Nom de la colonne p95 (auto si omis)")
-    p.add_argument("--out", default="zz-figures/chapter10/fig_03_convergence_p95_vs_n.png", help="PNG de sortie")
+    p.add_argument(
+        "--p95-col", default=None, help="Nom de la colonne p95 (auto si omis)"
+    )
+    p.add_argument(
+        "--out",
+        default="zz-figures/chapter10/fig_03_convergence_p95_vs_n.png",
+        help="PNG de sortie",
+    )
     p.add_argument("--B", type=int, default=2000, help="Nombre de réplicats bootstrap")
     p.add_argument("--seed", type=int, default=12345, help="Seed RNG")
     p.add_argument("--dpi", type=int, default=150, help="DPI PNG")
     p.add_argument("--npoints", type=int, default=100, help="Nb de valeurs N évaluées")
-    p.add_argument("--trim", type=float, default=0.05, help="Proportion tronquée de chaque côté (trimmed mean)")
-    p.add_argument("--zoom-center-n", type=int, default=None, help="Centre en N (par défaut ~M/2)")
-    p.add_argument("--zoom-w", type=float, default=0.35, help="Largeur de base de l'encart (fraction figure)")
-    p.add_argument("--zoom-h", type=float, default=0.20, help="Hauteur de base de l'encart (fraction figure)")
+    p.add_argument(
+        "--trim",
+        type=float,
+        default=0.05,
+        help="Proportion tronquée de chaque côté (trimmed mean)",
+    )
+    p.add_argument(
+        "--zoom-center-n", type=int, default=None, help="Centre en N (par défaut ~M/2)"
+    )
+    p.add_argument(
+        "--zoom-w",
+        type=float,
+        default=0.35,
+        help="Largeur de base de l'encart (fraction figure)",
+    )
+    p.add_argument(
+        "--zoom-h",
+        type=float,
+        default=0.20,
+        help="Hauteur de base de l'encart (fraction figure)",
+    )
     args = p.parse_args()

     df = pd.read_csv(args.results)
@@ -104,25 +144,65 @@ def main():
     ref_median = float(np.median(p95))
     ref_tmean = trimmed_mean(p95, args.trim)

-    print(f"[INFO] Bootstrap convergence: M={M}, B={args.B}, points={len(N_list)}, seed={args.seed}, trim={args.trim:.3f}")
-    (mean_est, mean_low, mean_high,
-     median_est, median_low, median_high,
-     tmean_est, tmean_low, tmean_high) = compute_bootstrap_convergence(p95, N_list, args.B, args.seed, args.trim)
+    print(
+        f"[INFO] Bootstrap convergence: M={M}, B={args.B}, points={len(N_list)}, seed={args.seed}, trim={args.trim:.3f}"
+    )
+    (
+        mean_est,
+        mean_low,
+        mean_high,
+        median_est,
+        median_low,
+        median_high,
+        tmean_est,
+        tmean_low,
+        tmean_high,
+    ) = compute_bootstrap_convergence(p95, N_list, args.B, args.seed, args.trim)

     final_i = np.where(N_list == M)[0][0] if (N_list == M).any() else -1
-    final_mean, final_mean_ci = mean_est[final_i], (mean_low[final_i], mean_high[final_i])
-    final_median, final_median_ci = median_est[final_i], (median_low[final_i], median_high[final_i])
-    final_tmean, final_tmean_ci = tmean_est[final_i], (tmean_low[final_i], tmean_high[final_i])
+    final_mean, final_mean_ci = (
+        mean_est[final_i],
+        (mean_low[final_i], mean_high[final_i]),
+    )
+    final_median, final_median_ci = (
+        median_est[final_i],
+        (median_low[final_i], median_high[final_i]),
+    )
+    final_tmean, final_tmean_ci = (
+        tmean_est[final_i],
+        (tmean_low[final_i], tmean_high[final_i]),
+    )

     plt.style.use("classic")
     fig, ax = plt.subplots(figsize=(14, 6))

-    ax.fill_between(N_list, mean_low, mean_high, color='tab:blue', alpha=0.18, label="IC 95% (bootstrap, mean)")
-    ax.plot(N_list, mean_est,   color='tab:blue',   lw=2.0, label="Estimateur (mean)")
-    ax.plot(N_list, median_est, color='tab:orange', lw=1.6, ls='--', label="Estimateur (median)")
-    ax.plot(N_list, tmean_est,  color='tab:green',  lw=1.6, ls='-.', label=f"Estimateur (trimmed mean, α={args.trim:.2f})")
-
-    ax.axhline(ref_mean, color='crimson', lw=2, label=f"Estimation à N={M} (mean réf)")
+    ax.fill_between(
+        N_list,
+        mean_low,
+        mean_high,
+        color="tab:blue",
+        alpha=0.18,
+        label="IC 95% (bootstrap, mean)",
+    )
+    ax.plot(N_list, mean_est, color="tab:blue", lw=2.0, label="Estimateur (mean)")
+    ax.plot(
+        N_list,
+        median_est,
+        color="tab:orange",
+        lw=1.6,
+        ls="--",
+        label="Estimateur (median)",
+    )
+    ax.plot(
+        N_list,
+        tmean_est,
+        color="tab:green",
+        lw=1.6,
+        ls="-.",
+        label=f"Estimateur (trimmed mean, α={args.trim:.2f})",
+    )
+
+    ax.axhline(ref_mean, color="crimson", lw=2, label=f"Estimation à N={M} (mean réf)")

     ax.set_xlim(0, M)
     ax.set_xlabel("Taille d'échantillon N")
@@ -143,31 +223,47 @@ def main():

     sel = (N_list >= xin0) & (N_list <= xin1)
     if np.sum(sel) == 0:
-        sel = slice(len(N_list)//3, 2*len(N_list)//3)
-        ylo = np.min(mean_low[sel]); yhi = np.max(mean_high[sel])
+        sel = slice(len(N_list) // 3, 2 * len(N_list) // 3)
+        ylo = np.min(mean_low[sel])
+        yhi = np.max(mean_high[sel])
     else:
-        ylo = float(np.nanmin(mean_low[sel])); yhi = float(np.nanmax(mean_high[sel]))
+        ylo = float(np.nanmin(mean_low[sel]))
+        yhi = float(np.nanmax(mean_high[sel]))
     ypad = 0.02 * (yhi - ylo) if (yhi - ylo) > 0 else 0.005
     yin0, yin1 = ylo - ypad, yhi + ypad

     inset_x = 0.62 - inset_w / 2.0
     inset_y = 0.18
-    inset_ax = inset_axes(ax,
-                          width=f"{inset_w*100}%", height=f"{inset_h*100}%",
-                          bbox_to_anchor=(inset_x, inset_y, inset_w, inset_h),
-                          bbox_transform=fig.transFigure,
-                          loc='lower left', borderpad=1)
+    inset_ax = inset_axes(
+        ax,
+        width=f"{inset_w*100}%",
+        height=f"{inset_h*100}%",
+        bbox_to_anchor=(inset_x, inset_y, inset_w, inset_h),
+        bbox_transform=fig.transFigure,
+        loc="lower left",
+        borderpad=1,
+    )

     sel_idx = (N_list >= xin0) & (N_list <= xin1)
-    inset_ax.fill_between(N_list[sel_idx], mean_low[sel_idx], mean_high[sel_idx], color='tab:blue', alpha=0.18)
-    inset_ax.plot(N_list[sel_idx], mean_est[sel_idx],   color='tab:blue',   lw=1.5)
-    inset_ax.plot(N_list[sel_idx], median_est[sel_idx], color='tab:orange', lw=1.2, ls='--')
-    inset_ax.plot(N_list[sel_idx], tmean_est[sel_idx],  color='tab:green',  lw=1.2, ls='-.')
-    inset_ax.axhline(ref_mean, color='crimson', lw=1.0, ls='--')
+    inset_ax.fill_between(
+        N_list[sel_idx],
+        mean_low[sel_idx],
+        mean_high[sel_idx],
+        color="tab:blue",
+        alpha=0.18,
+    )
+    inset_ax.plot(N_list[sel_idx], mean_est[sel_idx], color="tab:blue", lw=1.5)
+    inset_ax.plot(
+        N_list[sel_idx], median_est[sel_idx], color="tab:orange", lw=1.2, ls="--"
+    )
+    inset_ax.plot(
+        N_list[sel_idx], tmean_est[sel_idx], color="tab:green", lw=1.2, ls="-."
+    )
+    inset_ax.axhline(ref_mean, color="crimson", lw=1.0, ls="--")
     inset_ax.set_xlim(xin0, xin1)
     inset_ax.set_ylim(yin0, yin1)
     inset_ax.set_title("zoom (mean)", fontsize=10)
-    inset_ax.tick_params(axis='both', which='major', labelsize=8)
+    inset_ax.tick_params(axis="both", which="major", labelsize=8)
     inset_ax.grid(False)

     stat_lines = [
@@ -179,14 +275,26 @@ def main():
         f"bootstrap = percentile, B = {args.B}, seed = {args.seed}",
     ]
     bbox = dict(boxstyle="round", fc="white", ec="black", lw=1, alpha=0.95)
-    ax.text(0.98, 0.28, "\n".join(stat_lines),
-            transform=ax.transAxes, fontsize=9,
-            va='bottom', ha='right', bbox=bbox, zorder=20)
-
-    fig.text(0.5, 0.02,
-             f"Bootstrap (B={args.B}, percentile) sur {M} échantillons. "
-             f"Estimateurs tracés = mean (solid), median (dashed), trimmed mean (dash-dot, α={args.trim:.2f}).",
-             ha='center', fontsize=9)
+    ax.text(
+        0.98,
+        0.28,
+        "\n".join(stat_lines),
+        transform=ax.transAxes,
+        fontsize=9,
+        va="bottom",
+        ha="right",
+        bbox=bbox,
+        zorder=20,
+    )
+
+    fig.text(
+        0.5,
+        0.02,
+        f"Bootstrap (B={args.B}, percentile) sur {M} échantillons. "
+        f"Estimateurs tracés = mean (solid), median (dashed), trimmed mean (dash-dot, α={args.trim:.2f}).",
+        ha="center",
+        fontsize=9,
+    )

     plt.tight_layout(rect=[0, 0.05, 1, 0.97])
     fig.savefig(args.out, dpi=args.dpi)
diff --git a/zz-scripts/chapter10/plot_fig03b_bootstrap_coverage_vs_n.py b/zz-scripts/chapter10/plot_fig03b_bootstrap_coverage_vs_n.py
index 068055e..b4137db 100755
--- a/zz-scripts/chapter10/plot_fig03b_bootstrap_coverage_vs_n.py
+++ b/zz-scripts/chapter10/plot_fig03b_bootstrap_coverage_vs_n.py
@@ -4,6 +4,7 @@
 plot_fig03b_coverage_bootstrap_vs_n.py

 """
+
 from __future__ import annotations

 import argparse
@@ -19,10 +20,18 @@ from mpl_toolkits.axes_grid1.inset_locator import inset_axes

 # ----------------------------- utilitaires ---------------------------------

+
 def detect_p95_column(df: pd.DataFrame, hint: str | None) -> str:
     if hint and hint in df.columns:
         return hint
-    for c in ["p95_20_300_recalc", "p95_20_300_circ", "p95_20_300", "p95_circ", "p95_recalc", "p95"]:
+    for c in [
+        "p95_20_300_recalc",
+        "p95_20_300_circ",
+        "p95_20_300",
+        "p95_circ",
+        "p95_recalc",
+        "p95",
+    ]:
         if c in df.columns:
             return c
     for c in df.columns:
@@ -30,35 +39,40 @@ def detect_p95_column(df: pd.DataFrame, hint: str | None) -> str:
             return c
     raise KeyError("Aucune colonne p95 détectée (utiliser --p95-col).")

+
 def wilson_err95(p: float, n: int) -> tuple[float, float]:
     """Retourne (err_bas, err_haut) Wilson 95% pour une proportion p sur n."""
     if n <= 0:
         return 0.0, 0.0
     z = 1.959963984540054  # 97.5e percentile
-    denom = 1.0 + (z*z)/n
-    center = (p + (z*z)/(2*n)) / denom
-    half = (z/denom) * np.sqrt((p*(1-p)/n) + (z*z)/(4*n*n))
+    denom = 1.0 + (z * z) / n
+    center = (p + (z * z) / (2 * n)) / denom
+    half = (z / denom) * np.sqrt((p * (1 - p) / n) + (z * z) / (4 * n * n))
     lo = max(0.0, center - half)
     hi = min(1.0, center + half)
     return (p - lo, hi - p)

-def bootstrap_percentile_ci(vals: np.ndarray, B: int, rng: np.random.Generator,
-                            alpha: float = 0.05) -> tuple[float, float]:
+
+def bootstrap_percentile_ci(
+    vals: np.ndarray, B: int, rng: np.random.Generator, alpha: float = 0.05
+) -> tuple[float, float]:
     """IC percentile (95% par défaut) pour la moyenne linéaire."""
     n = len(vals)
     boots = np.empty(B, dtype=float)
     for b in range(B):
         samp = rng.choice(vals, size=n, replace=True)
         boots[b] = float(np.mean(samp))
-    lo = float(np.percentile(boots, 100*(alpha/2)))
-    hi = float(np.percentile(boots, 100*(1 - alpha/2)))
+    lo = float(np.percentile(boots, 100 * (alpha / 2)))
+    hi = float(np.percentile(boots, 100 * (1 - alpha / 2)))
     return lo, hi

+
 def circ_mean_rad(angles: np.ndarray) -> float:
     """Moyenne circulaire d'angles (radians)."""
     z = np.mean(np.exp(1j * angles))
     return float(np.angle(z))

+
 @dataclass
 class RowRes:
     N: int
@@ -69,37 +83,87 @@ class RowRes:
     n_hits: int
     method: str

+
 # ------------------------------- coeur --------------------------------------

+
 def main():
     p = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
     p.add_argument("--results", required=True, help="CSV avec colonne p95.")
     p.add_argument("--p95-col", default=None, help="Nom exact de la colonne p95.")
-    p.add_argument("--out", default="zz-figures/chapter10/fig_03b_coverage_bootstrap_vs_n.png", help="PNG de sortie")
-    p.add_argument("--outer", type=int, default=400, help="Nombre de réplicats externes (couverture).")
-    p.add_argument("--M", type=int, default=None, help="Alias de --outer (si précisé, remplace --outer).")
-    p.add_argument("--inner", type=int, default=2000, help="Nombre de réplicats internes (IC).")
-    p.add_argument("--alpha", type=float, default=0.05, help="Niveau d'erreur pour IC (ex. 0.05).")
+    p.add_argument(
+        "--out",
+        default="zz-figures/chapter10/fig_03b_coverage_bootstrap_vs_n.png",
+        help="PNG de sortie",
+    )
+    p.add_argument(
+        "--outer",
+        type=int,
+        default=400,
+        help="Nombre de réplicats externes (couverture).",
+    )
+    p.add_argument(
+        "--M",
+        type=int,
+        default=None,
+        help="Alias de --outer (si précisé, remplace --outer).",
+    )
+    p.add_argument(
+        "--inner", type=int, default=2000, help="Nombre de réplicats internes (IC)."
+    )
+    p.add_argument(
+        "--alpha", type=float, default=0.05, help="Niveau d'erreur pour IC (ex. 0.05)."
+    )
     p.add_argument("--npoints", type=int, default=10, help="Nombre de points N.")
     p.add_argument("--minN", type=int, default=100, help="Plus petit N.")
     p.add_argument("--seed", type=int, default=12345, help="Seed RNG.")
     p.add_argument("--dpi", type=int, default=300, help="DPI PNG.")
-    p.add_argument("--ymin-coverage", type=float, default=None, help="Ymin panneau couverture.")
-    p.add_argument("--ymax-coverage", type=float, default=None, help="Ymax panneau couverture.")
-    p.add_argument("--title-left", default="Couverture IC vs N (estimateur: mean)", help="Titre panneau gauche.")
-    p.add_argument("--title-right", default="Largeur d'IC vs N", help="Titre panneau droit.")
-    p.add_argument("--hires2000", action="store_true",
-                   help="Utiliser outer=2000, inner=2000 (ne change pas les défauts globaux).")
-    p.add_argument("--angular", action="store_true",
-                   help="Active l'encart comparant moyenne linéaire vs moyenne circulaire (p95 en radians).")
-    p.add_argument("--make-sensitivity", action="store_true",
-                   help="Produit une figure annexe de sensibilité (coverage vs outer/inner).")
-    p.add_argument("--sens-mode", choices=["outer", "inner"], default="outer",
-                   help="Paramètre de sensibilité (outer ou inner).")
-    p.add_argument("--sens-N", type=int, default=None,
-                   help="N fixe utilisé pour la sensibilité (défaut: N max du dataset).")
-    p.add_argument("--sens-B-list", default="100,200,400,800,1200,2000",
-                   help="Liste de B séparés par virgules pour la sensibilité.")
+    p.add_argument(
+        "--ymin-coverage", type=float, default=None, help="Ymin panneau couverture."
+    )
+    p.add_argument(
+        "--ymax-coverage", type=float, default=None, help="Ymax panneau couverture."
+    )
+    p.add_argument(
+        "--title-left",
+        default="Couverture IC vs N (estimateur: mean)",
+        help="Titre panneau gauche.",
+    )
+    p.add_argument(
+        "--title-right", default="Largeur d'IC vs N", help="Titre panneau droit."
+    )
+    p.add_argument(
+        "--hires2000",
+        action="store_true",
+        help="Utiliser outer=2000, inner=2000 (ne change pas les défauts globaux).",
+    )
+    p.add_argument(
+        "--angular",
+        action="store_true",
+        help="Active l'encart comparant moyenne linéaire vs moyenne circulaire (p95 en radians).",
+    )
+    p.add_argument(
+        "--make-sensitivity",
+        action="store_true",
+        help="Produit une figure annexe de sensibilité (coverage vs outer/inner).",
+    )
+    p.add_argument(
+        "--sens-mode",
+        choices=["outer", "inner"],
+        default="outer",
+        help="Paramètre de sensibilité (outer ou inner).",
+    )
+    p.add_argument(
+        "--sens-N",
+        type=int,
+        default=None,
+        help="N fixe utilisé pour la sensibilité (défaut: N max du dataset).",
+    )
+    p.add_argument(
+        "--sens-B-list",
+        default="100,200,400,800,1200,2000",
+        help="Liste de B séparés par virgules pour la sensibilité.",
+    )
     args = p.parse_args()

     df = pd.read_csv(args.results)
@@ -124,7 +188,9 @@ def main():
     print(f"[INFO] N_list = {N_list.tolist()}")

     outer_for_cov = int(args.M) if args.M is not None else int(args.outer)
-    print(f"[INFO] outer={outer_for_cov}, inner={args.inner}, alpha={args.alpha}, seed={args.seed}")
+    print(
+        f"[INFO] outer={outer_for_cov}, inner={args.inner}, alpha={args.alpha}, seed={args.seed}"
+    )

     rng = np.random.default_rng(args.seed)
     ref_value_lin = float(np.mean(vals_all))
@@ -142,16 +208,20 @@ def main():
                 hits += 1
         p_hat = hits / outer_for_cov
         e_lo, e_hi = wilson_err95(p_hat, outer_for_cov)
-        results.append(RowRes(
-            N=int(N),
-            coverage=float(p_hat),
-            cov_err95_low=float(e_lo),
-            cov_err95_high=float(e_hi),
-            width_mean=float(np.mean(widths)),
-            n_hits=int(hits),
-            method="percentile"
-        ))
-        print(f"[{idx}/{len(N_list)}] N={N:5d}  coverage={p_hat:0.3f}  width_mean={np.mean(widths):0.5f} rad")
+        results.append(
+            RowRes(
+                N=int(N),
+                coverage=float(p_hat),
+                cov_err95_low=float(e_lo),
+                cov_err95_high=float(e_hi),
+                width_mean=float(np.mean(widths)),
+                n_hits=int(hits),
+                method="percentile",
+            )
+        )
+        print(
+            f"[{idx}/{len(N_list)}] N={N:5d}  coverage={p_hat:0.3f}  width_mean={np.mean(widths):0.5f} rad"
+        )

     plt.style.use("classic")
     fig = plt.figure(figsize=(15, 6))
@@ -163,40 +233,70 @@ def main():
     yC = [r.coverage for r in results]
     yerr_low = [r.cov_err95_low for r in results]
     yerr_high = [r.cov_err95_high for r in results]
-    ax1.errorbar(xN, yC, yerr=[yerr_low, yerr_high],
-                 fmt='o-', lw=1.6, ms=6, color='tab:blue', ecolor='tab:blue',
-                 elinewidth=1.0, capsize=3, label="Couverture empirique")
-    ax1.axhline(1 - args.alpha, color='crimson', ls='--', lw=1.5, label="Niveau nominal 95%")
+    ax1.errorbar(
+        xN,
+        yC,
+        yerr=[yerr_low, yerr_high],
+        fmt="o-",
+        lw=1.6,
+        ms=6,
+        color="tab:blue",
+        ecolor="tab:blue",
+        elinewidth=1.0,
+        capsize=3,
+        label="Couverture empirique",
+    )
+    ax1.axhline(
+        1 - args.alpha, color="crimson", ls="--", lw=1.5, label="Niveau nominal 95%"
+    )

     ax1.set_xlabel("Taille d'échantillon N")
     ax1.set_ylabel("Couverture (IC 95% contient la référence)")
     ax1.set_title(args.title_left)
     if (args.ymin_coverage is not None) or (args.ymax_coverage is not None):
-        ymin = args.ymin_coverage if args.ymin_coverage is not None else ax1.get_ylim()[0]
-        ymax = args.ymax_coverage if args.ymax_coverage is not None else ax1.get_ylim()[1]
+        ymin = (
+            args.ymin_coverage if args.ymin_coverage is not None else ax1.get_ylim()[0]
+        )
+        ymax = (
+            args.ymax_coverage if args.ymax_coverage is not None else ax1.get_ylim()[1]
+        )
         ax1.set_ylim(ymin, ymax)
     ax1.legend(loc="lower right", frameon=True)

-    txt = (f"N = {Mtot}\n"
-           f"mean(ref) = {ref_value_lin:0.3f} rad\n"
-           f"outer B = {outer_for_cov}, inner B = {args.inner}\n"
-           f"seed = {args.seed}\n"
-           f"note: IC = percentile (inner bootstrap)")
-    ax1.text(0.02, 0.97, txt, transform=ax1.transAxes,
-             va="top", ha="left",
-             bbox=dict(boxstyle="round", fc="white", ec="black", alpha=0.95))
+    txt = (
+        f"N = {Mtot}\n"
+        f"mean(ref) = {ref_value_lin:0.3f} rad\n"
+        f"outer B = {outer_for_cov}, inner B = {args.inner}\n"
+        f"seed = {args.seed}\n"
+        f"note: IC = percentile (inner bootstrap)"
+    )
+    ax1.text(
+        0.02,
+        0.97,
+        txt,
+        transform=ax1.transAxes,
+        va="top",
+        ha="left",
+        bbox=dict(boxstyle="round", fc="white", ec="black", alpha=0.95),
+    )

     if args.angular:
-        inset = inset_axes(ax1, width="33%", height="27%", loc="lower left",
-                           bbox_to_anchor=(0.04, 0.08, 0.33, 0.27),
-                           bbox_transform=ax1.transAxes, borderpad=0.5)
+        inset = inset_axes(
+            ax1,
+            width="33%",
+            height="27%",
+            loc="lower left",
+            bbox_to_anchor=(0.04, 0.08, 0.33, 0.27),
+            bbox_transform=ax1.transAxes,
+            borderpad=0.5,
+        )
         bars = [ref_value_lin, ref_value_circ]
         inset.bar([0, 1], bars)
         inset.set_xticks([0, 1])
         inset.set_xticklabels(["mean\n(lin)", "mean\n(circ)"])
         inset.set_title("Référence N=max", fontsize=9)
         inset.set_ylabel("[rad]", fontsize=8)
-        inset.tick_params(axis='both', labelsize=8)
+        inset.tick_params(axis="both", labelsize=8)

     ax2.plot(xN, [r.width_mean for r in results], "-", lw=2.0, color="tab:green")
     ax2.set_xlabel("Taille d'échantillon N")
@@ -205,8 +305,10 @@ def main():

     fig.subplots_adjust(left=0.08, right=0.98, top=0.92, bottom=0.18, wspace=0.25)

-    foot = (f"Bootstrap imbriqué: outer={outer_for_cov}, inner={args.inner}. "
-            f"Référence = estimateur({Mtot}) = {ref_value_lin:0.3f} rad. Seed={args.seed}.")
+    foot = (
+        f"Bootstrap imbriqué: outer={outer_for_cov}, inner={args.inner}. "
+        f"Référence = estimateur({Mtot}) = {ref_value_lin:0.3f} rad. Seed={args.seed}."
+    )
     fig.text(0.5, 0.012, foot, ha="center", fontsize=10)

     os.makedirs(os.path.dirname(args.out), exist_ok=True)
@@ -219,16 +321,25 @@ def main():
         "generated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
         "inputs": {"results": args.results, "p95_col": p95_col},
         "params": {
-            "outer": int(outer_for_cov), "inner": int(args.inner),
-            "alpha": float(args.alpha), "seed": int(args.seed),
-            "minN": int(args.minN), "npoints": int(args.npoints),
-            "ymin_coverage": None if args.ymin_coverage is None else float(args.ymin_coverage),
-            "ymax_coverage": None if args.ymax_coverage is None else float(args.ymax_coverage),
-            "angular_inset": bool(args.angular)
+            "outer": int(outer_for_cov),
+            "inner": int(args.inner),
+            "alpha": float(args.alpha),
+            "seed": int(args.seed),
+            "minN": int(args.minN),
+            "npoints": int(args.npoints),
+            "ymin_coverage": None
+            if args.ymin_coverage is None
+            else float(args.ymin_coverage),
+            "ymax_coverage": None
+            if args.ymax_coverage is None
+            else float(args.ymax_coverage),
+            "angular_inset": bool(args.angular),
         },
         "ref_value_linear_rad": float(ref_value_lin),
-        "ref_value_circular_rad": None if ref_value_circ is None else float(ref_value_circ),
-        "N_list": [int(x) for x in np.asarray(N_list).tolist() ],
+        "ref_value_circular_rad": None
+        if ref_value_circ is None
+        else float(ref_value_circ),
+        "N_list": [int(x) for x in np.asarray(N_list).tolist()],
         "results": [
             {
                 "N": int(r.N),
@@ -237,10 +348,11 @@ def main():
                 "coverage_err95_high": float(r.cov_err95_high),
                 "width_mean_rad": float(r.width_mean),
                 "hits": int(r.n_hits),
-                "method": r.method
-            } for r in results
+                "method": r.method,
+            }
+            for r in results
         ],
-        "figure_path": args.out
+        "figure_path": args.out,
     }
     with open(manifest_path, "w", encoding="utf-8") as f:
         json.dump(manifest, f, indent=2)
@@ -260,7 +372,9 @@ def main():
                 hits = 0
                 for b in range(B):
                     samp = rng2.choice(vals_all, size=sensN, replace=True)
-                    lo, hi = bootstrap_percentile_ci(samp, args.inner, rng2, alpha=args.alpha)
+                    lo, hi = bootstrap_percentile_ci(
+                        samp, args.inner, rng2, alpha=args.alpha
+                    )
                     if (ref_value_lin >= lo) and (ref_value_lin <= hi):
                         hits += 1
                 p_hat = hits / B
@@ -281,13 +395,26 @@ def main():
             print(f"[SENS] B={B:4d}  coverage={p_hat:0.3f}")

         figS, axS = plt.subplots(figsize=(7.5, 4.2))
-        axS.errorbar(B_list, cov_list, yerr=[lo_list, hi_list], fmt="o-",
-                     color="tab:blue", ecolor="tab:blue", capsize=3, lw=1.6, ms=6,
-                     label="Couverture empirique")
-        axS.axhline(1 - args.alpha, color='crimson', ls='--', lw=1.5, label="Niveau nominal 95%")
+        axS.errorbar(
+            B_list,
+            cov_list,
+            yerr=[lo_list, hi_list],
+            fmt="o-",
+            color="tab:blue",
+            ecolor="tab:blue",
+            capsize=3,
+            lw=1.6,
+            ms=6,
+            label="Couverture empirique",
+        )
+        axS.axhline(
+            1 - args.alpha, color="crimson", ls="--", lw=1.5, label="Niveau nominal 95%"
+        )
         axS.set_xlabel("B (outer)" if mode == "outer" else "B (inner)")
         axS.set_ylabel("Couverture (IC 95% contient la référence)")
-        axS.set_title(f"Sensibilité de la couverture vs {'outer' if mode=='outer' else 'inner'}  (N={sensN})")
+        axS.set_title(
+            f"Sensibilité de la couverture vs {'outer' if mode=='outer' else 'inner'}  (N={sensN})"
+        )
         axS.legend(loc="lower right", frameon=True)
         figS.tight_layout()
         out_sens = os.path.splitext(args.out)[0] + f"_sensitivity_{mode}.png"
@@ -298,17 +425,19 @@ def main():
             "script": "plot_fig03b_coverage_bootstrap_vs_n.py",
             "annex": "sensitivity",
             "generated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
-            "mode": mode, "N": int(sensN),
+            "mode": mode,
+            "N": int(sensN),
             "B_list": [int(b) for b in B_list],
             "coverage": [float(c) for c in cov_list],
             "err95_low": [float(e) for e in lo_list],
             "err95_high": [float(e) for e in hi_list],
-            "figure_path": out_sens
+            "figure_path": out_sens,
         }
         sens_path = os.path.splitext(out_sens)[0] + ".manifest.json"
         with open(sens_path, "w", encoding="utf-8") as f:
             json.dump(manifest_sens, f, indent=2)
         print(f"[OK] Manifest annexe écrit: {sens_path}")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter10/plot_fig04_scatter_p95_recalc_vs_orig.py b/zz-scripts/chapter10/plot_fig04_scatter_p95_recalc_vs_orig.py
index 85d4ba8..82ebbff 100755
--- a/zz-scripts/chapter10/plot_fig04_scatter_p95_recalc_vs_orig.py
+++ b/zz-scripts/chapter10/plot_fig04_scatter_p95_recalc_vs_orig.py
@@ -4,15 +4,16 @@
 plot_fig04_scatter_p95_recalc_vs_orig.py

 """
+
 from __future__ import annotations
 import argparse
-import textwrap
 import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt
 from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset

-def detect_column(df: pd.DataFrame, hint: str|None, candidates: list[str]) -> str:
+
+def detect_column(df: pd.DataFrame, hint: str | None, candidates: list[str]) -> str:
     if hint and hint in df.columns:
         return hint
     for c in candidates:
@@ -26,40 +27,90 @@ def detect_column(df: pd.DataFrame, hint: str|None, candidates: list[str]) -> st
                 return df.columns[i]
     raise KeyError(f"Aucune colonne trouvée parmi : {candidates} (hint={hint})")

-def fmt_sci_power(v: float) -> tuple[float,int]:
+
+def fmt_sci_power(v: float) -> tuple[float, int]:
     """Return (scaled_value, exponent) where scaled_value = v / 10**exp and exp is power of ten."""
     if v == 0:
         return 0.0, 0
     exp = int(np.floor(np.log10(abs(v))))
-    scale = 10.0 ** exp
+    scale = 10.0**exp
     return v / scale, exp

+
 def main():
     p = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
-    p.add_argument("--results", required=True, help="CSV results (must contain orig/recalc columns)")
+    p.add_argument(
+        "--results",
+        required=True,
+        help="CSV results (must contain orig/recalc columns)",
+    )
     p.add_argument("--orig-col", default="p95_20_300", help="Original p95 column")
-    p.add_argument("--recalc-col", default="p95_20_300_recalc", help="Recalculated p95 column")
-    p.add_argument("--out", default="zz-figures/chapter10/fig_04_scatter_p95_recalc_vs_orig.png", help="Output PNG")
+    p.add_argument(
+        "--recalc-col", default="p95_20_300_recalc", help="Recalculated p95 column"
+    )
+    p.add_argument(
+        "--out",
+        default="zz-figures/chapter10/fig_04_scatter_p95_recalc_vs_orig.png",
+        help="Output PNG",
+    )
     p.add_argument("--dpi", type=int, default=300, help="PNG dpi")
     p.add_argument("--point-size", type=float, default=10.0, help="scatter marker size")
     p.add_argument("--alpha", type=float, default=0.7, help="scatter alpha")
     p.add_argument("--cmap", default="viridis", help="colormap")
-    p.add_argument("--change-eps", type=float, default=1e-6, help="threshold for 'changed' count (abs Δ > eps)")
-    p.add_argument("--with-zoom", action="store_true", help="Enable inset zoom box (disabled by default)")
-    p.add_argument("--zoom-center-x", type=float, default=None, help="Zoom center (x) in data units")
-    p.add_argument("--zoom-center-y", type=float, default=None, help="Zoom center (y) in data units")
-    p.add_argument("--zoom-w", type=float, default=0.45, help="Inset zoom width (fraction fig)")
-    p.add_argument("--zoom-h", type=float, default=0.10, help="Inset zoom height (fraction fig)")
-
-    p.add_argument("--hist-x", type=float, default=0.60,
-                   help="X (figure coords) de l’histogramme (réduit → plus à gauche)")
-    p.add_argument("--hist-y", type=float, default=0.18,
-                   help="Y (figure coords) de l’histogramme (augmenté → plus haut)")
-    p.add_argument("--hist-scale", type=float, default=3.0,
-                   help="Scale factor for histogram inset size (1.0 = base size; >1 = larger)")
+    p.add_argument(
+        "--change-eps",
+        type=float,
+        default=1e-6,
+        help="threshold for 'changed' count (abs Δ > eps)",
+    )
+    p.add_argument(
+        "--with-zoom",
+        action="store_true",
+        help="Enable inset zoom box (disabled by default)",
+    )
+    p.add_argument(
+        "--zoom-center-x",
+        type=float,
+        default=None,
+        help="Zoom center (x) in data units",
+    )
+    p.add_argument(
+        "--zoom-center-y",
+        type=float,
+        default=None,
+        help="Zoom center (y) in data units",
+    )
+    p.add_argument(
+        "--zoom-w", type=float, default=0.45, help="Inset zoom width (fraction fig)"
+    )
+    p.add_argument(
+        "--zoom-h", type=float, default=0.10, help="Inset zoom height (fraction fig)"
+    )
+
+    p.add_argument(
+        "--hist-x",
+        type=float,
+        default=0.60,
+        help="X (figure coords) de l’histogramme (réduit → plus à gauche)",
+    )
+    p.add_argument(
+        "--hist-y",
+        type=float,
+        default=0.18,
+        help="Y (figure coords) de l’histogramme (augmenté → plus haut)",
+    )
+    p.add_argument(
+        "--hist-scale",
+        type=float,
+        default=3.0,
+        help="Scale factor for histogram inset size (1.0 = base size; >1 = larger)",
+    )
     p.add_argument("--bins", type=int, default=50, help="Histogram bins")
-    p.add_argument("--title", default="Comparaison de p95_20_300 : original vs recalculé (métrique linéaire)",
-                   help="Figure title (fontsize=15)")
+    p.add_argument(
+        "--title",
+        default="Comparaison de p95_20_300 : original vs recalculé (métrique linéaire)",
+        help="Figure title (fontsize=15)",
+    )
     args = p.parse_args()

     df = pd.read_csv(args.results)
@@ -87,7 +138,7 @@ def main():
     frac_changed = 100.0 * n_changed / N

     plt.style.use("classic")
-    fig, ax = plt.subplots(figsize=(10,10))
+    fig, ax = plt.subplots(figsize=(10, 10))

     if abs_delta.size == 0:
         vmax = 1.0
@@ -98,18 +149,28 @@ def main():
     if vmax <= 0.0:
         vmax = 1.0

-    sc = ax.scatter(x, y, c=abs_delta, s=args.point_size, alpha=args.alpha,
-                    cmap=args.cmap, edgecolor='none', vmin=0.0, vmax=vmax, zorder=2)
+    sc = ax.scatter(
+        x,
+        y,
+        c=abs_delta,
+        s=args.point_size,
+        alpha=args.alpha,
+        cmap=args.cmap,
+        edgecolor="none",
+        vmin=0.0,
+        vmax=vmax,
+        zorder=2,
+    )

     lo = min(np.min(x), np.min(y))
     hi = max(np.max(x), np.max(y))
-    ax.plot([lo, hi], [lo, hi], color='gray', linestyle='--', linewidth=1.0, zorder=1)
+    ax.plot([lo, hi], [lo, hi], color="gray", linestyle="--", linewidth=1.0, zorder=1)

     ax.set_xlabel(f"{orig_col} [rad]")
     ax.set_ylabel(f"{recalc_col} [rad]")
     ax.set_title(args.title, fontsize=15)

-    extend = 'max' if np.max(abs_delta) > vmax else 'neither'
+    extend = "max" if np.max(abs_delta) > vmax else "neither"
     cbar = fig.colorbar(sc, ax=ax, extend=extend, pad=0.02)
     cbar.set_label(r"$|\Delta p95|$ [rad]")

@@ -119,11 +180,21 @@ def main():
         f"mean(recalc) = {mean_y:.3f} rad",
         f"Δ = recalc - orig : mean = {mean_delta:.3e}, median = {med_delta:.3e}, std = {std_delta:.3e}",
         f"p95(|Δ|) = {p95_abs:.3e} rad, max |Δ| = {max_abs:.3e} rad",
-        f"N_changed (|Δ| > {args.change_eps}) = {n_changed} ({frac_changed:.2f}%)"
+        f"N_changed (|Δ| > {args.change_eps}) = {n_changed} ({frac_changed:.2f}%)",
     ]
     stats_text = "\n".join(stats)
     bbox = dict(boxstyle="round", fc="white", ec="black", lw=1, alpha=0.95)
-    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=9, va='top', ha='left', bbox=bbox, zorder=10)
+    ax.text(
+        0.02,
+        0.98,
+        stats_text,
+        transform=ax.transAxes,
+        fontsize=9,
+        va="top",
+        ha="left",
+        bbox=bbox,
+        zorder=10,
+    )

     hist_base_w = 0.18
     hist_base_h = 0.14
@@ -131,10 +202,15 @@ def main():
     hist_h = hist_base_h * args.hist_scale
     hist_x = args.hist_x
     hist_y = args.hist_y
-    hist_ax = inset_axes(ax,
-                         width=f"{hist_w*100}%", height=f"{hist_h*100}%",
-                         bbox_to_anchor=(hist_x, hist_y, hist_w, hist_h),
-                         bbox_transform=fig.transFigure, loc='lower left', borderpad=1.0)
+    hist_ax = inset_axes(
+        ax,
+        width=f"{hist_w*100}%",
+        height=f"{hist_h*100}%",
+        bbox_to_anchor=(hist_x, hist_y, hist_w, hist_h),
+        bbox_transform=fig.transFigure,
+        loc="lower left",
+        borderpad=1.0,
+    )

     max_abs = float(np.max(abs_delta)) if abs_delta.size else 0.0
     if max_abs <= 0.0:
@@ -142,21 +218,21 @@ def main():
         exp = 0
     else:
         exp = int(np.floor(np.log10(max_abs)))
-        scale = 10.0 ** exp
+        scale = 10.0**exp
         if max_abs / scale < 1.0:
             exp -= 1
-            scale = 10.0 ** exp
+            scale = 10.0**exp

     hist_vals = abs_delta / scale
-    hist_ax.hist(hist_vals, bins=args.bins, color='tab:blue', edgecolor='black')
-    hist_ax.axvline(0.0, color='red', linewidth=2.0)
+    hist_ax.hist(hist_vals, bins=args.bins, color="tab:blue", edgecolor="black")
+    hist_ax.axvline(0.0, color="red", linewidth=2.0)
     hist_ax.set_title("Histogramme |Δp95|", fontsize=9)
     hist_ax.set_xlabel(f"$\\times 10^{{{exp}}}$", fontsize=8)
-    hist_ax.tick_params(axis='both', which='major', labelsize=8)
+    hist_ax.tick_params(axis="both", which="major", labelsize=8)

     if args.with_zoom:
         if args.zoom_center_x is None:
-            zx_center = 3.0 if (np.max(x) > 3.0) else 0.5*(lo+hi)
+            zx_center = 3.0 if (np.max(x) > 3.0) else 0.5 * (lo + hi)
         else:
             zx_center = args.zoom_center_x
         if args.zoom_center_y is None:
@@ -165,30 +241,47 @@ def main():
             zy_center = args.zoom_center_y
         dx = 0.06 * (hi - lo) if (hi - lo) > 0 else 0.1
         dy = dx
-        zx0, zx1 = zx_center - dx/2.0, zx_center + dx/2.0
-        zy0, zy1 = zy_center - dy/2.0, zy_center + dy/2.0
+        zx0, zx1 = zx_center - dx / 2.0, zx_center + dx / 2.0
+        zy0, zy1 = zy_center - dy / 2.0, zy_center + dy / 2.0

         inset_w = args.zoom_w
         inset_h = args.zoom_h
-        inz = inset_axes(ax,
-                         width=f"{inset_w*100}%", height=f"{inset_h*100}%",
-                         bbox_to_anchor=(0.48, 0.58, inset_w, inset_h),
-                         bbox_transform=fig.transFigure, loc='lower left', borderpad=1.0)
-        inz.scatter(x, y, c=abs_delta, s=max(1.0, args.point_size/2.0), alpha=min(1.0, args.alpha+0.1),
-                    cmap=args.cmap, edgecolor='none', vmin=0.0, vmax=vmax)
-        inz.plot([zx0, zx1], [zx0, zx1], color='gray', linestyle='--', linewidth=0.8)
+        inz = inset_axes(
+            ax,
+            width=f"{inset_w*100}%",
+            height=f"{inset_h*100}%",
+            bbox_to_anchor=(0.48, 0.58, inset_w, inset_h),
+            bbox_transform=fig.transFigure,
+            loc="lower left",
+            borderpad=1.0,
+        )
+        inz.scatter(
+            x,
+            y,
+            c=abs_delta,
+            s=max(1.0, args.point_size / 2.0),
+            alpha=min(1.0, args.alpha + 0.1),
+            cmap=args.cmap,
+            edgecolor="none",
+            vmin=0.0,
+            vmax=vmax,
+        )
+        inz.plot([zx0, zx1], [zx0, zx1], color="gray", linestyle="--", linewidth=0.8)
         inz.set_xlim(zx0, zx1)
         inz.set_ylim(zy0, zy1)
         inz.set_title("zoom", fontsize=8)
         mark_inset(ax, inz, loc1=2, loc2=4, fc="none", ec="0.5", lw=0.8)

-    foot = (r"$\Delta p95 = p95_{recalc} - p95_{orig}$. Couleur = $|\Delta p95|$. "
-            "Zoom optionnel (--with-zoom). Histogramme déplacé (place & taille paramétrables).")
-    fig.text(0.5, 0.02, foot, ha='center', fontsize=9)
+    foot = (
+        r"$\Delta p95 = p95_{recalc} - p95_{orig}$. Couleur = $|\Delta p95|$. "
+        "Zoom optionnel (--with-zoom). Histogramme déplacé (place & taille paramétrables)."
+    )
+    fig.text(0.5, 0.02, foot, ha="center", fontsize=9)

     plt.tight_layout(rect=[0, 0.04, 1, 0.98])
     fig.savefig(args.out, dpi=args.dpi)
     print(f"Wrote: {args.out}")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter10/plot_fig05_hist_cdf_metrics.py b/zz-scripts/chapter10/plot_fig05_hist_cdf_metrics.py
index 3f5fa0e..66b6835 100755
--- a/zz-scripts/chapter10/plot_fig05_hist_cdf_metrics.py
+++ b/zz-scripts/chapter10/plot_fig05_hist_cdf_metrics.py
@@ -5,19 +5,26 @@ plot_fig05_hist_cdf_metrics.py
 Figure 05 : Histogramme + CDF des p95 recalculés en métrique circulaire.

 """
+
 from __future__ import annotations
-import argparse, textwrap
+import argparse
+import textwrap
 import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt
 import matplotlib.lines as mlines
 from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset

+
 # ---------- utils ----------
 def detect_p95_column(df: pd.DataFrame) -> str:
     candidates = [
-        "p95_20_300_recalc", "p95_20_300_circ", "p95_20_300_recalced",
-        "p95_20_300", "p95_circ", "p95_recalc"
+        "p95_20_300_recalc",
+        "p95_20_300_circ",
+        "p95_20_300_recalced",
+        "p95_20_300",
+        "p95_circ",
+        "p95_recalc",
     ]
     for c in candidates:
         if c in df.columns:
@@ -27,22 +34,44 @@ def detect_p95_column(df: pd.DataFrame) -> str:
             return c
     raise KeyError("Aucune colonne 'p95' détectée dans le CSV results.")

+
 # ---------- main ----------
 def main():
     ap = argparse.ArgumentParser()
-    ap.add_argument("--results", required=True, help="CSV avec p95 circulaire recalculé")
-    ap.add_argument("--out", default="zz-figures/chapter10/fig_05_hist_cdf_metrics.png", help="PNG de sortie")
-    ap.add_argument("--ref-p95", type=float, default=0.7104087123286049, help="p95 de référence [rad]")
+    ap.add_argument(
+        "--results", required=True, help="CSV avec p95 circulaire recalculé"
+    )
+    ap.add_argument(
+        "--out",
+        default="zz-figures/chapter10/fig_05_hist_cdf_metrics.png",
+        help="PNG de sortie",
+    )
+    ap.add_argument(
+        "--ref-p95",
+        type=float,
+        default=0.7104087123286049,
+        help="p95 de référence [rad]",
+    )
     ap.add_argument("--bins", type=int, default=50, help="Nb de bacs histogramme")
     ap.add_argument("--dpi", type=int, default=150, help="DPI du PNG")
     # position et fenêtre du zoom (centre + demi-étendues)
     ap.add_argument("--zoom-x", type=float, default=3.0, help="centre X du zoom (rad)")
-    ap.add_argument("--zoom-y", type=float, default=35.0, help="centre Y du zoom (counts)")
-    ap.add_argument("--zoom-dx", type=float, default=0.30, help="demi-largeur X du zoom (rad)")
-    ap.add_argument("--zoom-dy", type=float, default=30.0, help="demi-hauteur Y du zoom (counts)")
+    ap.add_argument(
+        "--zoom-y", type=float, default=35.0, help="centre Y du zoom (counts)"
+    )
+    ap.add_argument(
+        "--zoom-dx", type=float, default=0.30, help="demi-largeur X du zoom (rad)"
+    )
+    ap.add_argument(
+        "--zoom-dy", type=float, default=30.0, help="demi-hauteur Y du zoom (counts)"
+    )
     # taille du panneau de zoom (fraction de l’axe)
-    ap.add_argument("--zoom-w", type=float, default=0.35, help="largeur du zoom (fraction)")
-    ap.add_argument("--zoom-h", type=float, default=0.25, help="hauteur du zoom (fraction)")
+    ap.add_argument(
+        "--zoom-w", type=float, default=0.35, help="largeur du zoom (fraction)"
+    )
+    ap.add_argument(
+        "--zoom-h", type=float, default=0.25, help="hauteur du zoom (fraction)"
+    )
     args = ap.parse_args()

     # --- lecture & colonne p95 ---
@@ -60,7 +89,11 @@ def main():

     # --- stats ---
     N = p95.size
-    mean, median, std = float(np.mean(p95)), float(np.median(p95)), float(np.std(p95, ddof=0))
+    mean, median, std = (
+        float(np.mean(p95)),
+        float(np.median(p95)),
+        float(np.std(p95, ddof=0)),
+    )
     n_below = int((p95 < args.ref_p95).sum())
     frac_below = n_below / max(1, N)

@@ -77,15 +110,22 @@ def main():
     ax2 = ax.twinx()
     sorted_p = np.sort(p95)
     ecdf = np.arange(1, N + 1) / N
-    cdf_line, = ax2.plot(sorted_p, ecdf, lw=2)
+    (cdf_line,) = ax2.plot(sorted_p, ecdf, lw=2)
     ax2.set_ylabel("CDF empirique")
     ax2.set_ylim(0.0, 1.02)

     # Ligne verticale de référence
     ax.axvline(args.ref_p95, color="crimson", linestyle="--", lw=2)
-    ax.text(args.ref_p95, ax.get_ylim()[1] * 0.45,
-            f"ref = {args.ref_p95:.4f} rad",
-            color="crimson", rotation=90, va="center", ha="right", fontsize=10)
+    ax.text(
+        args.ref_p95,
+        ax.get_ylim()[1] * 0.45,
+        f"ref = {args.ref_p95:.4f} rad",
+        color="crimson",
+        rotation=90,
+        va="center",
+        ha="right",
+        fontsize=10,
+    )

     # Boîte de stats (haut-gauche)
     stat_lines = [
@@ -98,9 +138,16 @@ def main():
         stat_lines.append(f"wrapped_corrected = {wrapped_corrected}")
     stat_lines.append(f"p(P95 < ref) = {frac_below:.3f} (n={n_below})")
     stat_text = "\n".join(stat_lines)
-    ax.text(0.02, 0.98, stat_text, transform=ax.transAxes, fontsize=10,
-            va="top", ha="left",
-            bbox=dict(boxstyle="round", fc="white", ec="black", lw=1, alpha=0.95))
+    ax.text(
+        0.02,
+        0.98,
+        stat_text,
+        transform=ax.transAxes,
+        fontsize=10,
+        va="top",
+        ha="left",
+        bbox=dict(boxstyle="round", fc="white", ec="black", lw=1, alpha=0.95),
+    )

     # Petite légende (sous la boîte de stats)
     handles = []
@@ -108,13 +155,22 @@ def main():
         handles.append(patches[0])
     else:
         from matplotlib.patches import Rectangle
-        handles.append(Rectangle((0, 0), 1, 1, facecolor="C0", edgecolor="k", alpha=0.7))
+
+        handles.append(
+            Rectangle((0, 0), 1, 1, facecolor="C0", edgecolor="k", alpha=0.7)
+        )
     proxy_cdf = mlines.Line2D([], [], color=cdf_line.get_color(), lw=2)
     proxy_ref = mlines.Line2D([], [], color="crimson", linestyle="--", lw=2)
     handles += [proxy_cdf, proxy_ref]
     labels = ["Histogramme (effectifs)", "CDF empirique", "p95 réf"]
-    ax.legend(handles, labels, loc="upper left", bbox_to_anchor=(0.02, 0.72),
-              frameon=True, fontsize=10)
+    ax.legend(
+        handles,
+        labels,
+        loc="upper left",
+        bbox_to_anchor=(0.02, 0.72),
+        frameon=True,
+        fontsize=10,
+    )

     # Inset zoom — centré dans l’axe, fenêtre contrôlée par (zoom-x/y, zoom-dx/dy).
     inset_ax = inset_axes(
@@ -150,14 +206,16 @@ def main():
     ax.set_title("Distribution de p95_20_300 (MC global)", fontsize=15)

     foot = textwrap.fill(
-        (r"Métrique : distance circulaire (mod $2\pi$). "
-         r"Définition : p95 = $95^{\mathrm{e}}$ centile de $|\Delta\phi(f)|$ pour $f\in[20,300]\ \mathrm{Hz}$. "
-         r"Corrections : sauts de branchement corrigés, "
-         rf"$N_{{\mathrm{{wrapped\_corrected}}}} = {wrapped_corrected if wrapped_corrected is not None else 0}$. "
-         r"Comparaison : "
-         rf"$p(\mathrm{{p95}}<\mathrm{{p95_{{ref}}}}) = {frac_below:.3f}$ "
-         rf"(n = {n_below})."),
-        width=180
+        (
+            r"Métrique : distance circulaire (mod $2\pi$). "
+            r"Définition : p95 = $95^{\mathrm{e}}$ centile de $|\Delta\phi(f)|$ pour $f\in[20,300]\ \mathrm{Hz}$. "
+            r"Corrections : sauts de branchement corrigés, "
+            rf"$N_{{\mathrm{{wrapped\_corrected}}}} = {wrapped_corrected if wrapped_corrected is not None else 0}$. "
+            r"Comparaison : "
+            rf"$p(\mathrm{{p95}}<\mathrm{{p95_{{ref}}}}) = {frac_below:.3f}$ "
+            rf"(n = {n_below})."
+        ),
+        width=180,
     )
     plt.tight_layout(rect=[0, 0.14, 1, 0.98])
     fig.text(0.5, 0.04, foot, ha="center", va="bottom", fontsize=9)
@@ -165,5 +223,6 @@ def main():
     fig.savefig(args.out, dpi=args.dpi)
     print(f"Wrote : {args.out}")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter10/plot_fig06_residual_map.py b/zz-scripts/chapter10/plot_fig06_residual_map.py
index addec76..8d3b69f 100755
--- a/zz-scripts/chapter10/plot_fig06_residual_map.py
+++ b/zz-scripts/chapter10/plot_fig06_residual_map.py
@@ -16,6 +16,7 @@ python zz-scripts/chapter10/plot_fig06_residual_map.py \
   --figsize 15,9 --dpi 300 --manifest \
   --out zz-figures/chapter10/fig_06_heatmap_absdp95_m1m2.png
 """
+
 from __future__ import annotations

 import argparse
@@ -26,10 +27,12 @@ import pandas as pd
 import matplotlib.pyplot as plt
 from matplotlib.ticker import MaxNLocator

+
 # ------------------------- utilitaires -------------------------------------
 def wrap_pi(x: np.ndarray) -> np.ndarray:
     """Ramène les angles en radians dans (-π, π]."""
-    return (x + np.pi) % (2*np.pi) - np.pi
+    return (x + np.pi) % (2 * np.pi) - np.pi
+

 def detect_col(df: pd.DataFrame, candidates: list[str]) -> str:
     """Trouve une colonne par nom exact ou par inclusion insensible à la casse."""
@@ -43,24 +46,45 @@ def detect_col(df: pd.DataFrame, candidates: list[str]) -> str:
                 return c
     raise KeyError(f"Impossible de trouver l'une des colonnes : {candidates}")

+
 # --------------------------- script principal ------------------------------
 def main():
     ap = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
     ap.add_argument("--results", required=True, help="CSV d'entrée.")
     ap.add_argument("--metric", choices=["dp95", "dphi"], default="dp95")
     ap.add_argument("--abs", action="store_true", help="Prendre la valeur absolue.")
-    ap.add_argument("--m1-col", default="m1"); ap.add_argument("--m2-col", default="m2")
-    ap.add_argument("--orig-col", default="p95_20_300", help="Colonne p95 originale (dp95).")
-    ap.add_argument("--recalc-col", default="p95_20_300_recalc", help="Colonne p95 recalculée (dp95).")
+    ap.add_argument("--m1-col", default="m1")
+    ap.add_argument("--m2-col", default="m2")
+    ap.add_argument(
+        "--orig-col", default="p95_20_300", help="Colonne p95 originale (dp95)."
+    )
+    ap.add_argument(
+        "--recalc-col",
+        default="p95_20_300_recalc",
+        help="Colonne p95 recalculée (dp95).",
+    )
     ap.add_argument("--phi-ref-col", default=None, help="Colonne phi_ref (dphi).")
     ap.add_argument("--phi-mcgt-col", default=None, help="Colonne phi_mcgt (dphi).")
     ap.add_argument("--gridsize", type=int, default=36)
-    ap.add_argument("--mincnt", type=int, default=3, help="Masque les hexagones avec nb<mincnt.")
+    ap.add_argument(
+        "--mincnt", type=int, default=3, help="Masque les hexagones avec nb<mincnt."
+    )
     ap.add_argument("--cmap", default="viridis")
-    ap.add_argument("--vclip", default="1,99", help="Percentiles pour vmin,vmax (ex: '1,99').")
-    ap.add_argument("--scale-exp", type=int, default=-7, help="Exponent pour l'échelle ×10^exp rad.")
-    ap.add_argument("--threshold", type=float, default=1e-6, help="Seuil pour fraction |metric|>threshold [rad].")
-    ap.add_argument("--figsize", default="15,9", help="Largeur,hauteur en pouces (ex: '15,9').")
+    ap.add_argument(
+        "--vclip", default="1,99", help="Percentiles pour vmin,vmax (ex: '1,99')."
+    )
+    ap.add_argument(
+        "--scale-exp", type=int, default=-7, help="Exponent pour l'échelle ×10^exp rad."
+    )
+    ap.add_argument(
+        "--threshold",
+        type=float,
+        default=1e-6,
+        help="Seuil pour fraction |metric|>threshold [rad].",
+    )
+    ap.add_argument(
+        "--figsize", default="15,9", help="Largeur,hauteur en pouces (ex: '15,9')."
+    )
     ap.add_argument("--dpi", type=int, default=300)
     ap.add_argument("--out", default="fig_06_residual_map.png")
     ap.add_argument("--manifest", action="store_true")
@@ -79,8 +103,10 @@ def main():
         metric_name = r"\Delta p_{95}"
     else:  # dphi
         col_ref = detect_col(df, [args.phi_ref_col or "phi_ref_fpeak"])
-        col_mc  = detect_col(df, [args.phi_mcgt_col or "phi_mcgt_fpeak", "phi_mcgt"])
-        raw = wrap_pi(df[col_mc].astype(float).values - df[col_ref].astype(float).values)
+        col_mc = detect_col(df, [args.phi_mcgt_col or "phi_mcgt_fpeak", "phi_mcgt"])
+        raw = wrap_pi(
+            df[col_mc].astype(float).values - df[col_ref].astype(float).values
+        )
         metric_name = r"\Delta \phi"

     if args.abs:
@@ -90,7 +116,7 @@ def main():
         metric_label = rf"{metric_name}"

     # Pré-scaling pour l'affichage : valeurs en unités “×10^exp rad”
-    scale_factor = 10.0 ** args.scale_exp
+    scale_factor = 10.0**args.scale_exp
     scaled = raw / scale_factor

     # vmin/vmax via percentiles sur *scaled*
@@ -112,38 +138,57 @@ def main():

     # -> plus d'espace horizontal entre carte/colorbar et inserts (left=0.75)
     # -> moins d'espace vertical avec le footer (bottom abaissé)
-    ax_main = fig.add_axes([0.07, 0.145, 0.56, 0.74])   # left, bottom, width, height
+    ax_main = fig.add_axes([0.07, 0.145, 0.56, 0.74])  # left, bottom, width, height
     ax_cbar = fig.add_axes([0.645, 0.145, 0.025, 0.74])
     right_left = 0.75
-    right_w    = 0.23
-    ax_cnt  = fig.add_axes([right_left, 0.60, right_w, 0.30])
+    right_w = 0.23
+    ax_cnt = fig.add_axes([right_left, 0.60, right_w, 0.30])
     ax_hist = fig.add_axes([right_left, 0.20, right_w, 0.30])

     # ------------------------------- main hexbin ---------------------------
     hb = ax_main.hexbin(
-        x, y, C=scaled, gridsize=args.gridsize,
-        reduce_C_function=np.median, mincnt=args.mincnt,
-        vmin=vmin, vmax=vmax, cmap=args.cmap
+        x,
+        y,
+        C=scaled,
+        gridsize=args.gridsize,
+        reduce_C_function=np.median,
+        mincnt=args.mincnt,
+        vmin=vmin,
+        vmax=vmax,
+        cmap=args.cmap,
     )
     cbar = fig.colorbar(hb, cax=ax_cbar)
     exp_txt = f"× 10^{args.scale_exp}"  # ex: × 10^-7
     cbar.set_label(rf"{metric_label} {exp_txt} [rad]")

-    ax_main.set_title(rf"Carte des résidus ${metric_label}$ sur $(m_1,m_2)$" + (" (absolu)" if args.abs else ""))
-    ax_main.set_xlabel("m1"); ax_main.set_ylabel("m2")
+    ax_main.set_title(
+        rf"Carte des résidus ${metric_label}$ sur $(m_1,m_2)$"
+        + (" (absolu)" if args.abs else "")
+    )
+    ax_main.set_xlabel("m1")
+    ax_main.set_ylabel("m2")

     # annotation mincnt
-    ax_main.text(0.02, 0.02,
-                 f"Hexagones vides = count < {args.mincnt}",
-                 transform=ax_main.transAxes, ha="left", va="bottom",
-                 bbox=dict(boxstyle="round", fc="white", ec="0.5", alpha=0.9), fontsize=9)
+    ax_main.text(
+        0.02,
+        0.02,
+        f"Hexagones vides = count < {args.mincnt}",
+        transform=ax_main.transAxes,
+        ha="left",
+        va="bottom",
+        bbox=dict(boxstyle="round", fc="white", ec="0.5", alpha=0.9),
+        fontsize=9,
+    )

     # ------------------------------- counts inset --------------------------
     hb_counts = ax_cnt.hexbin(x, y, gridsize=args.gridsize, cmap="gray_r")
-    cbar_cnt = fig.colorbar(hb_counts, ax=ax_cnt, orientation="vertical", fraction=0.046, pad=0.03)
+    cbar_cnt = fig.colorbar(
+        hb_counts, ax=ax_cnt, orientation="vertical", fraction=0.046, pad=0.03
+    )
     cbar_cnt.set_label("Counts")
     ax_cnt.set_title("Counts (par cellule)")
-    ax_cnt.set_xlabel("m1"); ax_cnt.set_ylabel("m2")
+    ax_cnt.set_xlabel("m1")
+    ax_cnt.set_ylabel("m2")
     ax_cnt.xaxis.set_major_locator(MaxNLocator(nbins=5))
     ax_cnt.yaxis.set_major_locator(MaxNLocator(nbins=5))

@@ -161,20 +206,33 @@ def main():
     stats_lines = [
         rf"median={med:.2f}, mean={mean:.2f}",
         rf"std={std:.2f}, p95={p95:.2f} {exp_txt} [rad]",
-        rf"fraction |metric|>{args.threshold:.0e} rad = {100*frac_over:.2f}%"
+        rf"fraction |metric|>{args.threshold:.0e} rad = {100*frac_over:.2f}%",
     ]
-    ax_hist.text(0.02, 0.02, "\n".join(stats_lines),
-                 transform=ax_hist.transAxes, ha="left", va="bottom",
-                 fontsize=9, bbox=dict(boxstyle="round", fc="white", ec="0.5", alpha=0.9))
+    ax_hist.text(
+        0.02,
+        0.02,
+        "\n".join(stats_lines),
+        transform=ax_hist.transAxes,
+        ha="left",
+        va="bottom",
+        fontsize=9,
+        bbox=dict(boxstyle="round", fc="white", ec="0.5", alpha=0.9),
+    )

     # ------------------------------- footers --------------------------------
-    foot_scale = (f"Réduction par médiane (gridsize={args.gridsize}, mincnt={args.mincnt}). "
-                  f"Échelle: vmin={vmin:.6g}, vmax={vmax:.6g}  (percentiles {p_lo}–{p_hi}).")
-    foot_stats = (f"Stats globales: median={med:.2f}, mean={mean:.2f}, std={std:.2f}, "
-                  f"p95={p95:.2f} {exp_txt} [rad]. N={N}, cellules actives (≥{args.mincnt}) = ")
+    foot_scale = (
+        f"Réduction par médiane (gridsize={args.gridsize}, mincnt={args.mincnt}). "
+        f"Échelle: vmin={vmin:.6g}, vmax={vmax:.6g}  (percentiles {p_lo}–{p_hi})."
+    )
+    foot_stats = (
+        f"Stats globales: median={med:.2f}, mean={mean:.2f}, std={std:.2f}, "
+        f"p95={p95:.2f} {exp_txt} [rad]. N={N}, cellules actives (≥{args.mincnt}) = "
+    )

     # (subplots_adjust n'affecte pas add_axes, on l'utilise juste pour la bbox globale)
-    fig.subplots_adjust(left=0.07, right=0.96, top=0.96, bottom=0.12, wspace=0.34, hspace=0.30)
+    fig.subplots_adjust(
+        left=0.07, right=0.96, top=0.96, bottom=0.12, wspace=0.34, hspace=0.30
+    )
     fig.text(0.5, 0.053, foot_scale, ha="center", fontsize=10)
     fig.text(0.5, 0.032, foot_stats + f"{n_active}/{N}.", ha="center", fontsize=10)

@@ -190,28 +248,43 @@ def main():
             "generated_at": pd.Timestamp.utcnow().isoformat() + "Z",
             "inputs": {
                 "csv": args.results,
-                "m1_col": args.m1_col, "m2_col": args.m2_col
+                "m1_col": args.m1_col,
+                "m2_col": args.m2_col,
             },
             "metric": {
-                "name": args.metric, "absolute": bool(args.abs),
-                "orig_col": args.orig_col, "recalc_col": args.recalc_col,
-                "phi_ref_col": args.phi_ref_col, "phi_mcgt_col": args.phi_mcgt_col
+                "name": args.metric,
+                "absolute": bool(args.abs),
+                "orig_col": args.orig_col,
+                "recalc_col": args.recalc_col,
+                "phi_ref_col": args.phi_ref_col,
+                "phi_mcgt_col": args.phi_mcgt_col,
             },
             "plot_params": {
-                "gridsize": int(args.gridsize), "mincnt": int(args.mincnt),
-                "cmap": args.cmap, "vclip_percentiles": [p_lo, p_hi],
-                "vmin_scaled": float(vmin), "vmax_scaled": float(vmax),
-                "scale_exp": int(args.scale_exp), "threshold_rad": float(args.threshold),
-                "figsize": [fig_w, fig_h], "dpi": int(args.dpi)
+                "gridsize": int(args.gridsize),
+                "mincnt": int(args.mincnt),
+                "cmap": args.cmap,
+                "vclip_percentiles": [p_lo, p_hi],
+                "vmin_scaled": float(vmin),
+                "vmax_scaled": float(vmax),
+                "scale_exp": int(args.scale_exp),
+                "threshold_rad": float(args.threshold),
+                "figsize": [fig_w, fig_h],
+                "dpi": int(args.dpi),
             },
             "dataset": {"N": int(N), "n_active_points": int(n_active)},
-                "stats_scaled": {"median": med, "mean": mean, "std": std, "p95": p95,
-                             "fraction_abs_gt_threshold": frac_over},
-            "figure_path": args.out
+            "stats_scaled": {
+                "median": med,
+                "mean": mean,
+                "std": std,
+                "p95": p95,
+                "fraction_abs_gt_threshold": frac_over,
+            },
+            "figure_path": args.out,
         }
         with open(man_path, "w", encoding="utf-8") as f:
             json.dump(manifest, f, indent=2)
         print(f"[OK] Manifest écrit: {man_path}")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter10/plot_fig07_synthesis.py b/zz-scripts/chapter10/plot_fig07_synthesis.py
index 565f352..75362f2 100755
--- a/zz-scripts/chapter10/plot_fig07_synthesis.py
+++ b/zz-scripts/chapter10/plot_fig07_synthesis.py
@@ -4,8 +4,13 @@
 plot_fig07_summary.py — Figure 7 (synthèse)

 """
+
 from __future__ import annotations
-import argparse, json, os, sys, csv
+import argparse
+import json
+import os
+import sys
+import csv
 from dataclasses import dataclass
 from typing import Any, Dict, List, Optional, Tuple

@@ -14,27 +19,34 @@ import matplotlib.pyplot as plt
 from matplotlib.lines import Line2D
 from matplotlib.gridspec import GridSpec

+
 # ---------- utils ----------
 def parse_figsize(s: str) -> Tuple[float, float]:
     try:
         a, b = s.split(",")
         return float(a), float(b)
     except Exception as e:
-        raise argparse.ArgumentTypeError("figsize doit être 'largeur,hauteur' (ex: 14,6)") from e
+        raise argparse.ArgumentTypeError(
+            "figsize doit être 'largeur,hauteur' (ex: 14,6)"
+        ) from e
+

 def load_manifest(path: str) -> Dict[str, Any]:
     with open(path, "r", encoding="utf-8") as f:
         return json.load(f)

+
 def _first(d: Dict[str, Any], keys: List[str], default=np.nan):
     for k in keys:
         if k in d and d[k] is not None:
             return d[k]
     return default

+
 def _param(params: Dict[str, Any], candidates: List[str], default=np.nan):
     return _first(params, candidates, default)

+
 @dataclass
 class Series:
     label: str
@@ -46,46 +58,83 @@ class Series:
     alpha: float
     params: Dict[str, Any]

-def series_from_manifest(man: Dict[str, Any], label_override: Optional[str]=None) -> Series:
+
+def series_from_manifest(
+    man: Dict[str, Any], label_override: Optional[str] = None
+) -> Series:
     results = man.get("results", [])
     if not results:
         raise ValueError("Manifest ne contient pas de 'results'.")

-    N          = np.array([_first(r, ["N"], np.nan) for r in results], dtype=float)
-    coverage   = np.array([_first(r, ["coverage"], np.nan) for r in results], dtype=float)
-    err_low    = np.array([_first(r, ["coverage_err95_low","coverage_err_low"], 0.0) for r in results], dtype=float)
-    err_high   = np.array([_first(r, ["coverage_err95_high","coverage_err_high"], 0.0) for r in results], dtype=float)
-    width_mean = np.array([_first(r, ["width_mean_rad","width_mean"], np.nan) for r in results], dtype=float)
+    N = np.array([_first(r, ["N"], np.nan) for r in results], dtype=float)
+    coverage = np.array([_first(r, ["coverage"], np.nan) for r in results], dtype=float)
+    err_low = np.array(
+        [_first(r, ["coverage_err95_low", "coverage_err_low"], 0.0) for r in results],
+        dtype=float,
+    )
+    err_high = np.array(
+        [_first(r, ["coverage_err95_high", "coverage_err_high"], 0.0) for r in results],
+        dtype=float,
+    )
+    width_mean = np.array(
+        [_first(r, ["width_mean_rad", "width_mean"], np.nan) for r in results],
+        dtype=float,
+    )

     params = man.get("params", {})
-    alpha  = float(_param(params, ["alpha","conf_alpha"], 0.05))
-    label  = label_override or man.get("series_label") or man.get("label") or "série"
+    alpha = float(_param(params, ["alpha", "conf_alpha"], 0.05))
+    label = label_override or man.get("series_label") or man.get("label") or "série"
+
+    return Series(
+        label=label,
+        N=N,
+        coverage=coverage,
+        err_low=err_low,
+        err_high=err_high,
+        width_mean=width_mean,
+        alpha=alpha,
+        params=params,
+    )

-    return Series(label=label, N=N, coverage=coverage, err_low=err_low,
-                  err_high=err_high, width_mean=width_mean, alpha=alpha, params=params)

 def detect_reps_params(params: Dict[str, Any]) -> Tuple[float, float, float]:
-    M       = _param(params, ["M","num_trials","n_trials","n_repeat","repeats","nsimu"], np.nan)
-    outer_B = _param(params, ["outer_B","outer","B_outer","outerB","Bouter"], np.nan)
-    inner_B = _param(params, ["inner_B","inner","B_inner","innerB","Binner"], np.nan)
+    M = _param(
+        params, ["M", "num_trials", "n_trials", "n_repeat", "repeats", "nsimu"], np.nan
+    )
+    outer_B = _param(
+        params, ["outer_B", "outer", "B_outer", "outerB", "Bouter"], np.nan
+    )
+    inner_B = _param(
+        params, ["inner_B", "inner", "B_inner", "innerB", "Binner"], np.nan
+    )
     return float(M), float(outer_B), float(inner_B)

+
 # ---------- stats & résumé ----------
 def compute_summary_rows(series_list: List[Series]) -> List[List[Any]]:
     rows = []
     for s in series_list:
         mean_cov = np.nanmean(s.coverage)
-        med_cov  = np.nanmedian(s.coverage)
-        std_cov  = np.nanstd(s.coverage)
-        p95_cov  = np.nanpercentile(s.coverage, 95)
-        med_w    = np.nanmedian(s.width_mean)
+        med_cov = np.nanmedian(s.coverage)
+        std_cov = np.nanstd(s.coverage)
+        p95_cov = np.nanpercentile(s.coverage, 95)
+        med_w = np.nanmedian(s.width_mean)
         _, outer_B, inner_B = detect_reps_params(s.params)
-        rows.append([s.label,
-                     int(outer_B) if np.isfinite(outer_B) else "",
-                     int(inner_B) if np.isfinite(inner_B) else "",
-                     mean_cov, med_cov, std_cov, p95_cov, med_w])
+        rows.append(
+            [
+                s.label,
+                int(outer_B) if np.isfinite(outer_B) else "",
+                int(inner_B) if np.isfinite(inner_B) else "",
+                mean_cov,
+                med_cov,
+                std_cov,
+                p95_cov,
+                med_w,
+            ]
+        )
     return rows

+
 def powerlaw_slope(N: np.ndarray, W: np.ndarray) -> float:
     m = np.isfinite(N) & np.isfinite(W) & (N > 0) & (W > 0)
     if m.sum() < 2:
@@ -93,41 +142,68 @@ def powerlaw_slope(N: np.ndarray, W: np.ndarray) -> float:
     p = np.polyfit(np.log(N[m]), np.log(W[m]), 1)
     return float(p[0])

+
 # ---------- CSV ----------
 def save_summary_csv(series_list: List[Series], out_csv: str) -> None:
     os.makedirs(os.path.dirname(out_csv) or ".", exist_ok=True)
-    fields = ["series","N","coverage","err95_low","err95_high","width_mean","M","outer_B","inner_B","alpha"]
+    fields = [
+        "series",
+        "N",
+        "coverage",
+        "err95_low",
+        "err95_high",
+        "width_mean",
+        "M",
+        "outer_B",
+        "inner_B",
+        "alpha",
+    ]
     with open(out_csv, "w", newline="", encoding="utf-8") as f:
         w = csv.DictWriter(f, fieldnames=fields)
         w.writeheader()
         for s in series_list:
             M, outer_B, inner_B = detect_reps_params(s.params)
             for i in range(len(s.N)):
-                w.writerow({
-                    "series": s.label,
-                    "N": int(s.N[i]) if np.isfinite(s.N[i]) else "",
-                    "coverage": float(s.coverage[i]) if np.isfinite(s.coverage[i]) else "",
-                    "err95_low": float(s.err_low[i]) if np.isfinite(s.err_low[i]) else "",
-                    "err95_high": float(s.err_high[i]) if np.isfinite(s.err_high[i]) else "",
-                    "width_mean": float(s.width_mean[i]) if np.isfinite(s.width_mean[i]) else "",
-                    "M": int(M) if np.isfinite(M) else "",
-                    "outer_B": int(outer_B) if np.isfinite(outer_B) else "",
-                    "inner_B": int(inner_B) if np.isfinite(inner_B) else "",
-                    "alpha": s.alpha,
-                })
+                w.writerow(
+                    {
+                        "series": s.label,
+                        "N": int(s.N[i]) if np.isfinite(s.N[i]) else "",
+                        "coverage": float(s.coverage[i])
+                        if np.isfinite(s.coverage[i])
+                        else "",
+                        "err95_low": float(s.err_low[i])
+                        if np.isfinite(s.err_low[i])
+                        else "",
+                        "err95_high": float(s.err_high[i])
+                        if np.isfinite(s.err_high[i])
+                        else "",
+                        "width_mean": float(s.width_mean[i])
+                        if np.isfinite(s.width_mean[i])
+                        else "",
+                        "M": int(M) if np.isfinite(M) else "",
+                        "outer_B": int(outer_B) if np.isfinite(outer_B) else "",
+                        "inner_B": int(inner_B) if np.isfinite(inner_B) else "",
+                        "alpha": s.alpha,
+                    }
+                )

-# ---------- tracé ----------
-def plot_synthese(series_list: List[Series], out_png: str,
-                  figsize=(14,6), dpi=300,
-                  ymin_cov: float|None=None, ymax_cov: float|None=None):

+# ---------- tracé ----------
+def plot_synthese(
+    series_list: List[Series],
+    out_png: str,
+    figsize=(14, 6),
+    dpi=300,
+    ymin_cov: float | None = None,
+    ymax_cov: float | None = None,
+):
     plt.style.use("classic")
     fig = plt.figure(figsize=figsize, constrained_layout=False)

     gs = GridSpec(2, 2, figure=fig, height_ratios=[0.78, 0.22], width_ratios=[1.0, 1.0])
-    ax_cov   = fig.add_subplot(gs[0, 0])
+    ax_cov = fig.add_subplot(gs[0, 0])
     ax_width = fig.add_subplot(gs[0, 1])
-    ax_tab   = fig.add_subplot(gs[1, :])
+    ax_tab = fig.add_subplot(gs[1, :])

     alpha = series_list[0].alpha if series_list else 0.05
     nominal_level = 1.0 - alpha
@@ -135,18 +211,36 @@ def plot_synthese(series_list: List[Series], out_png: str,
     handles = []
     for s in series_list:
         yerr = np.vstack([s.err_low, s.err_high])
-        h = ax_cov.errorbar(s.N, s.coverage, yerr=yerr,
-                            fmt='o-', lw=1.6, ms=6, capsize=3, zorder=3,
-                            label=s.label)
+        h = ax_cov.errorbar(
+            s.N,
+            s.coverage,
+            yerr=yerr,
+            fmt="o-",
+            lw=1.6,
+            ms=6,
+            capsize=3,
+            zorder=3,
+            label=s.label,
+        )
         handles.append(h)

-    ax_cov.axhline(nominal_level, color='crimson', ls='--', lw=1.5, zorder=1)
-    nominal_handle = Line2D([0],[0], color='crimson', lw=1.5, ls='--',
-                            label=f"Niveau nominal {int(nominal_level*100)}%")
-
-    ax_cov.legend([nominal_handle] + handles,
-                  [nominal_handle.get_label()] + [h.get_label() for h in handles],
-                  loc="upper left", frameon=True, fontsize=10)
+    ax_cov.axhline(nominal_level, color="crimson", ls="--", lw=1.5, zorder=1)
+    nominal_handle = Line2D(
+        [0],
+        [0],
+        color="crimson",
+        lw=1.5,
+        ls="--",
+        label=f"Niveau nominal {int(nominal_level*100)}%",
+    )
+
+    ax_cov.legend(
+        [nominal_handle] + handles,
+        [nominal_handle.get_label()] + [h.get_label() for h in handles],
+        loc="upper left",
+        frameon=True,
+        fontsize=10,
+    )

     ax_cov.set_title("Couverture vs N")
     ax_cov.set_xlabel("Taille d'échantillon N")
@@ -156,11 +250,22 @@ def plot_synthese(series_list: List[Series], out_png: str,
         ymax = ymax_cov if ymax_cov is not None else ax_cov.get_ylim()[1]
         ax_cov.set_ylim(ymin, ymax)

-    ax_cov.text(0.02, 0.06,
-                "Barres = Wilson 95% (n = outer B=400,2000); IC interne = percentile (inner B=2000)",
-                transform=ax_cov.transAxes, fontsize=9, va="bottom")
-    ax_cov.text(0.02, 0.03, "α=0.05. Variabilité ↑ pour petits N.",
-                transform=ax_cov.transAxes, fontsize=9, va="bottom")
+    ax_cov.text(
+        0.02,
+        0.06,
+        "Barres = Wilson 95% (n = outer B=400,2000); IC interne = percentile (inner B=2000)",
+        transform=ax_cov.transAxes,
+        fontsize=9,
+        va="bottom",
+    )
+    ax_cov.text(
+        0.02,
+        0.03,
+        "α=0.05. Variabilité ↑ pour petits N.",
+        transform=ax_cov.transAxes,
+        fontsize=9,
+        va="bottom",
+    )

     for s, h in zip(series_list, handles):
         color = h.lines[0].get_color() if hasattr(h, "lines") and h.lines else None
@@ -173,25 +278,39 @@ def plot_synthese(series_list: List[Series], out_png: str,
     ax_tab.set_title("Synthèse numérique (résumé)", y=0.88, pad=12, fontsize=12)
     rows = compute_summary_rows(series_list)

-    col_labels = ["série", "outer_B", "inner_B", "mean_cov", "med_cov", "std_cov", "p95_cov", "med_width [rad]"]
+    col_labels = [
+        "série",
+        "outer_B",
+        "inner_B",
+        "mean_cov",
+        "med_cov",
+        "std_cov",
+        "p95_cov",
+        "med_width [rad]",
+    ]
     cell_text = []
     for r in rows:
-        cell_text.append([
-            r[0],
-            f"{r[1]}" if r[1] != "" else "-",
-            f"{r[2]}" if r[2] != "" else "-",
-            f"{r[3]:.3f}" if np.isfinite(r[3]) else "-",
-            f"{r[4]:.3f}" if np.isfinite(r[4]) else "-",
-            f"{r[5]:.3f}" if np.isfinite(r[5]) else "-",
-            f"{r[6]:.3f}" if np.isfinite(r[6]) else "-",
-            f"{r[7]:.5f}" if np.isfinite(r[7]) else "-",
-        ])
+        cell_text.append(
+            [
+                r[0],
+                f"{r[1]}" if r[1] != "" else "-",
+                f"{r[2]}" if r[2] != "" else "-",
+                f"{r[3]:.3f}" if np.isfinite(r[3]) else "-",
+                f"{r[4]:.3f}" if np.isfinite(r[4]) else "-",
+                f"{r[5]:.3f}" if np.isfinite(r[5]) else "-",
+                f"{r[6]:.3f}" if np.isfinite(r[6]) else "-",
+                f"{r[7]:.5f}" if np.isfinite(r[7]) else "-",
+            ]
+        )

     ax_tab.axis("off")
-    table = ax_tab.table(cellText=cell_text,
-                         colLabels=col_labels,
-                         cellLoc='center', colLoc='center',
-                         loc='center')
+    table = ax_tab.table(
+        cellText=cell_text,
+        colLabels=col_labels,
+        cellLoc="center",
+        colLoc="center",
+        loc="center",
+    )
     table.auto_set_font_size(False)
     table.set_fontsize(10)
     table.scale(1.0, 1.3)
@@ -212,21 +331,29 @@ def plot_synthese(series_list: List[Series], out_png: str,
     if series_list:
         s0 = series_list[0]
         M0, outer0, inner0 = detect_reps_params(s0.params)
-        cap1 = (f"Protocole : M={int(M0) if np.isfinite(M0) else '?'} réalisations/point ; "
-                f"barres = Wilson 95% (calculées sur M). "
-                f"Bootstrap : outer_B={int(outer0) if np.isfinite(outer0) else '?'}, "
-                f"inner_B={int(inner0) if np.isfinite(inner0) else '?'} ; α={s0.alpha:.2f}.")
-        cap2 = ("Largeur moyenne en radians ; ajustement log–log width ≈ a·N^b ; " +
-                " ; ".join(slopes) + ".")
+        cap1 = (
+            f"Protocole : M={int(M0) if np.isfinite(M0) else '?'} réalisations/point ; "
+            f"barres = Wilson 95% (calculées sur M). "
+            f"Bootstrap : outer_B={int(outer0) if np.isfinite(outer0) else '?'}, "
+            f"inner_B={int(inner0) if np.isfinite(inner0) else '?'} ; α={s0.alpha:.2f}."
+        )
+        cap2 = (
+            "Largeur moyenne en radians ; ajustement log–log width ≈ a·N^b ; "
+            + " ; ".join(slopes)
+            + "."
+        )
         fig.text(0.5, 0.035, cap1, ha="center", fontsize=9)
         fig.text(0.5, 0.017, cap2, ha="center", fontsize=9)

-    fig.subplots_adjust(left=0.06, right=0.98, top=0.93, bottom=0.09, wspace=0.25, hspace=0.35)
+    fig.subplots_adjust(
+        left=0.06, right=0.98, top=0.93, bottom=0.09, wspace=0.25, hspace=0.35
+    )

     os.makedirs(os.path.dirname(out_png) or ".", exist_ok=True)
     fig.savefig(out_png, dpi=dpi, bbox_inches="tight")
     print(f"[OK] Figure écrite : {out_png}")

+
 # ---------- CLI ----------
 def main(argv=None):
     ap = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
@@ -262,8 +389,15 @@ def main(argv=None):
     save_summary_csv(series_list, out_csv)
     print(f"[OK] CSV écrit : {out_csv}")

-    plot_synthese(series_list, args.out, figsize=(fig_w, fig_h), dpi=args.dpi,
-                  ymin_cov=args.ymin_coverage, ymax_cov=args.ymax_coverage)
+    plot_synthese(
+        series_list,
+        args.out,
+        figsize=(fig_w, fig_h),
+        dpi=args.dpi,
+        ymin_cov=args.ymin_coverage,
+        ymax_cov=args.ymax_coverage,
+    )
+

 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter10/qc_wrapped_vs_unwrapped.py b/zz-scripts/chapter10/qc_wrapped_vs_unwrapped.py
index 96414a0..4573bff 100755
--- a/zz-scripts/chapter10/qc_wrapped_vs_unwrapped.py
+++ b/zz-scripts/chapter10/qc_wrapped_vs_unwrapped.py
@@ -9,12 +9,14 @@ Vérification rapide : calcul p95 des résidus φ_ref - φ_mcgt


 """
+
 from __future__ import annotations
-import argparse, json, os
+import argparse
+import json
+import os
 import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt
-from math import pi

 # import fonctions existantes
 try:
@@ -23,40 +25,52 @@ try:
 except Exception as e:
     raise SystemExit(f"Erreur import mcgt : {e}")

+
 def circ_diff(a: np.ndarray, b: np.ndarray) -> np.ndarray:
     """Distance angulaire minimale a-b renvoyée dans [-pi,pi]."""
     d = (a - b) % (2 * np.pi)
     d = np.where(d > np.pi, d - 2 * np.pi, d)
     return d

+
 def load_ref_grid(path):
-    arr = np.loadtxt(path, delimiter=',', skiprows=1, usecols=[0])
+    arr = np.loadtxt(path, delimiter=",", skiprows=1, usecols=[0])
     return arr

+
 def select_ids(best_json, results_df, k=10):
-    with open(best_json, 'r', encoding='utf-8') as f:
+    with open(best_json, "r", encoding="utf-8") as f:
         bj = json.load(f)
-    top = [int(x['id']) for x in bj.get('top_k', [])][:k]
+    top = [int(x["id"]) for x in bj.get("top_k", [])][:k]
     # median id: nearest to median p95 in results
-    med_p95 = results_df['p95_20_300'].median()
-    med_id = int((results_df.iloc[(results_df['p95_20_300'] - med_p95).abs().argsort()]).iloc[0]['id'])
-    worst_id = int(results_df.loc[results_df['p95_20_300'].idxmax()]['id'])
+    med_p95 = results_df["p95_20_300"].median()
+    med_id = int(
+        (results_df.iloc[(results_df["p95_20_300"] - med_p95).abs().argsort()]).iloc[0][
+            "id"
+        ]
+    )
+    worst_id = int(results_df.loc[results_df["p95_20_300"].idxmax()]["id"])
     # ensure uniqueness and include top few
-    ids = top[:min(len(top), k)]
+    ids = top[: min(len(top), k)]
     if med_id not in ids:
         ids.append(med_id)
     if worst_id not in ids:
         ids.append(worst_id)
     return ids

+
 def compute_resids_for_id(id_, samples_df, fgrid, outdir, ref_grid_path):
-    row = samples_df.loc[samples_df['id'] == int(id_)].squeeze()
+    row = samples_df.loc[samples_df["id"] == int(id_)].squeeze()
     if row.empty:
         raise KeyError(f"id {id_} absent des samples")
     # construire theta dict
-    theta = {k: float(row[k]) for k in ["m1", "m2", "q0star", "alpha", "phi0", "tc", "dist", "incl"] if k in row.index}
+    theta = {
+        k: float(row[k])
+        for k in ["m1", "m2", "q0star", "alpha", "phi0", "tc", "dist", "incl"]
+        if k in row.index
+    }
     # calcul des phases
-    phi_ref = compute_phi_ref(fgrid, float(row['m1']), float(row['m2']))
+    phi_ref = compute_phi_ref(fgrid, float(row["m1"]), float(row["m2"]))
     phi_m = phi_mcgt(fgrid, theta)
     # résidus
     raw = np.abs(phi_ref - phi_m)
@@ -70,36 +84,47 @@ def compute_resids_for_id(id_, samples_df, fgrid, outdir, ref_grid_path):
     # sauvegarde CSV
     os.makedirs(outdir, exist_ok=True)
     csvfile = os.path.join(outdir, f"qc_resid_id{int(id_)}.csv")
-    dfout = pd.DataFrame({
-        "f_Hz": fgrid,
-        "raw_abs": raw,
-        "unwrap_abs": unwrap,
-        "circ_abs": circ
-    })
+    dfout = pd.DataFrame(
+        {"f_Hz": fgrid, "raw_abs": raw, "unwrap_abs": unwrap, "circ_abs": circ}
+    )
     dfout.to_csv(csvfile, index=False)
     # trace
     pngfile = os.path.join(outdir, f"qc_resid_id{int(id_)}.png")
-    plt.figure(figsize=(7,4))
+    plt.figure(figsize=(7, 4))
     plt.semilogx(fgrid, raw, label=f"raw p95={p95_raw:.4f} rad", alpha=0.6)
     plt.semilogx(fgrid, unwrap, label=f"unwrap p95={p95_unwrap:.4f} rad", alpha=0.6)
     plt.semilogx(fgrid, circ, label=f"circ p95={p95_circ:.4f} rad", alpha=0.9)
-    plt.xlabel("f [Hz]"); plt.ylabel("|Δφ(f)| [rad]")
+    plt.xlabel("f [Hz]")
+    plt.ylabel("|Δφ(f)| [rad]")
     plt.title(f"Residus ID {int(id_)} (raw / unwrap / circ)")
     plt.legend(loc="best", fontsize="small")
     plt.tight_layout()
     plt.savefig(pngfile, dpi=150)
     plt.close()
-    return {"id": int(id_), "p95_raw": p95_raw, "p95_unwrap": p95_unwrap, "p95_circ": p95_circ,
-            "csv": csvfile, "png": pngfile}
+    return {
+        "id": int(id_),
+        "p95_raw": p95_raw,
+        "p95_unwrap": p95_unwrap,
+        "p95_circ": p95_circ,
+        "csv": csvfile,
+        "png": pngfile,
+    }
+

 def main(argv=None):
     parser = argparse.ArgumentParser(description="QC: wrapped vs unwrapped p95")
-    parser.add_argument("--best", required=True, help="json top-K (zz-data/chapter10/10_mc_best.json)")
+    parser.add_argument(
+        "--best", required=True, help="json top-K (zz-data/chapter10/10_mc_best.json)"
+    )
     parser.add_argument("--samples", required=True, help="csv samples")
-    parser.add_argument("--results", required=True, help="csv results (pour median/worst)")
+    parser.add_argument(
+        "--results", required=True, help="csv results (pour median/worst)"
+    )
     parser.add_argument("--ref-grid", required=True, help="grille de référence (csv)")
     parser.add_argument("--k", type=int, default=10, help="combien de top-K à inclure")
-    parser.add_argument("--outdir", default="zz-data/chapter10/qc_wrapped", help="répertoire de sortie")
+    parser.add_argument(
+        "--outdir", default="zz-data/chapter10/qc_wrapped", help="répertoire de sortie"
+    )
     args = parser.parse_args(argv)

     # load
@@ -116,19 +141,27 @@ def main(argv=None):
         try:
             out = compute_resids_for_id(id_, samples, fgrid, args.outdir, args.ref_grid)
             summary.append(out)
-            print("   -> p95_raw:   ", f"{out['p95_raw']:.6f}",
-                  " p95_unwrap:", f"{out['p95_unwrap']:.6f}",
-                  " p95_circ:", f"{out['p95_circ']:.6f}")
+            print(
+                "   -> p95_raw:   ",
+                f"{out['p95_raw']:.6f}",
+                " p95_unwrap:",
+                f"{out['p95_unwrap']:.6f}",
+                " p95_circ:",
+                f"{out['p95_circ']:.6f}",
+            )
         except Exception as e:
             print("   ERREUR pour id", id_, ":", e)

     # rapport synthèse
     print("\n=== RAPPORT SYNTHÈSE ===")
     for s in summary:
-        change = (s['p95_raw'] - s['p95_circ']) / (s['p95_raw'] + 1e-12)
-        print(f"id={s['id']:5d}  raw={s['p95_raw']:.6f}  circ={s['p95_circ']:.6f}  unwrap={s['p95_unwrap']:.6f}  delta%={(change*100):+.2f}%")
+        change = (s["p95_raw"] - s["p95_circ"]) / (s["p95_raw"] + 1e-12)
+        print(
+            f"id={s['id']:5d}  raw={s['p95_raw']:.6f}  circ={s['p95_circ']:.6f}  unwrap={s['p95_unwrap']:.6f}  delta%={(change*100):+.2f}%"
+        )
     print("\nFichiers écrits dans:", os.path.abspath(args.outdir))
     return 0

+
 if __name__ == "__main__":
     raise SystemExit(main())
diff --git a/zz-scripts/chapter10/recompute_p95_circular.py b/zz-scripts/chapter10/recompute_p95_circular.py
index 741dbbf..59d80ed 100755
--- a/zz-scripts/chapter10/recompute_p95_circular.py
+++ b/zz-scripts/chapter10/recompute_p95_circular.py
@@ -12,17 +12,23 @@ Usage:
     --ref-grid zz-data/chapter09/09_phases_imrphenom.csv \
     --out zz-data/chapter10/10_mc_results.circ.csv
 """
+
 from __future__ import annotations
-import argparse, json, os
-import numpy as np, pandas as pd
+import argparse
+import json
+import os
+import numpy as np
+import pandas as pd
 from mcgt.backends.ref_phase import compute_phi_ref
 from mcgt.phase import phi_mcgt

-def circ_diff(a,b):
-    d = (a - b) % (2*np.pi)
-    d = np.where(d > np.pi, d - 2*np.pi, d)
+
+def circ_diff(a, b):
+    d = (a - b) % (2 * np.pi)
+    d = np.where(d > np.pi, d - 2 * np.pi, d)
     return d

+
 def main(argv=None):
     parser = argparse.ArgumentParser()
     parser.add_argument("--results", required=True)
@@ -33,7 +39,7 @@ def main(argv=None):

     df_res = pd.read_csv(args.results)
     df_samp = pd.read_csv(args.samples)
-    fgrid = np.loadtxt(args.ref_grid, delimiter=',', skiprows=1, usecols=[0])
+    fgrid = np.loadtxt(args.ref_grid, delimiter=",", skiprows=1, usecols=[0])

     mask = (fgrid >= 20.0) & (fgrid <= 300.0)

@@ -41,21 +47,25 @@ def main(argv=None):
     df_out = df_res.copy()
     new_p95 = []
     for idx, row in df_out.iterrows():
-        id_ = int(row['id'])
-        samp = df_samp.loc[df_samp['id'] == id_].squeeze()
+        id_ = int(row["id"])
+        samp = df_samp.loc[df_samp["id"] == id_].squeeze()
         if samp.empty:
             new_p95.append(np.nan)
             continue
-        theta = {k: float(samp[k]) for k in ["m1","m2","q0star","alpha","phi0","tc","dist","incl"] if k in samp.index}
-        phi_ref = compute_phi_ref(fgrid, float(samp['m1']), float(samp['m2']))
+        theta = {
+            k: float(samp[k])
+            for k in ["m1", "m2", "q0star", "alpha", "phi0", "tc", "dist", "incl"]
+            if k in samp.index
+        }
+        phi_ref = compute_phi_ref(fgrid, float(samp["m1"]), float(samp["m2"]))
         phi_m = phi_mcgt(fgrid, theta)
         circ = np.abs(circ_diff(phi_ref, phi_m))
         p95 = float(np.percentile(circ[mask], 95))
         new_p95.append(p95)

-    df_out['p95_20_300_circ'] = new_p95
+    df_out["p95_20_300_circ"] = new_p95
     # optionally replace existing column:
-    df_out['p95_20_300_recalc'] = df_out['p95_20_300_circ']
+    df_out["p95_20_300_recalc"] = df_out["p95_20_300_circ"]

     outpath = args.out or args.results.replace(".csv", ".circ.csv")
     df_out.to_csv(outpath, index=False)
@@ -65,7 +75,7 @@ def main(argv=None):
         "src_samples": os.path.abspath(args.samples),
         "ref_grid": os.path.abspath(args.ref_grid),
         "out_results": os.path.abspath(outpath),
-        "n_rows": int(len(df_out))
+        "n_rows": int(len(df_out)),
     }
     with open(outpath + ".manifest.json", "w", encoding="utf-8") as f:
         json.dump(man, f, indent=2)
@@ -73,5 +83,6 @@ def main(argv=None):
     print("Manifeste:", outpath + ".manifest.json")
     return 0

+
 if __name__ == "__main__":
     raise SystemExit(main())
diff --git a/zz-scripts/chapter10/regen_fig05_using_circp95.py b/zz-scripts/chapter10/regen_fig05_using_circp95.py
index edc6ea8..01755bf 100755
--- a/zz-scripts/chapter10/regen_fig05_using_circp95.py
+++ b/zz-scripts/chapter10/regen_fig05_using_circp95.py
@@ -5,19 +5,26 @@ regen_fig05_using_circp95.py
 Figure 05 : Histogramme + CDF des p95 recalculés en métrique circulaire.

 """
+
 from __future__ import annotations
-import argparse, textwrap
+import argparse
+import textwrap
 import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt
 import matplotlib.lines as mlines
 from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset

+
 # ---------- utils ----------
 def detect_p95_column(df: pd.DataFrame) -> str:
     candidates = [
-        "p95_20_300_recalc", "p95_20_300_circ", "p95_20_300_recalced",
-        "p95_20_300", "p95_circ", "p95_recalc"
+        "p95_20_300_recalc",
+        "p95_20_300_circ",
+        "p95_20_300_recalced",
+        "p95_20_300",
+        "p95_circ",
+        "p95_recalc",
     ]
     for c in candidates:
         if c in df.columns:
@@ -27,22 +34,42 @@ def detect_p95_column(df: pd.DataFrame) -> str:
             return c
     raise KeyError("Aucune colonne 'p95' détectée dans le CSV results.")

+
 # ---------- main ----------
 def main():
     ap = argparse.ArgumentParser()
-    ap.add_argument("--results", required=True, help="CSV avec p95 circulaire recalculé")
-    ap.add_argument("--out", default="fig_05_hist_cdf_metrics.png", help="PNG de sortie")
-    ap.add_argument("--ref-p95", type=float, default=0.7104087123286049, help="p95 de référence [rad]")
+    ap.add_argument(
+        "--results", required=True, help="CSV avec p95 circulaire recalculé"
+    )
+    ap.add_argument(
+        "--out", default="fig_05_hist_cdf_metrics.png", help="PNG de sortie"
+    )
+    ap.add_argument(
+        "--ref-p95",
+        type=float,
+        default=0.7104087123286049,
+        help="p95 de référence [rad]",
+    )
     ap.add_argument("--bins", type=int, default=50, help="Nb de bacs histogramme")
     ap.add_argument("--dpi", type=int, default=150, help="DPI du PNG")
     # position et fenêtre du zoom (centre + demi-étendues)
     ap.add_argument("--zoom-x", type=float, default=3.0, help="centre X du zoom (rad)")
-    ap.add_argument("--zoom-y", type=float, default=35.0, help="centre Y du zoom (counts)")
-    ap.add_argument("--zoom-dx", type=float, default=0.30, help="demi-largeur X du zoom (rad)")
-    ap.add_argument("--zoom-dy", type=float, default=30.0, help="demi-hauteur Y du zoom (counts)")
+    ap.add_argument(
+        "--zoom-y", type=float, default=35.0, help="centre Y du zoom (counts)"
+    )
+    ap.add_argument(
+        "--zoom-dx", type=float, default=0.30, help="demi-largeur X du zoom (rad)"
+    )
+    ap.add_argument(
+        "--zoom-dy", type=float, default=30.0, help="demi-hauteur Y du zoom (counts)"
+    )
     # taille du panneau de zoom (fraction de l’axe)
-    ap.add_argument("--zoom-w", type=float, default=0.35, help="largeur du zoom (fraction)")
-    ap.add_argument("--zoom-h", type=float, default=0.25, help="hauteur du zoom (fraction)")
+    ap.add_argument(
+        "--zoom-w", type=float, default=0.35, help="largeur du zoom (fraction)"
+    )
+    ap.add_argument(
+        "--zoom-h", type=float, default=0.25, help="hauteur du zoom (fraction)"
+    )
     args = ap.parse_args()

     # --- lecture & colonne p95 ---
@@ -60,7 +87,11 @@ def main():

     # --- stats ---
     N = p95.size
-    mean, median, std = float(np.mean(p95)), float(np.median(p95)), float(np.std(p95, ddof=0))
+    mean, median, std = (
+        float(np.mean(p95)),
+        float(np.median(p95)),
+        float(np.std(p95, ddof=0)),
+    )
     n_below = int((p95 < args.ref_p95).sum())
     frac_below = n_below / max(1, N)

@@ -77,15 +108,22 @@ def main():
     ax2 = ax.twinx()
     sorted_p = np.sort(p95)
     ecdf = np.arange(1, N + 1) / N
-    cdf_line, = ax2.plot(sorted_p, ecdf, lw=2)
+    (cdf_line,) = ax2.plot(sorted_p, ecdf, lw=2)
     ax2.set_ylabel("CDF empirique")
     ax2.set_ylim(0.0, 1.02)

     # Ligne verticale de référence
     ax.axvline(args.ref_p95, color="crimson", linestyle="--", lw=2)
-    ax.text(args.ref_p95, ax.get_ylim()[1] * 0.45,
-            f"ref = {args.ref_p95:.4f} rad",
-            color="crimson", rotation=90, va="center", ha="right", fontsize=10)
+    ax.text(
+        args.ref_p95,
+        ax.get_ylim()[1] * 0.45,
+        f"ref = {args.ref_p95:.4f} rad",
+        color="crimson",
+        rotation=90,
+        va="center",
+        ha="right",
+        fontsize=10,
+    )

     # Boîte de stats (haut-gauche)
     stat_lines = [
@@ -98,9 +136,16 @@ def main():
         stat_lines.append(f"wrapped_corrected = {wrapped_corrected}")
     stat_lines.append(f"p(P95 < ref) = {frac_below:.3f} (n={n_below})")
     stat_text = "\n".join(stat_lines)
-    ax.text(0.02, 0.98, stat_text, transform=ax.transAxes, fontsize=10,
-            va="top", ha="left",
-            bbox=dict(boxstyle="round", fc="white", ec="black", lw=1, alpha=0.95))
+    ax.text(
+        0.02,
+        0.98,
+        stat_text,
+        transform=ax.transAxes,
+        fontsize=10,
+        va="top",
+        ha="left",
+        bbox=dict(boxstyle="round", fc="white", ec="black", lw=1, alpha=0.95),
+    )

     # Petite légende (sous la boîte de stats)
     handles = []
@@ -108,13 +153,22 @@ def main():
         handles.append(patches[0])
     else:
         from matplotlib.patches import Rectangle
-        handles.append(Rectangle((0, 0), 1, 1, facecolor="C0", edgecolor="k", alpha=0.7))
+
+        handles.append(
+            Rectangle((0, 0), 1, 1, facecolor="C0", edgecolor="k", alpha=0.7)
+        )
     proxy_cdf = mlines.Line2D([], [], color=cdf_line.get_color(), lw=2)
     proxy_ref = mlines.Line2D([], [], color="crimson", linestyle="--", lw=2)
     handles += [proxy_cdf, proxy_ref]
     labels = ["Histogramme (effectifs)", "CDF empirique", "p95 réf"]
-    ax.legend(handles, labels, loc="upper left", bbox_to_anchor=(0.02, 0.72),
-              frameon=True, fontsize=10)
+    ax.legend(
+        handles,
+        labels,
+        loc="upper left",
+        bbox_to_anchor=(0.02, 0.72),
+        frameon=True,
+        fontsize=10,
+    )

     # Inset zoom
     inset_ax = inset_axes(
@@ -145,14 +199,16 @@ def main():

     ax.set_title("Distribution de p95_20_300 (MC global)", fontsize=15)
     foot = textwrap.fill(
-        (r"Métrique : distance circulaire (mod $2\pi$). "
-         r"Définition : p95 = $95^{\mathrm{e}}$ centile de $|\Delta\phi(f)|$ pour $f\in[20,300]\ \mathrm{Hz}$. "
-         r"Corrections : sauts de branchement corrigés, "
-         rf"$N_{{\mathrm{{wrapped\_corrected}}}} = {wrapped_corrected if wrapped_corrected is not None else 0}$. "
-         r"Comparaison : "
-         rf"$p(\mathrm{{p95}}<\mathrm{{p95_{{ref}}}}) = {frac_below:.3f}$ "
-         rf"(n = {n_below})."),
-        width=180
+        (
+            r"Métrique : distance circulaire (mod $2\pi$). "
+            r"Définition : p95 = $95^{\mathrm{e}}$ centile de $|\Delta\phi(f)|$ pour $f\in[20,300]\ \mathrm{Hz}$. "
+            r"Corrections : sauts de branchement corrigés, "
+            rf"$N_{{\mathrm{{wrapped\_corrected}}}} = {wrapped_corrected if wrapped_corrected is not None else 0}$. "
+            r"Comparaison : "
+            rf"$p(\mathrm{{p95}}<\mathrm{{p95_{{ref}}}}) = {frac_below:.3f}$ "
+            rf"(n = {n_below})."
+        ),
+        width=180,
     )
     plt.tight_layout(rect=[0, 0.14, 1, 0.98])
     fig.text(0.5, 0.04, foot, ha="center", va="bottom", fontsize=9)
@@ -160,5 +216,6 @@ def main():
     fig.savefig(args.out, dpi=args.dpi)
     print(f"Wrote : {args.out}")

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/chapter10/update_manifest_with_hashes.py b/zz-scripts/chapter10/update_manifest_with_hashes.py
index 478de2a..312118e 100755
--- a/zz-scripts/chapter10/update_manifest_with_hashes.py
+++ b/zz-scripts/chapter10/update_manifest_with_hashes.py
@@ -1,10 +1,15 @@
 #!/usr/bin/env python3
-import json, subprocess, pathlib, sys
+import json
+import subprocess
+import pathlib
+import sys
 from importlib import metadata

 manifest_path = pathlib.Path("zz-data/chapter10/10_mc_run_manifest.json")
 if not manifest_path.exists():
-    print("Manifest missing:", manifest_path); sys.exit(1)
+    print("Manifest missing:", manifest_path)
+    sys.exit(1)
+

 def sha256(fpath):
     res = subprocess.run(["sha256sum", str(fpath)], capture_output=True, text=True)
@@ -12,6 +17,7 @@ def sha256(fpath):
         return None
     return res.stdout.split()[0]

+
 files = {
     "ref_phases": "zz-data/chapter09/09_phases_imrphenom.csv",
     "metrics_phase_json": "zz-data/chapter09/09_metrics_phase.json",
@@ -22,16 +28,20 @@ files = {
 }

 h = {}
-for k,p in files.items():
+for k, p in files.items():
     pth = pathlib.Path(p)
     if pth.exists():
-        h[k] = {"path": str(pth.resolve()), "sha256": sha256(pth), "size": pth.stat().st_size}
+        h[k] = {
+            "path": str(pth.resolve()),
+            "sha256": sha256(pth),
+            "size": pth.stat().st_size,
+        }
     else:
         h[k] = {"path": str(pth.resolve()), "sha256": None, "size": None}

 # versions libs
 versions = {}
-for pkg in ("numpy","pandas","scipy","matplotlib","joblib","pycbc"):
+for pkg in ("numpy", "pandas", "scipy", "matplotlib", "joblib", "pycbc"):
     try:
         versions[pkg] = metadata.version(pkg)
     except Exception:
@@ -39,6 +49,7 @@ for pkg in ("numpy","pandas","scipy","matplotlib","joblib","pycbc"):

 # python version
 import platform
+
 pyv = platform.python_version()

 m = json.loads(manifest_path.read_text())
diff --git a/zz-scripts/manifest_tools/populate_manifest.py b/zz-scripts/manifest_tools/populate_manifest.py
index 4f1de8e..1a35a88 100755
--- a/zz-scripts/manifest_tools/populate_manifest.py
+++ b/zz-scripts/manifest_tools/populate_manifest.py
@@ -10,11 +10,19 @@ Fonctions principales:
 Usage:
   python3 remplir_manifest.py zz-manifests/manifest_publication.json --repo-root /home/jplal/MCGT --force --sign sha256
 """
+
 from __future__ import annotations
-import argparse, json, os, hashlib, subprocess, sys, shutil
+import argparse
+import json
+import os
+import hashlib
+import subprocess
+import sys
+import shutil
 from datetime import datetime, timezone
 from fnmatch import fnmatch

+
 def sha256_of(path):
     h = hashlib.sha256()
     with open(path, "rb") as f:
@@ -22,10 +30,12 @@ def sha256_of(path):
             h.update(chunk)
     return h.hexdigest()

+
 def mtime_iso_of(path):
     st = os.stat(path)
     return datetime.fromtimestamp(st.st_mtime, tz=timezone.utc).isoformat()

+
 def git_hash_of(path, repo_root=None):
     # uses git hash-object (doesn't write object)
     try:
@@ -43,6 +53,7 @@ def git_hash_of(path, repo_root=None):
         pass
     return None

+
 def find_by_basename(basename, repo_root, max_results=10):
     matches = []
     for root, dirs, files in os.walk(repo_root):
@@ -52,13 +63,17 @@ def find_by_basename(basename, repo_root, max_results=10):
                 break
     return matches

+
 def should_exclude(path, exclude_patterns):
     for pat in exclude_patterns:
         if fnmatch(path, pat) or fnmatch(os.path.basename(path), pat):
             return True
     return False

-def process_manifest(manifest_path, repo_root, force=False, exclude_patterns=None, search_missing=False):
+
+def process_manifest(
+    manifest_path, repo_root, force=False, exclude_patterns=None, search_missing=False
+):
     with open(manifest_path, "r", encoding="utf-8") as f:
         manifest = json.load(f)

@@ -115,7 +130,12 @@ def process_manifest(manifest_path, repo_root, force=False, exclude_patterns=Non
             git_hash = git_hash_of(abs_path, repo_root=repo_root)
             # assign/overwrite only if missing or force
             changed = False
-            for k,v in (("size_bytes", size_bytes), ("mtime_iso", mtime_iso), ("sha256", sha), ("git_hash", git_hash)):
+            for k, v in (
+                ("size_bytes", size_bytes),
+                ("mtime_iso", mtime_iso),
+                ("sha256", sha),
+                ("git_hash", git_hash),
+            ):
                 if v is None:
                     # skip if cannot compute
                     continue
@@ -145,19 +165,34 @@ def process_manifest(manifest_path, repo_root, force=False, exclude_patterns=Non
     manifest["total_size_bytes"] = total_size
     return manifest, updated, missing

+
 def backup_file(path):
     bak = path + ".bak"
     shutil.copy2(path, bak)
     return bak

+
 def sign_manifest(manifest_path, method="sha256"):
     if method == "gpg":
         # try to create detached signature
         sig_path = manifest_path + ".sig"
         try:
-            r = subprocess.run(["gpg", "--batch", "--yes", "--output", sig_path, "--detach-sign", manifest_path])
+            r = subprocess.run(
+                [
+                    "gpg",
+                    "--batch",
+                    "--yes",
+                    "--output",
+                    sig_path,
+                    "--detach-sign",
+                    manifest_path,
+                ]
+            )
             if r.returncode == 0:
-                return {"signed_by": "gpg", "signature_file": os.path.basename(sig_path)}
+                return {
+                    "signed_by": "gpg",
+                    "signature_file": os.path.basename(sig_path),
+                }
         except Exception:
             pass
         return {"signed_by": None}
@@ -167,16 +202,40 @@ def sign_manifest(manifest_path, method="sha256"):
     else:
         return {}

+
 def main():
-    import argparse
-    ap = argparse.ArgumentParser(description="Remplir / mettre à jour manifest JSON (sha256, size, mtime, git_hash)")
+    ap = argparse.ArgumentParser(
+        description="Remplir / mettre à jour manifest JSON (sha256, size, mtime, git_hash)"
+    )
     ap.add_argument("manifest", help="chemin vers manifest JSON")
-    ap.add_argument("--repo-root", default=".", help="racine du dépôt (répertoire) — convertira paths en relatifs par rapport à cette racine")
-    ap.add_argument("--force", action="store_true", help="forcer la réécriture des champs même s'ils sont présents")
-    ap.add_argument("--search", action="store_true", help="si fichier introuvable, tenter une recherche par basename sous repo-root (unique match)")
-    ap.add_argument("--exclude", action="append", help="pattern à exclure (glob) pour ne pas mettre à jour certaines entrées; peut être utilisé plusieurs fois")
-    ap.add_argument("--sign", choices=["gpg", "sha256"], help="signer le manifeste (gpg) ou ajouter digest sha256 (sha256)")
-    ap.add_argument("--no-backup", action="store_true", help="ne pas créer de backup (danger !)")
+    ap.add_argument(
+        "--repo-root",
+        default=".",
+        help="racine du dépôt (répertoire) — convertira paths en relatifs par rapport à cette racine",
+    )
+    ap.add_argument(
+        "--force",
+        action="store_true",
+        help="forcer la réécriture des champs même s'ils sont présents",
+    )
+    ap.add_argument(
+        "--search",
+        action="store_true",
+        help="si fichier introuvable, tenter une recherche par basename sous repo-root (unique match)",
+    )
+    ap.add_argument(
+        "--exclude",
+        action="append",
+        help="pattern à exclure (glob) pour ne pas mettre à jour certaines entrées; peut être utilisé plusieurs fois",
+    )
+    ap.add_argument(
+        "--sign",
+        choices=["gpg", "sha256"],
+        help="signer le manifeste (gpg) ou ajouter digest sha256 (sha256)",
+    )
+    ap.add_argument(
+        "--no-backup", action="store_true", help="ne pas créer de backup (danger !)"
+    )
     args = ap.parse_args()

     manifest_path = args.manifest
@@ -195,7 +254,7 @@ def main():
         repo_root,
         force=args.force,
         exclude_patterns=args.exclude or [],
-        search_missing=args.search
+        search_missing=args.search,
     )

     # write manifest
@@ -224,5 +283,6 @@ def main():
     # exit code: 0 normal
     sys.exit(0)

+
 if __name__ == "__main__":
     main()
diff --git a/zz-scripts/manifest_tools/verify_manifest.py b/zz-scripts/manifest_tools/verify_manifest.py
index f9cdaa2..b53557e 100755
--- a/zz-scripts/manifest_tools/verify_manifest.py
+++ b/zz-scripts/manifest_tools/verify_manifest.py
@@ -8,10 +8,17 @@ Vérifier un manifest JSON :
 Usage:
   python3 verify_manifest.py zz-manifests/manifest_publication.json --repo-root . --output report.json
 """
+
 from __future__ import annotations
-import argparse, json, os, hashlib, subprocess, sys
+import argparse
+import json
+import os
+import hashlib
+import subprocess
+import sys
 from datetime import datetime, timezone

+
 def sha256_of(path):
     h = hashlib.sha256()
     with open(path, "rb") as f:
@@ -19,11 +26,13 @@ def sha256_of(path):
             h.update(chunk)
     return h.hexdigest()

+
 def mtime_iso_of(path):
     st = os.stat(path)
     # timezone-aware UTC
     return datetime.fromtimestamp(st.st_mtime, tz=timezone.utc).isoformat()

+
 def git_hash_of(path, repo_root=None):
     # Return git object hash (sha1) if git available and file inside git repo.
     try:
@@ -39,6 +48,7 @@ def git_hash_of(path, repo_root=None):
         pass
     return None

+
 def check_entry(entry, repo_root):
     path = entry.get("path")
     result = {"path": path, "exists": False, "issues": [], "expected": {}, "actual": {}}
@@ -61,27 +71,44 @@ def check_entry(entry, repo_root):
     actual_size = st.st_size
     actual_mtime = datetime.fromtimestamp(st.st_mtime, tz=timezone.utc).isoformat()
     actual_sha256 = sha256_of(abs_path)
-    actual_git = git_hash_of(os.path.relpath(abs_path, repo_root) if repo_root else abs_path, repo_root=repo_root)
+    actual_git = git_hash_of(
+        os.path.relpath(abs_path, repo_root) if repo_root else abs_path,
+        repo_root=repo_root,
+    )
     result["actual"]["size_bytes"] = actual_size
     result["actual"]["mtime_iso"] = actual_mtime
     result["actual"]["sha256"] = actual_sha256
     result["actual"]["git_hash"] = actual_git
     # comparisons
-    if entry.get("size_bytes") is not None and int(entry.get("size_bytes")) != actual_size:
+    if (
+        entry.get("size_bytes") is not None
+        and int(entry.get("size_bytes")) != actual_size
+    ):
         result["issues"].append("MISMATCH_SIZE")
     if entry.get("sha256") is not None and entry.get("sha256") != actual_sha256:
         result["issues"].append("MISMATCH_SHA256")
     # tolerance: strict equality for mtime (could be fine-tuned)
     if entry.get("mtime_iso") is not None and entry.get("mtime_iso") != actual_mtime:
         result["issues"].append("MISMATCH_MTIME")
-    if entry.get("git_hash") is not None and actual_git is not None and entry.get("git_hash") != actual_git:
+    if (
+        entry.get("git_hash") is not None
+        and actual_git is not None
+        and entry.get("git_hash") != actual_git
+    ):
         result["issues"].append("MISMATCH_GIT_HASH")
     return result

+
 def main():
-    ap = argparse.ArgumentParser(description="Vérifier manifest JSON (sha256, size, mtime, git hash)")
+    ap = argparse.ArgumentParser(
+        description="Vérifier manifest JSON (sha256, size, mtime, git hash)"
+    )
     ap.add_argument("manifest", help="chemin vers manifest JSON")
-    ap.add_argument("--repo-root", default=".", help="racine du dépôt (pour chemins relatifs, git operations)")
+    ap.add_argument(
+        "--repo-root",
+        default=".",
+        help="racine du dépôt (pour chemins relatifs, git operations)",
+    )
     ap.add_argument("--output", default=None, help="fichier JSON du rapport")
     ap.add_argument("--verbose", action="store_true")
     args = ap.parse_args()
@@ -110,17 +137,24 @@ def main():
             summary["mismatches"] += 1

     # output human readable
-    print("="*60)
+    print("=" * 60)
     print(f"Manifest: {manifest_path}")
     print(f"Repo root: {repo_root}")
-    print(f"Entries: {summary['total']}, OK: {summary['ok']}, Missing: {summary['missing']}, Mismatches: {summary['mismatches']}")
-    print("="*60)
+    print(
+        f"Entries: {summary['total']}, OK: {summary['ok']}, Missing: {summary['missing']}, Mismatches: {summary['mismatches']}"
+    )
+    print("=" * 60)
     if args.verbose:
         for r in results:
             print(json.dumps(r, indent=2, ensure_ascii=False))

     if args.output:
-        out = {"manifest_path": manifest_path, "repo_root": repo_root, "summary": summary, "results": results}
+        out = {
+            "manifest_path": manifest_path,
+            "repo_root": repo_root,
+            "summary": summary,
+            "results": results,
+        }
         with open(args.output, "w", encoding="utf-8") as f:
             json.dump(out, f, indent=2, ensure_ascii=False)
         print("Rapport écrit:", args.output)
@@ -130,10 +164,11 @@ def main():
             if r.get("issues"):
                 print(r["path"], "->", ", ".join(r["issues"]))
     # exit code: 0 if all ok or only missing? choose 0 if no mismatches and missing==0
-    if summary["mismatches"]==0 and summary["missing"]==0:
+    if summary["mismatches"] == 0 and summary["missing"] == 0:
         sys.exit(0)
     else:
         sys.exit(1)

+
 if __name__ == "__main__":
     main()
diff --git a/zz-tests/test_manifest.py b/zz-tests/test_manifest.py
index 93fd58d..80c1409 100755
--- a/zz-tests/test_manifest.py
+++ b/zz-tests/test_manifest.py
@@ -1,5 +1,4 @@
 import json
-import os
 import re
 import subprocess
 import sys
@@ -11,40 +10,61 @@ MASTER = MANIFEST_DIR / "manifest_master.json"
 PUBLICATION = MANIFEST_DIR / "manifest_publication.json"

 ALLOWED_ROLES = {
-    "data","config","code","figure","document","meta",
-    "script","schema","manifest","artifact","source","bibliography"
+    "data",
+    "config",
+    "code",
+    "figure",
+    "document",
+    "meta",
+    "script",
+    "schema",
+    "manifest",
+    "artifact",
+    "source",
+    "bibliography",
 }

+
 def load_json(p: Path):
     assert p.exists(), f"Missing file: {p}"
     with p.open("r", encoding="utf-8") as f:
         return json.load(f)

+
 def test_master_publication_exist_and_heads():
     for p in (MASTER, PUBLICATION):
         js = load_json(p)
         assert "manifest_version" in js and "project" in js and "entries" in js
         assert isinstance(js["entries"], list)

+
 def _is_relative_path(path_str: str) -> bool:
     if path_str.startswith("/") or re.match(r"^[A-Za-z]:\\", path_str):
         return False
     return True

+
 def test_entries_are_relative_and_roles_allowed():
     js = load_json(MASTER)
     for e in js["entries"]:
-        assert _is_relative_path(e.get("path","")), f"Absolute path found: {e}"
+        assert _is_relative_path(e.get("path", "")), f"Absolute path found: {e}"
         role = e.get("role")
         assert role in ALLOWED_ROLES, f"Unexpected role={role} for {e.get('path')}"

+
 def test_diag_master_no_errors_json_report():
     cmd = [
-        sys.executable, str(MANIFEST_DIR / "diag_consistency.py"),
+        sys.executable,
+        str(MANIFEST_DIR / "diag_consistency.py"),
         str(MASTER),
-        "--report", "json",
-        "--normalize-paths", "--apply-aliases", "--strip-internal",
-        "--content-check", "--fail-on", "errors"
+        "--report",
+        "json",
+        "--normalize-paths",
+        "--apply-aliases",
+        "--strip-internal",
+        "--content-check",
+        "--fail-on",
+        "errors",
     ]
     res = subprocess.run(cmd, capture_output=True, text=True)
     assert res.returncode == 0, f"diag_consistency failed:\n{res.stdout}\n{res.stderr}"
diff --git a/zz-tests/test_schemas.py b/zz-tests/test_schemas.py
index 93fd58d..80c1409 100755
--- a/zz-tests/test_schemas.py
+++ b/zz-tests/test_schemas.py
@@ -1,5 +1,4 @@
 import json
-import os
 import re
 import subprocess
 import sys
@@ -11,40 +10,61 @@ MASTER = MANIFEST_DIR / "manifest_master.json"
 PUBLICATION = MANIFEST_DIR / "manifest_publication.json"

 ALLOWED_ROLES = {
-    "data","config","code","figure","document","meta",
-    "script","schema","manifest","artifact","source","bibliography"
+    "data",
+    "config",
+    "code",
+    "figure",
+    "document",
+    "meta",
+    "script",
+    "schema",
+    "manifest",
+    "artifact",
+    "source",
+    "bibliography",
 }

+
 def load_json(p: Path):
     assert p.exists(), f"Missing file: {p}"
     with p.open("r", encoding="utf-8") as f:
         return json.load(f)

+
 def test_master_publication_exist_and_heads():
     for p in (MASTER, PUBLICATION):
         js = load_json(p)
         assert "manifest_version" in js and "project" in js and "entries" in js
         assert isinstance(js["entries"], list)

+
 def _is_relative_path(path_str: str) -> bool:
     if path_str.startswith("/") or re.match(r"^[A-Za-z]:\\", path_str):
         return False
     return True

+
 def test_entries_are_relative_and_roles_allowed():
     js = load_json(MASTER)
     for e in js["entries"]:
-        assert _is_relative_path(e.get("path","")), f"Absolute path found: {e}"
+        assert _is_relative_path(e.get("path", "")), f"Absolute path found: {e}"
         role = e.get("role")
         assert role in ALLOWED_ROLES, f"Unexpected role={role} for {e.get('path')}"

+
 def test_diag_master_no_errors_json_report():
     cmd = [
-        sys.executable, str(MANIFEST_DIR / "diag_consistency.py"),
+        sys.executable,
+        str(MANIFEST_DIR / "diag_consistency.py"),
         str(MASTER),
-        "--report", "json",
-        "--normalize-paths", "--apply-aliases", "--strip-internal",
-        "--content-check", "--fail-on", "errors"
+        "--report",
+        "json",
+        "--normalize-paths",
+        "--apply-aliases",
+        "--strip-internal",
+        "--content-check",
+        "--fail-on",
+        "errors",
     ]
     res = subprocess.run(cmd, capture_output=True, text=True)
     assert res.returncode == 0, f"diag_consistency failed:\n{res.stdout}\n{res.stderr}"
diff --git a/zz-tools/json_audit_strict.py b/zz-tools/json_audit_strict.py
index 2e4af99..71514f8 100755
--- a/zz-tools/json_audit_strict.py
+++ b/zz-tools/json_audit_strict.py
@@ -1,4 +1,8 @@
-import os, json, re, sys
+import os
+import json
+import re
+import sys
+
 bad = 0
 html = re.compile(r"<[A-Za-z][^>]*>")
 for root, _, files in os.walk("."):
@@ -10,9 +14,12 @@ for root, _, files in os.walk("."):
             txt = open(p, "r", encoding="utf-8").read()
             obj = json.loads(txt)
             if not txt.strip() or obj in ({}, []):
-                print("WARN empty-ish:", p); bad += 1
+                print("WARN empty-ish:", p)
+                bad += 1
             if html.search(txt):
-                print("WARN html-like:", p); bad += 1
+                print("WARN html-like:", p)
+                bad += 1
         except Exception as e:
-            print("ERR invalid JSON:", p, "->", e); bad += 1
+            print("ERR invalid JSON:", p, "->", e)
+            bad += 1
 sys.exit(1 if bad else 0)
diff --git a/zz-tools/manifest_check.py b/zz-tools/manifest_check.py
index f2265aa..1a2dd7e 100755
--- a/zz-tools/manifest_check.py
+++ b/zz-tools/manifest_check.py
@@ -1,25 +1,39 @@
-import json, os, sys
-p="zz-manifests/manifest_master.json"
+import json
+import os
+import sys
+
+p = "zz-manifests/manifest_master.json"
 if not os.path.exists(p):
-    print("SKIP:", p, "(missing)"); sys.exit(0)
-m=json.load(open(p,"r",encoding="utf-8"))
-missing=[]
+    print("SKIP:", p, "(missing)")
+    sys.exit(0)
+m = json.load(open(p, "r", encoding="utf-8"))
+missing = []
+
+
 def check(path):
     if not os.path.isabs(path):
-        path=os.path.join(".",path)
+        path = os.path.join(".", path)
     if not os.path.exists(path):
         missing.append(path)
+
+
 def visit(v):
     if isinstance(v, list):
-        for x in v: visit(x)
+        for x in v:
+            visit(x)
     elif isinstance(v, dict):
-        if "path" in v: check(v["path"])
-        if "file" in v: check(v["file"])
+        if "path" in v:
+            check(v["path"])
+        if "file" in v:
+            check(v["file"])
     elif isinstance(v, str):
         check(v)
-for k in ("files","data","artifacts"):
+
+
+for k in ("files", "data", "artifacts"):
     visit(m.get(k, []))
 if missing:
-    print("Missing files in manifest:"); [print("  -",x) for x in missing]
+    print("Missing files in manifest:")
+    [print("  -", x) for x in missing]
     sys.exit(1)
 print("Manifest OK.")
diff --git a/zz-tools/manifest_report.py b/zz-tools/manifest_report.py
index 54bef8d..a2eb9f7 100755
--- a/zz-tools/manifest_report.py
+++ b/zz-tools/manifest_report.py
@@ -1,25 +1,36 @@
-import json, os, datetime
-src="zz-manifests/manifest_master.json"
-dst="zz-manifests/manifest_report.md"
+import json
+import os
+import datetime
+
+src = "zz-manifests/manifest_master.json"
+dst = "zz-manifests/manifest_report.md"
 if not os.path.exists(src):
-    print("SKIP:", src, "(missing)"); raise SystemExit(0)
-m=json.load(open(src,"r",encoding="utf-8"))
-ts=datetime.datetime.utcnow().isoformat()+"Z"
-hdr=["# Manifest Report", f"- source: {src}", f"- generated: {ts}", ""]
+    print("SKIP:", src, "(missing)")
+    raise SystemExit(0)
+m = json.load(open(src, "r", encoding="utf-8"))
+ts = datetime.datetime.utcnow().isoformat() + "Z"
+hdr = ["# Manifest Report", f"- source: {src}", f"- generated: {ts}", ""]
+
+
 def sec(title, items):
-    out=[f"## {title}", ""]
-    if not items: out.append("*none*"); out.append(""); return out
+    out = [f"## {title}", ""]
+    if not items:
+        out.append("*none*")
+        out.append("")
+        return out
     for s in items:
         if isinstance(s, dict):
-            line="- "+str(s.get("path") or s.get("file") or s)
+            line = "- " + str(s.get("path") or s.get("file") or s)
         else:
-            line="- "+str(s)
+            line = "- " + str(s)
         out.append(line)
     out.append("")
     return out
-body=[]
+
+
+body = []
 body += sec("Files", m.get("files", []))
 body += sec("Data", m.get("data", []))
 body += sec("Artifacts", m.get("artifacts", []))
-open(dst,"w",encoding="utf-8").write("\n".join(hdr+body))
+open(dst, "w", encoding="utf-8").write("\n".join(hdr + body))
 print("Wrote", dst)
diff --git a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter02--02_optimal_parameters.json.bak b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter02--02_optimal_parameters.json.bak
index 6d0bbd8..75efe83 100755
--- a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter02--02_optimal_parameters.json.bak
+++ b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter02--02_optimal_parameters.json.bak
@@ -22,4 +22,4 @@
   },
   "max_epsilon_primary": 2.1947570587947003e-05,
   "max_epsilon_order2": 0.07194300543935292
-}
\ No newline at end of file
+}
diff --git a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter02--02_primordial_spectrum_spec.json.bak b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter02--02_primordial_spectrum_spec.json.bak
index d5be0e4..80b9d4c 100755
--- a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter02--02_primordial_spectrum_spec.json.bak
+++ b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter02--02_primordial_spectrum_spec.json.bak
@@ -12,4 +12,4 @@
     "c2": 0.01,
     "c2_2": 0.0
   }
-}
\ No newline at end of file
+}
diff --git a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter02--02_primordial_spectrum_spec.json.bak.bak b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter02--02_primordial_spectrum_spec.json.bak.bak
index c9a8b5a..d5c632c 100644
--- a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter02--02_primordial_spectrum_spec.json.bak.bak
+++ b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter02--02_primordial_spectrum_spec.json.bak.bak
@@ -12,4 +12,4 @@
     "c2": 0.01,
     "c2_2": 0.0
   }
-}
\ No newline at end of file
+}
diff --git a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter03--03_fR_stability_meta.json.bak b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter03--03_fR_stability_meta.json.bak
index 1ca22c8..4d19e78 100755
--- a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter03--03_fR_stability_meta.json.bak
+++ b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter03--03_fR_stability_meta.json.bak
@@ -5,4 +5,4 @@
     "03_domaine_stabilite_fR.csv",
     "03_frontiere_stabilite_fR.csv"
   ]
-}
\ No newline at end of file
+}
diff --git a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter05--05_bbn_params.json.bak b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter05--05_bbn_params.json.bak
index 78a7d2a..8b4f5c4 100755
--- a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter05--05_bbn_params.json.bak
+++ b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter05--05_bbn_params.json.bak
@@ -5,4 +5,4 @@
     "primary": 0.0,
     "order2": 0.0
   }
-}
\ No newline at end of file
+}
diff --git a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter06--06_params_cmb.json.bak b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter06--06_params_cmb.json.bak
index bff84a8..91b4433 100755
--- a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter06--06_params_cmb.json.bak
+++ b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter06--06_params_cmb.json.bak
@@ -20,4 +20,4 @@
   "c1": 0.1,
   "c2": 0.01,
   "max_delta_Cl_rel": 0.6296832401981298
-}
\ No newline at end of file
+}
diff --git a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter07--07_perturbations_meta.json.bak b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter07--07_perturbations_meta.json.bak
index c609c6a..fc4bc35 100755
--- a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter07--07_perturbations_meta.json.bak
+++ b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter07--07_perturbations_meta.json.bak
@@ -14,4 +14,4 @@
     "07_delta_phi_matrix.csv"
   ],
   "generated_at": "2025-09-14T18:00:42.364217+00:00"
-}
\ No newline at end of file
+}
diff --git a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter08--08_coupling_params.json.bak b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter08--08_coupling_params.json.bak
index 8a7dbbc..46697ff 100755
--- a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter08--08_coupling_params.json.bak
+++ b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter08--08_coupling_params.json.bak
@@ -8,4 +8,4 @@
   "param2_min": -1.0,
   "param2_max": 1.0,
   "n_param2": 101
-}
\ No newline at end of file
+}
diff --git a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_best_params.json.bak b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_best_params.json.bak
index 8d3936f..b268113 100755
--- a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_best_params.json.bak
+++ b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_best_params.json.bak
@@ -44,4 +44,4 @@
     "scan_csv": "zz-data/chapter09/09_scan_20250816T182747Z_-0.50_0.60_21x21.csv",
     "checkpoint_csv": "zz-data/chapter09/09_scan_20250816T182747Z_-0.50_0.60_21x21_ckpt.csv"
   }
-}
\ No newline at end of file
+}
diff --git a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_comparison_milestones.meta.json.bak b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_comparison_milestones.meta.json.bak
index 89e0daf..1b04758 100755
--- a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_comparison_milestones.meta.json.bak
+++ b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_comparison_milestones.meta.json.bak
@@ -18,4 +18,4 @@
     ],
     "flagged_csv": "zz-data/chapter09/09_comparison_milestones.flagged.csv"
   }
-}
\ No newline at end of file
+}
diff --git a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_metrics_phase.json.bak b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_metrics_phase.json.bak
index b13d4c2..4c7c2df 100755
--- a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_metrics_phase.json.bak
+++ b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_metrics_phase.json.bak
@@ -101,4 +101,4 @@
     "k_cycles": -56
   },
   "poly_rebranch_k_cycles": 0
-}
\ No newline at end of file
+}
diff --git a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_phases_imrphenom.meta.json.bak b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_phases_imrphenom.meta.json.bak
index 0cea480..6f03bce 100755
--- a/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_phases_imrphenom.meta.json.bak
+++ b/zz-trash/FR_legacy_20250918-101954/zz-data--chapter09--09_phases_imrphenom.meta.json.bak
@@ -12,4 +12,4 @@
     "pandas": "2.3.1",
     "lalsuite": "6.2.0"
   }
-}
\ No newline at end of file
+}
diff --git a/zz-workflows/README.md b/zz-workflows/README.md
index 0d972c9..0536348 100755
--- a/zz-workflows/README.md
+++ b/zz-workflows/README.md
@@ -37,6 +37,3 @@ les credentials correspondants dans les \_secrets\_ du dépôt et étendre `rele
 \- Préparer des jeux de données \*\*réduits\*\* pour les tests (fixtures) afin d’éviter

 &nbsp; des temps de CI trop longs.
-
-
-
